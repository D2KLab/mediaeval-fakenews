{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea287cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\" # the GPU on robinson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1f23731",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import deque\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertModel, AdamW, AutoTokenizer,XLNetForSequenceClassification, BertForSequenceClassification, RobertaForSequenceClassification\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import emoji\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "random_seed = 0\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "conspiracies = ['Suppressed Cures',\n",
    "     'Behaviour and Mind Control',\n",
    "     'Antivax',\n",
    "     'Fake virus',\n",
    "     'Intentional Pandemic',\n",
    "     'Harmful Radiation/ Influence',\n",
    "     'Population reduction',\n",
    "     'New World Order',\n",
    "     'Satanism']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "275c41d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'roberta-base'\n",
    "\n",
    "replace_lowercase_flag = False\n",
    "remove_stopwords_flag = False\n",
    "remove_hashtags_flag = True\n",
    "replace_emojis_flag = True\n",
    "clean_tweets_flag = False\n",
    "\n",
    "class_weights_flag = True\n",
    "all_data = True\n",
    "\n",
    "classification = True\n",
    "\n",
    "# fold\n",
    "k=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91907c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweets(tweets):\n",
    "    char_to_remove = ['\\n', '\\xa0']\n",
    "    # replace these words with a word that the model understands\n",
    "    corona_synonyms = ['coronavirus',\n",
    "                      'covid-19',\n",
    "                      'covid19',\n",
    "                      'covid 19',\n",
    "                      'covid',\n",
    "                      'corona',\n",
    "                      'sarscov2'\n",
    "                      'sars',\n",
    "                      'Coronavirus',\n",
    "                      'Corona',\n",
    "                      'Covid19',\n",
    "                      'COVID19',\n",
    "                      'Covid-19',\n",
    "                      'COVID-19',\n",
    "                      'COVID 19',\n",
    "                      'Covid',\n",
    "                      'COVID',\n",
    "                      'SARSCOV2',\n",
    "                      'SARS',\n",
    "                      'Coronaviruses']\n",
    "    corona_replaced = 'virus'\n",
    "    tweets_clean = []\n",
    "    for tw in tweets:\n",
    "        for c in char_to_remove:\n",
    "            tw = tw.replace(c, '')\n",
    "        tw = tw.replace('&amp;', '&')\n",
    "        \n",
    "        for syn in corona_synonyms:\n",
    "            if syn in tw:\n",
    "                tw = tw.replace(syn, corona_replaced)\n",
    "        tweets_clean.append(tw)\n",
    "    return tweets_clean\n",
    "\n",
    "def extract_hashtags(tweet):\n",
    "    # Returns hashtags in a list for a given tweet\n",
    "    \n",
    "    #tweet = tweet.replace('\\xa0','')\n",
    "    #tweet = tweet.replace('\\n','')\n",
    "    \n",
    "    tweet_words = tweet.split(' ')\n",
    "    tweet_words = [w for w in tweet_words if w!='']\n",
    "    hashtags = []\n",
    "    for word in tweet_words:\n",
    "        if word[0]=='#':\n",
    "            hashtags.append(word)\n",
    "    return hashtags\n",
    "\n",
    "def extract_emojis (tw):\n",
    "    # Returns emojis in a list for a given tweet\n",
    "    # Using Deque for a sliding window (emojis can be combined together to form other emojis)\n",
    "    \n",
    "    emojis = []\n",
    "    \n",
    "    l = []\n",
    "    max_l = 7\n",
    "    \n",
    "    for i in range(0, max_l):\n",
    "        l.append(tw[-1-i])\n",
    "    l = deque(l, maxlen=max_l)\n",
    "    skip=0\n",
    "    \n",
    "    for i in range (0, len(tw)):\n",
    "        if skip == 0:\n",
    "            for j in range (max_l-1, -1, -1):\n",
    "                str_to_test = ''\n",
    "                for k in range (0, j+1):\n",
    "                    str_to_test+=l[j-k]\n",
    "                if str_to_test in emoji.UNICODE_EMOJI['en']:\n",
    "                    \n",
    "                    emojis.append(str_to_test)\n",
    "                    skip=j\n",
    "                    break\n",
    "            try:\n",
    "                l.append(tw[-1-i-max_l])\n",
    "            except IndexError:\n",
    "                l.append('')\n",
    "        else:\n",
    "            skip=skip-1\n",
    "            try:\n",
    "                l.append(tw[-1-i-max_l])\n",
    "            except IndexError:\n",
    "                l.append('')\n",
    "    emojis.reverse()\n",
    "    return emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dddbd3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_lowercase(tweets):\n",
    "    # Converts tweets to lowercase\n",
    "    tweets_lowercase = []\n",
    "    for tw in tweets:\n",
    "        tweets_lowercase.append(tw.lower())\n",
    "    return tweets_lowercase\n",
    "\n",
    "def remove_stopwords(tweets):\n",
    "    # Removes stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    tweets_no_stopwords = []\n",
    "    for tw in tweets:\n",
    "        tw = tw.split(' ')\n",
    "        tweets_no_stopwords.append(' '.join([word for word in tw if not word in stop_words]))\n",
    "\n",
    "    return tweets_no_stopwords\n",
    "\n",
    "def remove_hashtags(tweets):\n",
    "    # Remove the # char\n",
    "    \n",
    "    tweets = [tw.replace('#', '') for tw in tweets]\n",
    "    return tweets\n",
    "\n",
    "def replace_emojis(tweets):\n",
    "    # Convert emoji to text\n",
    "    \n",
    "    tweets_no_emojis = []\n",
    "    for tw in tweets:\n",
    "        emojis = extract_emojis(tw)\n",
    "        for e in emojis:\n",
    "            e_text = emoji.UNICODE_EMOJI['en'][e].replace('_',' ').replace(':', '')\n",
    "            tw = tw.replace(e, e_text)\n",
    "        tweets_no_emojis.append(tw)\n",
    "\n",
    "    return tweets_no_emojis\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b96c60ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './mediaeval-fakenews/data/task1/'\n",
    "filelist = os.listdir(data_path)\n",
    "\n",
    "\n",
    "df_list = [pd.read_csv(data_path+file) for file in filelist]\n",
    "\n",
    "\n",
    "test_df = df_list[k]    \n",
    "train_df = pd.concat(df_list[:k]+df_list[k+1:])\n",
    "\n",
    "\n",
    "tw_train = train_df['tweet'].tolist()\n",
    "tw_test = test_df['tweet'].tolist()\n",
    "\n",
    "\n",
    "if all_data:\n",
    "    df = pd.read_csv('./mediaeval-fakenews/data/dev-full-task-1-clean.csv')\n",
    "    tw_train = df['tweet'].tolist()\n",
    "    labels_train = df['1'].tolist()\n",
    "\n",
    "if clean_tweets_flag:\n",
    "    tw_train = clean_tweets(tw_train)\n",
    "    tw_test = clean_tweets(tw_test)\n",
    "\n",
    "if replace_lowercase_flag:\n",
    "    tw_train = to_lowercase(tw_train)\n",
    "    tw_test = to_lowercase(tw_test)\n",
    "\n",
    "if remove_stopwords_flag:\n",
    "    tw_train = remove_stopwords(tw_train)\n",
    "    tw_test = remove_stopwords(tw_test)\n",
    "\n",
    "if remove_hashtags_flag:\n",
    "    tw_train = remove_hashtags(tw_train)\n",
    "    tw_test = remove_hashtags(tw_test)\n",
    "\n",
    "if replace_emojis_flag:\n",
    "    tw_train = replace_emojis(tw_train)\n",
    "    tw_test = replace_emojis(tw_test)\n",
    "\n",
    "\n",
    "if not all_data:\n",
    "    labels_train = train_df['label'].tolist()\n",
    "\n",
    "labels_train = [labels_train[i]-1 for i in range(0, len(labels_train))]\n",
    "labels_test = test_df['label'].tolist()\n",
    "labels_test = [labels_test[i]-1 for i in range(0, len(labels_test))]\n",
    "ids_test = test_df['ids'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4eee9191",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.0261, 5.7343, 3.0116], device='cuda:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./mediaeval-fakenews/data/'+'dev-full-task-1-clean.csv')\n",
    "labels = df['1'].tolist()\n",
    "labels =(np.array(labels)-1).tolist()\n",
    "tweets = df['tweet'].tolist()\n",
    "\n",
    "weights = torch.FloatTensor([1, 1, 1]).cuda()\n",
    "\n",
    "if class_weights_flag:\n",
    "    weights = torch.FloatTensor([len(labels)/labels.count(i) for i in range(0,3)]).cuda()\n",
    "\n",
    "# The weights are the inverse frequency of the class multiplied by the number of tweets in the training data (1554)    \n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51af1837",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenized_input = tokenizer(tw_train)\n",
    "\n",
    "m = 0\n",
    "for tokens in tokenized_input['input_ids']:\n",
    "    if len(tokens)>m:\n",
    "        m=len(tokens)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "303231ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 128\n",
    "\n",
    "tokenized_input = tokenizer(tw_train, max_length=MAX_LEN, padding='max_length', truncation=True)\n",
    "tokenized_test = tokenizer(tw_test, max_length=MAX_LEN, padding='max_length', truncation=True)\n",
    "\n",
    "\n",
    "if 'roberta' in model_name:\n",
    "    train_input_ids, train_attention_mask = tokenized_input['input_ids'], tokenized_input['attention_mask']\n",
    "    test_input_ids, test_attention_mask = tokenized_test['input_ids'], tokenized_test['attention_mask']\n",
    "\n",
    "\n",
    "else:\n",
    "    train_input_ids, train_token_type_ids, train_attention_mask = tokenized_input['input_ids'], tokenized_input['token_type_ids'], tokenized_input['attention_mask']\n",
    "    test_input_ids, test_token_type_ids, test_attention_mask = tokenized_test['input_ids'], tokenized_test['token_type_ids'], tokenized_test['attention_mask']\n",
    "\n",
    "    train_token_type_ids = torch.tensor(train_token_type_ids)\n",
    "    test_token_type_ids = torch.tensor(test_token_type_ids)\n",
    "\n",
    "\n",
    "train_labels = labels_train\n",
    "test_labels = labels_test\n",
    "\n",
    "\n",
    "# Convert to torch tensor\n",
    "train_input_ids = (torch.tensor(train_input_ids))\n",
    "train_labels = (torch.tensor(train_labels))\n",
    "train_attention_mask = (torch.tensor(train_attention_mask))\n",
    "\n",
    "test_input_ids = (torch.tensor(test_input_ids))\n",
    "test_labels = (torch.tensor(test_labels))\n",
    "test_attention_mask = (torch.tensor(test_attention_mask))\n",
    "test_ids = torch.tensor(ids_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7731e4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 70 #usually 32 for bert/roberta base, 24 for xlnet, 6 for bert/roberta large\n",
    "\n",
    "if 'roberta' in model_name:\n",
    "    train_data = (TensorDataset(train_input_ids, train_attention_mask, train_labels))\n",
    "    test_data = (TensorDataset(test_input_ids, test_attention_mask, test_labels, test_ids))\n",
    "\n",
    "else:\n",
    "    train_data = (TensorDataset(train_input_ids, train_attention_mask, train_labels, train_token_type_ids))\n",
    "    test_data = (TensorDataset(test_input_ids, test_attention_mask, test_labels, test_token_type_ids))\n",
    "\n",
    "# Build DataLoaders\n",
    "train_sampler = (RandomSampler(train_data))\n",
    "train_dataloader = (DataLoader(train_data, sampler=train_sampler, batch_size=batch_size))\n",
    "\n",
    "test_sampler = (SequentialSampler(test_data))\n",
    "test_dataloader = (DataLoader(test_data, sampler=test_sampler, batch_size=batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc4aef79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tesla K80'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be5e1726",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobertaClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_classes):\n",
    "        super().__init__()\n",
    "        self.n_classes = n_classes\n",
    "        self.bert = RobertaForSequenceClassification.from_pretrained(model_name, num_labels=n_classes)\n",
    "        if n_classes >1:\n",
    "            # For classification\n",
    "            self.criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "        else:\n",
    "            # For regression\n",
    "            self.criterion = nn.MSELoss()\n",
    "        \n",
    "    def forward(self, input_ids, input_mask, labels):\n",
    "        outputs = self.bert(input_ids, input_mask)\n",
    "        \n",
    "        logits = outputs[0]\n",
    "        if self.n_classes == 1:\n",
    "            labels=labels.float()\n",
    "        loss = self.criterion(logits, labels)\n",
    "        \n",
    "        return loss, logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae612207",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaClassifier(\n",
       "  (bert): RobertaForSequenceClassification(\n",
       "    (roberta): RobertaModel(\n",
       "      (embeddings): RobertaEmbeddings(\n",
       "        (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "        (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "        (token_type_embeddings): Embedding(1, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): RobertaEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (3): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (4): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (5): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (6): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (7): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (8): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (9): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (10): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (11): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (classifier): RobertaClassificationHead(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (out_proj): Linear(in_features=768, out_features=3, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (criterion): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if 'roberta' in model_name:\n",
    "    if classification:\n",
    "        model = RobertaClassifier(3)\n",
    "    else:\n",
    "        model = RobertaClassifier(1)\n",
    "else:\n",
    "    print(\"Please use Roberta model\")\n",
    "    \n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9fb421a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer_grouped_parameters\n",
    "# lr = 5e-5 good for base\n",
    "# lr = 5e-6 good for large\n",
    "\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr=5e-5,\n",
    "                  weight_decay = 0.01)\n",
    "\n",
    "scheduler = ReduceLROnPlateau(optimizer, patience=4, factor=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6a413c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_regression(val):\n",
    "    if val<0.5:\n",
    "        return 0\n",
    "    elif val<1.5:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc92b42b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   0%|                                                 | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.072143609109132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  10%|████                                     | 1/10 [01:06<09:56, 66.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Eval loss: 0.819002652168274\n",
      "\t Eval ACC: 0.5691318327974276\n",
      "\t Eval MCC: 0.3827846853381036\n",
      "\t Eval MCC 1 vs other: 0.25776954343712466\n",
      "[82, 95, 134] [153, 55, 103]\n",
      "Train loss: 0.8108663662620212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  20%|████████▏                                | 2/10 [02:12<08:51, 66.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Eval loss: 0.5889796257019043\n",
      "\t Eval ACC: 0.7781350482315113\n",
      "\t Eval MCC: 0.6686022437953016\n",
      "\t Eval MCC 1 vs other: 0.699785223971466\n",
      "[120, 84, 107] [153, 55, 103]\n",
      "Train loss: 0.524394567893899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  30%|████████████▎                            | 3/10 [03:19<07:45, 66.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Eval loss: 0.2790530055761337\n",
      "\t Eval ACC: 0.8713826366559485\n",
      "\t Eval MCC: 0.8051505210753744\n",
      "\t Eval MCC 1 vs other: 0.7862475952141406\n",
      "[126, 61, 124] [153, 55, 103]\n",
      "Train loss: 0.3164292865473291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  40%|████████████████▍                        | 4/10 [04:26<06:39, 66.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Eval loss: 0.13135399296879768\n",
      "\t Eval ACC: 0.954983922829582\n",
      "\t Eval MCC: 0.9285197760699242\n",
      "\t Eval MCC 1 vs other: 0.9430231421581566\n",
      "[146, 52, 113] [153, 55, 103]\n",
      "Train loss: 0.16268873376690823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  50%|████████████████████▌                    | 5/10 [05:32<05:32, 66.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Eval loss: 0.04699884001165629\n",
      "\t Eval ACC: 0.9742765273311897\n",
      "\t Eval MCC: 0.9595502019953146\n",
      "\t Eval MCC 1 vs other: 0.9497576168011006\n",
      "[145, 56, 110] [153, 55, 103]\n",
      "Train loss: 0.12096793107364488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  60%|████████████████████████▌                | 6/10 [06:39<04:26, 66.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Eval loss: 0.027229376137256622\n",
      "\t Eval ACC: 0.9871382636655949\n",
      "\t Eval MCC: 0.9795004486333443\n",
      "\t Eval MCC 1 vs other: 0.9808791872737402\n",
      "[150, 59, 102] [153, 55, 103]\n",
      "Train loss: 0.08926799889329982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  70%|████████████████████████████▋            | 7/10 [07:45<03:19, 66.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Eval loss: 0.008718459773808718\n",
      "\t Eval ACC: 1.0\n",
      "\t Eval MCC: 1.0\n",
      "\t Eval MCC 1 vs other: 1.0\n",
      "[153, 55, 103] [153, 55, 103]\n",
      "Train loss: 0.04810868067990826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  80%|████████████████████████████████▊        | 8/10 [08:52<02:12, 66.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Eval loss: 0.0023711236659437417\n",
      "\t Eval ACC: 1.0\n",
      "\t Eval MCC: 1.0\n",
      "\t Eval MCC 1 vs other: 1.0\n",
      "[153, 55, 103] [153, 55, 103]\n",
      "Train loss: 0.01338563962718067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  90%|████████████████████████████████████▉    | 9/10 [09:58<01:06, 66.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Eval loss: 0.001979693048633635\n",
      "\t Eval ACC: 1.0\n",
      "\t Eval MCC: 1.0\n",
      "\t Eval MCC 1 vs other: 1.0\n",
      "[153, 55, 103] [153, 55, 103]\n",
      "Train loss: 0.027174129454500002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|████████████████████████████████████████| 10/10 [11:04<00:00, 66.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Eval loss: 0.0017233240650966763\n",
      "\t Eval ACC: 1.0\n",
      "\t Eval MCC: 1.0\n",
      "\t Eval MCC 1 vs other: 1.0\n",
      "[153, 55, 103] [153, 55, 103]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "\n",
    "best_MCC = 0\n",
    "best_MCCNC = 0\n",
    "best_loss = 999\n",
    "best_acc = 0\n",
    "best_state_dict = model.state_dict()\n",
    "best_epoch = 0\n",
    "\n",
    "for e in trange(epochs, desc=\"Epoch\"):\n",
    "\n",
    "    # Training\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        if 'roberta' in model_name:\n",
    "            b_input_ids, b_input_mask, b_labels = batch\n",
    "        else:    \n",
    "            b_input_ids, b_input_mask, b_labels, b_token_type_ids = batch\n",
    "\n",
    "        if not classification:\n",
    "            b_labels = b_labels.view(-1, 1)        \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        \n",
    "        if 'roberta' in model_name:\n",
    "            outputs = model(b_input_ids, b_input_mask, b_labels)\n",
    "        else:\n",
    "            outputs = model(b_input_ids, b_token_type_ids, b_input_mask, b_labels)\n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "        \n",
    "        #print(step, loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "\n",
    "    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "\n",
    "\n",
    "    # Testing\n",
    "\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    labels = []\n",
    "\n",
    "    eval_loss = 0\n",
    "    steps=0\n",
    "    for step, batch in enumerate(test_dataloader):\n",
    "\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        if 'roberta' in model_name:\n",
    "            b_input_ids, b_input_mask, b_labels, ids = batch\n",
    "        else:    \n",
    "            b_input_ids, b_input_mask, b_labels, b_token_type_ids = batch\n",
    "\n",
    "        if not classification:\n",
    "            b_labels = b_labels.view(-1, 1)        \n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            if 'roberta' in model_name:\n",
    "                outputs = model(b_input_ids, b_input_mask, b_labels)\n",
    "            else:\n",
    "                outputs = model(b_input_ids, b_token_type_ids, b_input_mask, b_labels)\n",
    "            logits = outputs[1]\n",
    "            loss = outputs[0]\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        ground_truth = b_labels.detach().cpu().numpy()\n",
    "\n",
    "        steps+=1\n",
    "        eval_loss+=loss.detach().item()\n",
    "\n",
    "        \n",
    "        for p in logits:\n",
    "            if classification:\n",
    "                pred = p.argmax()\n",
    "            else:\n",
    "                pred = round_regression(p)\n",
    "            predictions.append(pred)\n",
    "\n",
    "        for gt in ground_truth:\n",
    "            labels.append(gt)\n",
    "    \n",
    "    scheduler.step(eval_loss/steps)\n",
    "    \n",
    "    LOSS = eval_loss/steps\n",
    "    ACC = metrics.accuracy_score(labels, predictions)\n",
    "    MCC = metrics.matthews_corrcoef(labels, predictions)\n",
    "    MCCNC = metrics.matthews_corrcoef(np.array(labels)>0, np.array(predictions)>0)\n",
    "    \n",
    "    if MCC>best_MCC:\n",
    "        best_loss = LOSS\n",
    "        best_acc = ACC\n",
    "        best_MCC =MCC\n",
    "        best_MCCNC = MCCNC\n",
    "        best_epoch = e\n",
    "        best_state_dict = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    print(\"\\t Eval loss: {}\".format(LOSS))\n",
    "    print(\"\\t Eval ACC: {}\".format(ACC))\n",
    "    print(\"\\t Eval MCC: {}\".format(MCC))\n",
    "    print(\"\\t Eval MCC 1 vs other: {}\".format(MCCNC))\n",
    "    print([predictions.count(i) for i in range(0,3)], [labels.count(i) for i in range(0, 3)])\n",
    "\n",
    "#torch.save(best_state_dict, './Models/task1/roberta_CV'+str(k)+'_e'+str(best_epoch)+'_'+str(round(best_MCC, 3))+'_classification.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8db86a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best epoch  6\n",
      "\t Eval loss: 0.008718459773808718\n",
      "\t Eval ACC: 1.0\n",
      "\t Eval MCC: 1.0\n",
      "\t Eval MCC 1 vs other: 1.0\n"
     ]
    }
   ],
   "source": [
    "print('Best epoch ', best_epoch)\n",
    "print(\"\\t Eval loss: {}\".format(best_loss))\n",
    "print(\"\\t Eval ACC: {}\".format(best_acc))\n",
    "print(\"\\t Eval MCC: {}\".format(best_MCC))\n",
    "print(\"\\t Eval MCC 1 vs other: {}\".format(best_MCCNC))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5c7f5914",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './Models/task1/roberta-base-all-train.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "caba909c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./mediaeval-fakenews/data/test-clean.csv')\n",
    "ids_test = df['ids']\n",
    "tw_test = df['tweet']\n",
    "\n",
    "if clean_tweets_flag:\n",
    "    tw_test = clean_tweets(tw_test)\n",
    "\n",
    "if replace_lowercase_flag:\n",
    "    tw_test = to_lowercase(tw_test)\n",
    "\n",
    "if remove_stopwords_flag:\n",
    "    tw_test = remove_stopwords(tw_test)\n",
    "\n",
    "if remove_hashtags_flag:\n",
    "    tw_test = remove_hashtags(tw_test)\n",
    "\n",
    "if replace_emojis_flag:\n",
    "    tw_test = replace_emojis(tw_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "971fc782",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_test = tokenizer(tw_test, max_length=MAX_LEN, padding='max_length', truncation=True)\n",
    "test_input_ids, test_attention_mask = tokenized_test['input_ids'], tokenized_test['attention_mask']\n",
    "\n",
    "test_labels = []\n",
    "for i in range(0, len(ids_test)):\n",
    "    test_labels.append(1)\n",
    "\n",
    "test_input_ids = torch.tensor(test_input_ids)\n",
    "test_attention_mask = torch.tensor(test_attention_mask)\n",
    "test_labels = torch.tensor(test_labels)\n",
    "test_ids = torch.tensor(ids_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e253dd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 70 # 32 if 256\n",
    "\n",
    "if 'roberta' in model_name:\n",
    "    test_data = TensorDataset(test_input_ids, test_attention_mask, test_labels, test_ids)\n",
    "    \n",
    "else:\n",
    "    test_data = TensorDataset(test_input_ids, test_attention_mask, test_token_type_ids)\n",
    "\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "24a815b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('./Models/task1/roberta-base-all-train.pth'))\n",
    "#model.load_state_dict(torch.load('./Models/task1/roberta_CV'+str(k)+'_e'+str(best_epoch)+'_'+str(round(best_MCC, 3))+'_classification.pth'))\n",
    "predictions = []\n",
    "labels = []\n",
    "ids_test = []\n",
    "\n",
    "logits_test = []\n",
    "\n",
    "eval_loss = 0\n",
    "steps=0\n",
    "for step, batch in enumerate(test_dataloader):\n",
    "\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "    if 'roberta' in model_name:\n",
    "        b_input_ids, b_input_mask, b_labels, ids = batch\n",
    "    else:    \n",
    "        b_input_ids, b_input_mask, b_labels, b_token_type_ids = batch\n",
    "\n",
    "    if not classification:\n",
    "        b_labels = b_labels.view(-1, 1)        \n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        if 'roberta' in model_name:\n",
    "            outputs = model(b_input_ids, b_input_mask, b_labels)\n",
    "        else:\n",
    "            outputs = model(b_input_ids, b_token_type_ids, b_input_mask, b_labels)\n",
    "        logits = outputs[1]\n",
    "        loss = outputs[0]\n",
    "\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    ground_truth = b_labels.detach().cpu().numpy()\n",
    "    \n",
    "    for l in logits:\n",
    "        logits_test.append(l)\n",
    "    \n",
    "    steps+=1\n",
    "    eval_loss+=loss.detach().item()\n",
    "    for i in ids:\n",
    "        ids_test.append(i)\n",
    "\n",
    "    for p in logits:\n",
    "        if classification:\n",
    "            pred = p.argmax()\n",
    "        else:\n",
    "            pred = round_regression(p)\n",
    "        predictions.append(pred)\n",
    "\n",
    "    for gt in ground_truth:\n",
    "        labels.append(gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f51fd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_test = np.array(logits_test).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331ab217",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame()\n",
    "new_df['predictions'] = logits_test\n",
    "new_df['ids'] = test_ids\n",
    "#new_df.to_csv('./results/task1_cv'+str(k)+'_classification_logits.csv', index=False)\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7db87029",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predictions</th>\n",
       "      <th>ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>1</td>\n",
       "      <td>2345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>0</td>\n",
       "      <td>2346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>0</td>\n",
       "      <td>2347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>1</td>\n",
       "      <td>2348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>0</td>\n",
       "      <td>2349</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>266 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     predictions   ids\n",
       "0              0  2084\n",
       "1              0  2085\n",
       "2              0  2086\n",
       "3              0  2087\n",
       "4              0  2088\n",
       "..           ...   ...\n",
       "261            1  2345\n",
       "262            0  2346\n",
       "263            0  2347\n",
       "264            1  2348\n",
       "265            0  2349\n",
       "\n",
       "[266 rows x 2 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "df['predictions'] = predictions\n",
    "df['ids'] = test_ids\n",
    "df.to_csv('./results/test_roberta-base-task-1.csv', index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5453e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.matthews_corrcoef(labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d82cec6",
   "metadata": {},
   "source": [
    "roberta-base-ne-nh-ct-cw 'wuhan virus'\n",
    "\n",
    "    CV0\n",
    "\t Eval MCC: 0.5740488728880938\n",
    "     Eval MCCNC: 0.579407994265013\n",
    "    CV1\n",
    "     Eval MCC: 0.\n",
    "     Eval MCCNC: 0.\n",
    "    CV2\n",
    "     Eval MCC: 0.\n",
    "     Eval MCCNC: 0.\n",
    "    CV3\n",
    "     Eval MCC: 0.\n",
    "     Eval MCCNC: 0.\n",
    "    CV4\n",
    "     Eval MCC: 0.\n",
    "     Eval MCCNC:0.\n",
    "\n",
    "roberta-base-ne-nh-ct-cw 'virus'\n",
    "\n",
    "    CV0\n",
    "\t Eval MCC: 0.5332947060791403\n",
    "     Eval MCCNC: 0.5944560919312555\n",
    "    CV1\n",
    "     Eval MCC: 0.5620520778928666\n",
    "     Eval MCCNC: 0.5831323068470113\n",
    "    CV2\n",
    "     Eval MCC: 0.6012749273582828\n",
    "     Eval MCCNC: 0.6206057406496038\n",
    "    CV3\n",
    "     Eval MCC: 0.5222413217451917\n",
    "     Eval MCCNC: 0.5719432011660591\n",
    "    CV4\n",
    "     Eval MCC: 0.5989481675690451\n",
    "     Eval MCCNC:0.5938504472685806\n",
    "\n",
    "roberta-base-vanilla-class-weight 0.5793202762478306\n",
    "\n",
    "    CV0\n",
    "\t Eval MCC: 0.6190765567385532\n",
    "     Eval MCCNC: 0.639980680629551\n",
    "    CV1\n",
    "     Eval MCC: 0.545472008421111\n",
    "     Eval MCCNC: \n",
    "    CV2\n",
    "     Eval MCC: 0.5883489914422874\n",
    "     Eval MCCNC: \n",
    "    CV3\n",
    "     Eval MCC: 0.5674449372721372\n",
    "     Eval MCCNC: \n",
    "    CV4\n",
    "     Eval MCC: 0.5762588873650638\n",
    "     Eval MCCNC:\n",
    "     \n",
    "roberta-base-no-emoji-no-hashtag-class-weight 0.5893798440875521\n",
    "\n",
    "    CV0\n",
    "     Eval MCC: 0.6076174020457984\n",
    "     Eval MCCNC: 0.6541683851583311\n",
    "    CV1\n",
    "     Eval MCC: 0.5960691272772399\n",
    "     Eval MCCNC: 0.6340402111743569\n",
    "    CV2\n",
    "     Eval MCC: 0.5966549455749154\n",
    "     Eval MCCNC: 0.5973813731167115\n",
    "    CV3\n",
    "     Eval MCC: 0.5510596772332308\n",
    "     Eval MCCNC: 0.5960364015836999\n",
    "    CV4\n",
    "     Eval MCC: 0.5954980683065756\n",
    "     Eval MCCNC: 0.6116070966482322\n",
    "\n",
    "\n",
    "roberta-base-vanilla 0.57378345911147\n",
    "\n",
    "    CV0\n",
    "\t Eval MCC: 0.6084180921347816  \n",
    "    CV1\n",
    "     Eval MCC: 0.5437470927733514\n",
    "    CV2\n",
    "     Eval MCC: 0.5766356923610586\n",
    "    CV3\n",
    "     Eval MCC: 0.5731702378126259\n",
    "    CV4\n",
    "     Eval MCC: 0.5669461804755326\n",
    "     \n",
    "roberta-base-no-emoji-no-hashtag 0.5883719640784448\n",
    "\n",
    "    CV0\n",
    "     Eval MCC: 0.6390250009224209\n",
    "    CV1\n",
    "     Eval MCC: 0.5899831605096687\n",
    "    CV2\n",
    "     Eval MCC: 0.5858960327082627\n",
    "    CV3\n",
    "     Eval MCC: 0.548088239881684\n",
    "    CV4\n",
    "     Eval MCC: 0.5788673863701883\n",
    "     \n",
    "     \n",
    "bert-base-cased-vanilla 0.5057326905271007\n",
    "\n",
    "    CV0\n",
    "     Eval MCC: 0.5392477883359744\n",
    "    CV1\n",
    "     Eval MCC: 0.4885165440023958\n",
    "    CV2\n",
    "     Eval MCC: 0.5200765630862544\n",
    "    CV3\n",
    "     Eval MCC: 0.4927722558210588\n",
    "    CV4\n",
    "     Eval MCC: 0.4880503013898194\n",
    "\n",
    "bert-base-cased-no-emoji-no-hashtag 0.5230331533462728\n",
    "\n",
    "    CV0\n",
    "     Eval MCC: 0.5050845125400772\n",
    "    CV1\n",
    "     Eval MCC: 0.5376159644130017\n",
    "    CV2\n",
    "     Eval MCC: 0.5427484098543526\n",
    "    CV3\n",
    "     Eval MCC: 0.5412526572334021\n",
    "    CV4\n",
    "     Eval MCC: 0.48846422269053064\n",
    "     \n",
    "     \n",
    "xlnet-base-cased-vanilla\n",
    "\n",
    "     CV0\n",
    "     Eval MCC: 0.6195502429843106\n",
    "    CV1\n",
    "     Eval MCC: 0.5044488707848376\n",
    "    CV2\n",
    "     Eval MCC: 0.5314066396429202\n",
    "    CV3\n",
    "     Eval MCC: 0.\n",
    "    CV4\n",
    "     Eval MCC: 0.\n",
    "     \n",
    "xlnet-base-cased-no-emoji-no-hashtag\n",
    "\n",
    "     CV0\n",
    "     Eval MCC: 0.6195502429843106\n",
    "    CV1\n",
    "     Eval MCC: 0.5044488707848376\n",
    "    CV2\n",
    "     Eval MCC: 0.\n",
    "    CV3\n",
    "     Eval MCC: 0.\n",
    "    CV4\n",
    "     Eval MCC: 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0abd4b",
   "metadata": {},
   "source": [
    "bert-base-cased\n",
    "\n",
    "\n",
    "    base\n",
    "     Eval loss: 0.6526233583688736\n",
    "\t Eval ACC: 0.7459807073954984\n",
    "\t Eval MCC: 0.5900214539385128\n",
    "    no emoji\n",
    "     Eval loss: 0.968928873538971\n",
    "\t Eval ACC: 0.5466237942122186\n",
    "\t Eval MCC: 0.24079659809795295\n",
    "    no hashtags\n",
    "     Eval loss: 0.9618231594562531\n",
    "\t Eval ACC: 0.7234726688102894\n",
    "\t Eval MCC: 0.5657135427683486\n",
    "    no hashtags no emoji\n",
    "     Eval loss: 0.8077533900737762\n",
    "\t Eval ACC: 0.6945337620578779\n",
    "\t Eval MCC: 0.5238978369594125\n",
    "    \n",
    "roberta-base\n",
    "\n",
    "\n",
    "    base\n",
    "     Eval loss: 0.8901744425296784\n",
    "\t Eval ACC: 0.7588424437299035\n",
    "\t Eval MCC: 0.6024912221714718\n",
    "    no emoji\n",
    "     Eval loss: 0.7809186726808548\n",
    "\t Eval ACC: 0.7813504823151125\n",
    "\t Eval MCC: 0.6390617615865098\n",
    "    no hashtags\n",
    "     Eval loss: 0.6772743284702301\n",
    "\t Eval ACC: 0.7652733118971061\n",
    "\t Eval MCC: 0.6226662582115678\n",
    "    no emojis no hashtags\n",
    "     Eval loss: 0.5915229886770248\n",
    "\t Eval ACC: 0.797427652733119\n",
    "\t Eval MCC: 0.6662196769335682\n",
    "\n",
    "roberta-large\n",
    "\n",
    "\n",
    "    base\n",
    "     Eval loss: 0.760116397197215\n",
    "\t Eval ACC: 0.7877813504823151\n",
    "\t Eval MCC: 0.6490270731941167\n",
    "    no emoji\n",
    "     Eval loss: 0.7919591816428763\n",
    "\t Eval ACC: 0.77491961414791\n",
    "\t Eval MCC: 0.6268532212797902\n",
    "    no hashtag\n",
    "     Eval loss: 0.8466774371918291\n",
    "\t Eval ACC: 0.7813504823151125\n",
    "\t Eval MCC: 0.6460687676205381\n",
    "    no emojis no hashtags\n",
    "     Eval loss: 0.7980419032526418\n",
    "\t Eval ACC: 0.77491961414791\n",
    "\t Eval MCC: 0.6383215368737196"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343e37d9",
   "metadata": {},
   "source": [
    "BERT:\n",
    "     Eval loss: 0.658161050081253\n",
    "\t Eval ACC: 0.7395498392282959\n",
    "\t Eval MCC: 0.5756176863338942\n",
    "\n",
    "BERT lowercase:\n",
    "     Eval loss: 0.8212587475776673\n",
    "\t Eval ACC: 0.729903536977492\n",
    "\t Eval MCC: 0.5510256976170747\n",
    "\n",
    "BERT lowercase no stopwords\n",
    "     Eval loss: 0.8212587475776673\n",
    "\t Eval ACC: 0.729903536977492\n",
    "\t Eval MCC: 0.5510256976170747\n",
    "     \n",
    "BERT no hash:\n",
    "     Eval loss: 1.0671971201896668\n",
    "\t Eval ACC: 0.6945337620578779\n",
    "\t Eval MCC: 0.5159577158053311\n",
    "\n",
    "BERT no hash no emoji\n",
    "\t Eval loss: 0.7151306211948395\n",
    "\t Eval ACC: 0.7395498392282959\n",
    "\t Eval MCC: 0.568296103521001\n",
    "     \n",
    "BERT no emoji:\n",
    "     Eval loss: 1.023846310377121\n",
    "\t Eval ACC: 0.49517684887459806\n",
    "\t Eval MCC: 0.0\n",
    "\n",
    "BERT uncased lowercase no emoji:\n",
    "     Eval loss: 0.712954780459404\n",
    "\t Eval ACC: 0.7620578778135049\n",
    "\t Eval MCC: 0.6083864539148676\n",
    "     \n",
    "BERT regression:\n",
    "     Eval loss: 0.5960814729332924\n",
    "\t Eval ACC: 0.594855305466238\n",
    "\t Eval MCC: 0.37905830818150554\n",
    "\n",
    "BERT uncased lowercase:\n",
    "     Eval loss: 0.725039142370224\n",
    "\t Eval ACC: 0.7009646302250804\n",
    "\t Eval MCC: 0.5038352644951515\n",
    "     \n",
    "Roberta base:\n",
    "     Eval loss: 0.6113575458526611\n",
    "\t Eval ACC: 0.7813504823151125\n",
    "\t Eval MCC: 0.6395105003752871\n",
    "     \n",
    "Roberta no emoji:\n",
    "     Eval loss: 0.6168820470571518\n",
    "\t Eval ACC: 0.7877813504823151\n",
    "\t Eval MCC: 0.6489923604459321\n",
    "     \n",
    "Roberta lowercase no emoji:\n",
    "     Eval loss: 0.7398509830236435\n",
    "\t Eval ACC: 0.752411575562701\n",
    "\t Eval MCC: 0.5939735163517177\n",
    "     \n",
    "Roberta large:\n",
    "     Eval loss: 0.6709848251193762\n",
    "\t Eval ACC: 0.7684887459807074\n",
    "\t Eval MCC: 0.6170674668532852\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e35ce0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
