{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "782fabe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\" # the GPU on robinson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "358b7706",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import deque\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertForPreTraining\n",
    "from transformers import BertModel, AdamW, AutoTokenizer, BertForSequenceClassification, RobertaForSequenceClassification\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import emoji\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "random_seed = 0\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "conspiracies = ['Suppressed Cures',\n",
    "     'Behaviour and Mind Control',\n",
    "     'Antivax',\n",
    "     'Fake virus',\n",
    "     'Intentional Pandemic',\n",
    "     'Harmful Radiation/ Influence',\n",
    "     'Population reduction',\n",
    "     'New World Order',\n",
    "     'Satanism']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04bbfb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_name = 'bert-base-cased'\n",
    "# model_name = 'roberta-base'\n",
    "# model_name = 'covid-twitter-bert'\n",
    "model_name = 'roberta-large'\n",
    "\n",
    "replace_lowercase_flag = False\n",
    "remove_stopwords_flag = False\n",
    "remove_hashtags_flag = True\n",
    "replace_emojis_flag = True\n",
    "clean_tweets_flag = False\n",
    "\n",
    "all_data = False\n",
    "class_weights_flag = True # always true for now\n",
    "\n",
    "classification = True\n",
    "\n",
    "# fold\n",
    "k=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2abe94ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweets(tweets):\n",
    "    char_to_remove = ['\\n', '\\xa0']\n",
    "    corona_synonyms = ['coronavirus',\n",
    "                      'covid-19',\n",
    "                      'covid19',\n",
    "                      'covid 19',\n",
    "                      'covid',\n",
    "                      'corona',\n",
    "                      'sarscov2'\n",
    "                      'sars',\n",
    "                      'Coronaviruses',\n",
    "                      'Coronavirus',\n",
    "                      'Corona',\n",
    "                      'Covid19',\n",
    "                      'COVID19',\n",
    "                      'Covid-19',\n",
    "                      'COVID-19',\n",
    "                      'COVID 19',\n",
    "                      'Covid',\n",
    "                      'COVID',\n",
    "                      'SARSCOV2',\n",
    "                      'SARS']\n",
    "    \n",
    "    tweets_clean = []\n",
    "    for tw in tweets:\n",
    "        for c in char_to_remove:\n",
    "            tw = tw.replace(c, '')\n",
    "        tw = tw.replace('&amp;', '&')\n",
    "        \n",
    "        for syn in corona_synonyms:\n",
    "            if syn in tw:\n",
    "                tw = tw.replace(syn, 'wuhan virus')\n",
    "        tweets_clean.append(tw)\n",
    "    return tweets_clean\n",
    "\n",
    "def extract_hashtags(tweet):\n",
    "    # Returns hashtags in a list for a given tweet\n",
    "    \n",
    "    #tweet = tweet.replace('\\xa0','')\n",
    "    #tweet = tweet.replace('\\n','')\n",
    "    \n",
    "    tweet_words = tweet.split(' ')\n",
    "    tweet_words = [w for w in tweet_words if w!='']\n",
    "    hashtags = []\n",
    "    for word in tweet_words:\n",
    "        if word[0]=='#':\n",
    "            hashtags.append(word)\n",
    "    return hashtags\n",
    "\n",
    "def extract_emojis (tw):\n",
    "    # Returns emojis in a list for a given tweet\n",
    "    # Using Deque for a sliding window (emojis can be combined together to form other emojis)\n",
    "    \n",
    "    emojis = []\n",
    "    \n",
    "    l = []\n",
    "    max_l = 7\n",
    "    \n",
    "    for i in range(0, max_l):\n",
    "        l.append(tw[-1-i])\n",
    "    l = deque(l, maxlen=max_l)\n",
    "    skip=0\n",
    "    \n",
    "    for i in range (0, len(tw)):\n",
    "        if skip == 0:\n",
    "            for j in range (max_l-1, -1, -1):\n",
    "                str_to_test = ''\n",
    "                for k in range (0, j+1):\n",
    "                    str_to_test+=l[j-k]\n",
    "                if str_to_test in emoji.UNICODE_EMOJI['en']:\n",
    "                    \n",
    "                    emojis.append(str_to_test)\n",
    "                    skip=j\n",
    "                    break\n",
    "            try:\n",
    "                l.append(tw[-1-i-max_l])\n",
    "            except IndexError:\n",
    "                l.append('')\n",
    "        else:\n",
    "            skip=skip-1\n",
    "            try:\n",
    "                l.append(tw[-1-i-max_l])\n",
    "            except IndexError:\n",
    "                l.append('')\n",
    "    emojis.reverse()\n",
    "    return emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a03dccbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_lowercase(tweets):\n",
    "    tweets_lowercase = []\n",
    "    for tw in tweets:\n",
    "        tweets_lowercase.append(tw.lower())\n",
    "    return tweets_lowercase\n",
    "\n",
    "def remove_stopwords(tweets):\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    tweets_no_stopwords = []\n",
    "    for tw in tweets:\n",
    "        tw = tw.split(' ')\n",
    "        tweets_no_stopwords.append(' '.join([word for word in tw if not word in stop_words]))\n",
    "\n",
    "    return tweets_no_stopwords\n",
    "\n",
    "def remove_hashtags(tweets):\n",
    "    tweets = [tw.replace('#', '') for tw in tweets]\n",
    "    return tweets\n",
    "\n",
    "def replace_emojis(tweets):\n",
    "    tweets_no_emojis = []\n",
    "    for tw in tweets:\n",
    "        emojis = extract_emojis(tw)\n",
    "        for e in emojis:\n",
    "            e_text = emoji.UNICODE_EMOJI['en'][e].replace('_',' ').replace(':', '')\n",
    "            tw = tw.replace(e, e_text)\n",
    "        tweets_no_emojis.append(tw)\n",
    "\n",
    "    return tweets_no_emojis\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0c19f33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([44.3929, 10.9035,  8.1242,  6.8297,  6.7189, 27.0217, 11.3000, 12.4300,\n",
       "        19.4219], device='cuda:0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = './mediaeval-fakenews/data/task2/'\n",
    "filelist = os.listdir(data_path)\n",
    "\n",
    "\n",
    "df_list = [pd.read_csv(data_path+file) for file in filelist]\n",
    "\n",
    "\n",
    "test_df = df_list[k]    \n",
    "train_df = pd.concat(df_list[:k]+df_list[k+1:])\n",
    "\n",
    "\n",
    "tw_train = train_df['tweet'].tolist()\n",
    "tw_test = test_df['tweet'].tolist()\n",
    "\n",
    "if all_data:\n",
    "    df = pd.read_csv('./mediaeval-fakenews/data/dev-full-task-2-clean.csv')\n",
    "    tw_train = df['tweet']\n",
    "    labels_train = df.iloc[:,1:10].values.tolist()\n",
    "\n",
    "if clean_tweets_flag:\n",
    "    tw_train = clean_tweets(tw_train)\n",
    "    tw_test = clean_tweets(tw_test)\n",
    "\n",
    "if replace_lowercase_flag:\n",
    "    tw_train = to_lowercase(tw_train)\n",
    "    tw_test = to_lowercase(tw_test)\n",
    "\n",
    "if remove_stopwords_flag:\n",
    "    tw_train = remove_stopwords(tw_train)\n",
    "    tw_test = remove_stopwords(tw_test)\n",
    "\n",
    "if remove_hashtags_flag:\n",
    "    tw_train = remove_hashtags(tw_train)\n",
    "    tw_test = remove_hashtags(tw_test)\n",
    "\n",
    "if replace_emojis_flag:\n",
    "    tw_train = replace_emojis(tw_train)\n",
    "    tw_test = replace_emojis(tw_test)\n",
    "\n",
    "weights_tmp = [0,0,0,0,0,0,0,0,0]\n",
    "weights = [1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "if not all_data:\n",
    "    labels_train = train_df.iloc[:,1:10].values.tolist()\n",
    "labels_test = test_df.iloc[:,1:10].values.tolist()\n",
    "ids_test = test_df['ids'].tolist()\n",
    "\n",
    "for i in range(0, 9):\n",
    "    for j in range(0, len(labels_train)):\n",
    "        weights_tmp[i]+=labels_train[j][i]\n",
    "        \n",
    "weights = [len(labels_train)/w for w in weights_tmp]\n",
    "#weights = [1/w for w in weights_tmp]\n",
    "\n",
    "weights = torch.FloatTensor(weights).cuda()\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32a72ced",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if 'twitter' in model_name:\n",
    "    tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenized_input = tokenizer(tw_train)\n",
    "\n",
    "m = 0\n",
    "for tokens in tokenized_input['input_ids']:\n",
    "    if len(tokens)>m:\n",
    "        m=len(tokens)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbb366dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 128\n",
    "\n",
    "tokenized_input = tokenizer(tw_train, max_length=MAX_LEN, padding='max_length', truncation=True)\n",
    "tokenized_test = tokenizer(tw_test, max_length=MAX_LEN, padding='max_length', truncation=True)\n",
    "\n",
    "\n",
    "if 'roberta' in model_name:\n",
    "    train_input_ids, train_attention_mask = tokenized_input['input_ids'], tokenized_input['attention_mask']\n",
    "    test_input_ids, test_attention_mask = tokenized_test['input_ids'], tokenized_test['attention_mask']\n",
    "    \n",
    "    \n",
    "else:\n",
    "    train_input_ids, train_token_type_ids, train_attention_mask = tokenized_input['input_ids'], tokenized_input['token_type_ids'], tokenized_input['attention_mask']\n",
    "    test_input_ids, test_token_type_ids, test_attention_mask = tokenized_test['input_ids'], tokenized_test['token_type_ids'], tokenized_test['attention_mask']\n",
    "\n",
    "    train_token_type_ids = torch.tensor(train_token_type_ids)\n",
    "    test_token_type_ids = torch.tensor(test_token_type_ids)\n",
    "    \n",
    "    \n",
    "train_labels = labels_train\n",
    "test_labels = labels_test\n",
    "\n",
    "\n",
    "# Convert to torch tensor\n",
    "train_input_ids = torch.tensor(train_input_ids)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "train_attention_mask = torch.tensor(train_attention_mask)\n",
    "\n",
    "test_input_ids = torch.tensor(test_input_ids)\n",
    "test_labels = torch.tensor(test_labels)\n",
    "test_attention_mask = torch.tensor(test_attention_mask)\n",
    "test_ids = torch.tensor(ids_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d922f9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_b_size = {'roberta-base':32,\n",
    "                 'bert-base-cased':32,\n",
    "                 'covid-twitter-bert':6,\n",
    "                 'roberta-large':10}\n",
    "batch_size = models_b_size[model_name] # 32 if 256\n",
    "\n",
    "if 'roberta' in model_name:\n",
    "    train_data = TensorDataset(train_input_ids, train_attention_mask, train_labels)\n",
    "    test_data = TensorDataset(test_input_ids, test_attention_mask, test_labels, test_ids)\n",
    "    \n",
    "else:\n",
    "    train_data = TensorDataset(train_input_ids, train_attention_mask, train_labels, train_token_type_ids)\n",
    "    test_data = TensorDataset(test_input_ids, test_attention_mask, test_labels, test_token_type_ids)\n",
    "\n",
    "    \n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "812d3b98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tesla K80'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0e9de3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_classes):\n",
    "        super().__init__()\n",
    "        self.n_classes = n_classes\n",
    "        #self.bert = BertForPreTraining.from_pretrained('digitalepidemiologylab/covid-twitter-bert-v2')    \n",
    "        self.bert = BertForSequenceClassification.from_pretrained(model_name, num_labels=n_classes)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        #self.classifier = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "        #self.bert.cls.seq_relationship = nn.Linear(1024, n_classes)\n",
    "        \n",
    "        if n_classes >1:\n",
    "            self.criterion = nn.BCELoss()\n",
    "        else:\n",
    "            self.criterion = nn.MSELoss()\n",
    "        \n",
    "    def forward(self, input_ids, token_type_ids, input_mask, labels):\n",
    "        outputs = self.bert(input_ids = input_ids, token_type_ids = token_type_ids, attention_mask = input_mask)\n",
    "        #outputs = self.classifier(outputs.pooler_output)\n",
    "        \n",
    "        logits = self.sigmoid(outputs[0])\n",
    "        \n",
    "        loss = self.criterion(logits, labels)\n",
    "        loss = (loss * weights).mean()\n",
    "        \n",
    "        \n",
    "        return loss, logits\n",
    "\n",
    "class CovidTwitterBertClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_classes):\n",
    "        super().__init__()\n",
    "        self.n_classes = n_classes\n",
    "        self.bert = BertForPreTraining.from_pretrained('digitalepidemiologylab/covid-twitter-bert-v2')    \n",
    "        #self.bert = BertForSequenceClassification.from_pretrained(model_name, num_labels=n_classes)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        #self.classifier = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "        self.bert.cls.seq_relationship = nn.Linear(1024, n_classes)\n",
    "        \n",
    "        if n_classes >1:\n",
    "            self.criterion = nn.BCELoss()\n",
    "        else:\n",
    "            self.criterion = nn.MSELoss()\n",
    "        \n",
    "    def forward(self, input_ids, token_type_ids, input_mask, labels):\n",
    "        outputs = self.bert(input_ids = input_ids, token_type_ids = token_type_ids, attention_mask = input_mask)\n",
    "        #outputs = self.classifier(outputs.pooler_output)\n",
    "        \n",
    "        logits = self.sigmoid(outputs[1])\n",
    "        \n",
    "        loss = self.criterion(logits, labels)\n",
    "        loss = (loss * weights).mean()\n",
    "        \n",
    "        \n",
    "        return loss, logits\n",
    "    \n",
    "    \n",
    "class RobertaClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_classes):\n",
    "        super().__init__()\n",
    "        self.n_classes = n_classes\n",
    "        self.bert = RobertaForSequenceClassification.from_pretrained(model_name, num_labels=n_classes)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        if n_classes >1:\n",
    "            self.criterion = nn.BCELoss(reduction='none')\n",
    "            #self.criterion = nn.BCEWithLogitsLoss()\n",
    "            \n",
    "        else:\n",
    "            self.criterion = nn.MSELoss()\n",
    "        \n",
    "    def forward(self, input_ids, input_mask, labels):\n",
    "        outputs = self.bert(input_ids, input_mask)\n",
    "        #outputs = self.classifier(outputs.pooler_output)\n",
    "        logits = outputs[0]\n",
    "        logits = self.sigmoid(logits)\n",
    "        \n",
    "        if self.n_classes == 1:\n",
    "            labels=labels.float()\n",
    "        loss = self.criterion(logits, labels)\n",
    "        loss = (loss * weights).mean()\n",
    "        \n",
    "        return loss, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28149c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaClassifier(\n",
       "  (bert): RobertaForSequenceClassification(\n",
       "    (roberta): RobertaModel(\n",
       "      (embeddings): RobertaEmbeddings(\n",
       "        (word_embeddings): Embedding(50265, 1024, padding_idx=1)\n",
       "        (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
       "        (token_type_embeddings): Embedding(1, 1024)\n",
       "        (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): RobertaEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (3): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (4): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (5): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (6): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (7): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (8): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (9): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (10): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (11): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (12): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (13): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (14): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (15): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (16): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (17): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (18): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (19): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (20): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (21): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (22): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (23): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (classifier): RobertaClassificationHead(\n",
       "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (out_proj): Linear(in_features=1024, out_features=9, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (sigmoid): Sigmoid()\n",
       "  (criterion): BCELoss()\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if 'roberta' in model_name:\n",
    "    model = RobertaClassifier(9)\n",
    "elif 'twitter' in model_name:\n",
    "    model = CovidTwitterBertClassifier(9)\n",
    "else:\n",
    "    model = BertClassifier(9)\n",
    "    \n",
    "    \n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d39099d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer_grouped_parameters\n",
    "# lr 5e-5 for base models\n",
    "# lr 7e-6 for larger models\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr=7e-6,\n",
    "                  weight_decay = 0.001)\n",
    "\n",
    "scheduler = ReduceLROnPlateau(optimizer, patience=4, factor=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d29ce0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_regression(val):\n",
    "    if val<0.5:\n",
    "        return 0\n",
    "    elif val<1.5:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b67f71",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   0%|                                                 | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 4.15667368221283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   3%|                                    | 1/30 [03:34<1:43:28, 214.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Eval loss: 3.38576590269804\n",
      "\t Eval ACC: 0.4855305466237942\n",
      "\t Eval MCCA: 0.0\n",
      "\t Eval MCCF: 0.0\n",
      "\t Eval MCCs: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "\t Eval MCC 1 vs other: 0.0\n",
      "\t Eval F1 weighted: 0.0\n",
      "Train loss: 3.2818857460021973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   7%|                                   | 2/30 [07:08<1:39:54, 214.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Eval loss: 2.694883955642581\n",
      "\t Eval ACC: 0.49517684887459806\n",
      "\t Eval MCCA: 0.16344438884748153\n",
      "\t Eval MCCF: 0.25266826996359093\n",
      "\t Eval MCCs: [0.0, 0.25577021009073103, 0.40414003022006, 0.0, 0.0, 0.8110892593165426, 0.0, 0.0, 0.0]\n",
      "\t Eval MCC 1 vs other: 0.23360346365431425\n",
      "\t Eval F1 weighted: 0.10810495286460613\n",
      "Train loss: 2.500204922199249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  10%|                                  | 3/30 [10:41<1:36:13, 213.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Eval loss: 2.13862815964967\n",
      "\t Eval ACC: 0.5530546623794212\n",
      "\t Eval MCCA: 0.4078677103884172\n",
      "\t Eval MCCF: 0.5048323020601615\n",
      "\t Eval MCCs: [0.0, 0.7333542672347239, 0.5197391466671093, 0.0, 0.0, 0.9098330807202929, 0.7856140711454555, 0.7222688277281736, 0.0]\n",
      "\t Eval MCC 1 vs other: 0.4412783365849191\n",
      "\t Eval F1 weighted: 0.3724528034800572\n",
      "Train loss: 1.882095663547516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  13%|                                 | 4/30 [14:15<1:32:37, 213.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Eval loss: 1.8714201068505645\n",
      "\t Eval ACC: 0.5852090032154341\n",
      "\t Eval MCCA: 0.5162407828255708\n",
      "\t Eval MCCF: 0.5446122067370258\n",
      "\t Eval MCCs: [0.5301802308720894, 0.7659710913812849, 0.5581446400094104, 0.0, 0.0, 0.8151982686453727, 0.8970672676222476, 0.72222082374386, 0.3573847231558724]\n",
      "\t Eval MCC 1 vs other: 0.4977314829734596\n",
      "\t Eval F1 weighted: 0.4139253603086656\n"
     ]
    }
   ],
   "source": [
    "epochs = 30\n",
    "\n",
    "threshold = 0.5\n",
    "\n",
    "best_MCCF = 0\n",
    "best_MCCA = 0\n",
    "best_F1 = 0\n",
    "best_MCCs = []\n",
    "best_MCCNC = 0\n",
    "best_loss = 999\n",
    "best_acc = 0\n",
    "best_state_dict = model.state_dict()\n",
    "best_epoch = 0\n",
    "\n",
    "for e in trange(epochs, desc=\"Epoch\"):\n",
    "\n",
    "    # Training\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        if 'roberta' in model_name:\n",
    "            b_input_ids, b_input_mask, b_labels = batch\n",
    "        else:    \n",
    "            b_input_ids, b_input_mask, b_labels, b_token_type_ids = batch\n",
    "            \n",
    "        if not classification:\n",
    "            b_labels = b_labels.view(-1, 1)        \n",
    "        \n",
    "        b_labels = b_labels.float()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if 'roberta' in model_name:\n",
    "            outputs = model(b_input_ids, b_input_mask, b_labels)\n",
    "        else:\n",
    "            outputs = model(b_input_ids, b_token_type_ids, b_input_mask, b_labels)\n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "        #print(step, loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "\n",
    "    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "    \n",
    "    \n",
    "    # Testing\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    tweets_test = []\n",
    "    \n",
    "    predictions = []\n",
    "    predictions_sep = [[], [], [], [], [], [], [], [], []]\n",
    "    \n",
    "    labels = []\n",
    "    labels_sep = [[], [], [], [], [], [], [], [], []]\n",
    "    \n",
    "    eval_loss = 0\n",
    "    steps=0\n",
    "    # Train the data for one epoch\n",
    "    for step, batch in enumerate(test_dataloader):\n",
    "\n",
    "        # Add batch to GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        if 'roberta' in model_name:\n",
    "            b_input_ids, b_input_mask, b_labels, ids = batch\n",
    "        else:    \n",
    "            b_input_ids, b_input_mask, b_labels, b_token_type_ids = batch\n",
    "            \n",
    "        if not classification:\n",
    "            b_labels = b_labels.view(-1, 1)        \n",
    "        \n",
    "        b_labels = b_labels.float()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "\n",
    "            if 'roberta' in model_name:\n",
    "                outputs = model(b_input_ids, b_input_mask, b_labels)\n",
    "            else:\n",
    "                outputs = model(b_input_ids, b_token_type_ids, b_input_mask, b_labels)\n",
    "            logits = outputs[1]\n",
    "            loss = outputs[0]\n",
    "\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        ground_truth = b_labels.detach().cpu().numpy()\n",
    "        \n",
    "        steps+=1\n",
    "        eval_loss+=loss.detach().item()\n",
    "        \n",
    "        tweets_test.append(b_input_ids)\n",
    "        for p in logits:\n",
    "            if classification:\n",
    "                pred = p.argmax()\n",
    "            else:\n",
    "                pred = round_regression(p)\n",
    "            predictions.append(p>threshold)\n",
    "            for i in range(0, 9):\n",
    "                predictions_sep[i].append(p[i]>threshold)\n",
    "            \n",
    "        for gt in ground_truth:\n",
    "            labels.append(gt>threshold)\n",
    "            for i in range(0, 9):\n",
    "                labels_sep[i].append(gt[i]>threshold)\n",
    "        \n",
    "    MCCs = []\n",
    "    for i in range(0, 9):\n",
    "        MCCs.append(metrics.matthews_corrcoef(labels_sep[i], predictions_sep[i]))\n",
    "    labels_one = []\n",
    "    predictions_one = []\n",
    "    for l in labels:\n",
    "        if list(l) == [False, False, False, False, False, False, False, False, False]:\n",
    "            labels_one.append(0)\n",
    "        else:\n",
    "            labels_one.append(1)\n",
    "    for p in predictions:\n",
    "        if list(p) == [False, False, False, False, False, False, False, False, False]:\n",
    "            predictions_one.append(0)\n",
    "        else:\n",
    "            predictions_one.append(1)\n",
    "    \n",
    "    \n",
    "    scheduler.step(eval_loss/steps)\n",
    "    MCCF = metrics.matthews_corrcoef(np.array(labels).flatten(), np.array(predictions).flatten())\n",
    "    ACC = metrics.accuracy_score(labels, predictions)\n",
    "    LOSS = eval_loss/steps\n",
    "    MCCNC = metrics.matthews_corrcoef(labels_one, predictions_one)\n",
    "    F1 = metrics.f1_score(labels, predictions, average='weighted')\n",
    "    MCCA = np.array(MCCs).mean()\n",
    "    if MCCA> best_MCCA:\n",
    "        best_MCCF = MCCF\n",
    "        best_MCCA = MCCA\n",
    "        best_loss = LOSS\n",
    "        best_acc = ACC\n",
    "        best_F1 = F1\n",
    "        best_MCCs = MCCs\n",
    "        best_MCCNC = MCCNC\n",
    "        best_state_dict = copy.deepcopy(model.state_dict())\n",
    "        best_epoch = e\n",
    "    \n",
    "    print(\"\\t Eval loss: {}\".format(LOSS))\n",
    "    print(\"\\t Eval ACC: {}\".format(ACC))\n",
    "    print(\"\\t Eval MCCA: {}\".format(MCCA))\n",
    "    print(\"\\t Eval MCCF: {}\".format(MCCF))\n",
    "    print(\"\\t Eval MCCs: {}\".format(MCCs))\n",
    "    print(\"\\t Eval MCC 1 vs other: {}\".format(MCCNC))\n",
    "    #print(\"\\t Eval Kappa: {}\".format(metrics.cohen_kappa_score(np.array(labels).flatten(), np.array(predictions).flatten())))\n",
    "    print(\"\\t Eval F1 weighted: {}\".format(F1))\n",
    "    #print(\"\\t Eval F1 micro: {}\".format(metrics.f1_score(labels, predictions, average='micro')))\n",
    "    #print(\"\\t Eval F1 samples: {}\".format(metrics.f1_score(labels, predictions, average='samples')))\n",
    "    #print(\"\\t Eval F1 None: {}\".format(metrics.f1_score(labels, predictions, average=None)))\n",
    "    \n",
    "    #print([predictions.count(i) for i in range(0,3)], [labels.count(i) for i in range(0, 3)])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e5e0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best epoch ', best_epoch)\n",
    "print(\"\\t Eval loss: {}\".format(best_loss))\n",
    "print(\"\\t Eval ACC: {}\".format(best_acc))\n",
    "print(\"\\t Eval MCCA: {}\".format(best_MCCA))\n",
    "print(\"\\t Eval MCCF: {}\".format(best_MCCF))\n",
    "print(\"\\t Eval MCCs: {}\".format(best_MCCs))\n",
    "print(\"\\t Eval MCC 1 vs other: {}\".format(best_MCCNC))\n",
    "print(\"\\t Eval F1 weighted: {}\".format(best_F1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0badba77",
   "metadata": {},
   "outputs": [],
   "source": [
    "raise error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ea48b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(best_state_dict, './Models/task2/roberta-base-all-train.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339dccf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9247f109",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ./Models/task2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877720ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./mediaeval-fakenews/data/test-clean.csv')\n",
    "ids_test = df['ids']\n",
    "tw_test = df['tweet']\n",
    "\n",
    "if clean_tweets_flag:\n",
    "    tw_test = clean_tweets(tw_test)\n",
    "\n",
    "if replace_lowercase_flag:\n",
    "    tw_test = to_lowercase(tw_test)\n",
    "\n",
    "if remove_stopwords_flag:\n",
    "    tw_test = remove_stopwords(tw_test)\n",
    "\n",
    "if remove_hashtags_flag:\n",
    "    tw_test = remove_hashtags(tw_test)\n",
    "\n",
    "if replace_emojis_flag:\n",
    "    tw_test = replace_emojis(tw_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ace5f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_test = tokenizer(tw_test, max_length=MAX_LEN, padding='max_length', truncation=True)\n",
    "test_input_ids, test_attention_mask = tokenized_test['input_ids'], tokenized_test['attention_mask']\n",
    "\n",
    "test_labels = []\n",
    "for i in range(0, len(ids_test)):\n",
    "    test_labels.append([1,1,1,1,1,1,1,1,1])\n",
    "\n",
    "test_input_ids = torch.tensor(test_input_ids)\n",
    "test_attention_mask = torch.tensor(test_attention_mask)\n",
    "test_labels = torch.tensor(test_labels)\n",
    "test_ids = torch.tensor(ids_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666a26f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 70 # 32 if 256\n",
    "\n",
    "if 'roberta' in model_name:\n",
    "    test_data = TensorDataset(test_input_ids, test_attention_mask, test_labels, test_ids)\n",
    "    \n",
    "else:\n",
    "    test_data = TensorDataset(test_input_ids, test_attention_mask, test_token_type_ids)\n",
    "\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a5d382",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.load_state_dict(torch.load('./Models/task2/'+model_name+'_CV'+str(k)+'_e'+str(best_epoch)+'_'+str(round(best_MCCA, 3))+'.pth'))\n",
    "model.load_state_dict(torch.load('./Models/task2/roberta-base-all-train.pth'))\n",
    "model.eval()\n",
    "    \n",
    "tweets_test = []\n",
    "ids_test = []\n",
    "\n",
    "predictions = []\n",
    "predictions_sep = [[], [], [], [], [], [], [], [], []]\n",
    "\n",
    "labels = []\n",
    "labels_sep = [[], [], [], [], [], [], [], [], []]\n",
    "\n",
    "logits_test = []\n",
    "\n",
    "eval_loss = 0\n",
    "steps=0\n",
    "# Train the data for one epoch\n",
    "for step, batch in enumerate(test_dataloader):\n",
    "\n",
    "    # Add batch to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "    if 'roberta' in model_name:\n",
    "        b_input_ids, b_input_mask, b_labels, ids = batch\n",
    "    else:    \n",
    "        b_input_ids, b_input_mask, b_labels, b_token_type_ids = batch\n",
    "\n",
    "    if not classification:\n",
    "        b_labels = b_labels.view(-1, 1)        \n",
    "\n",
    "    b_labels = b_labels.float()\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        if 'roberta' in model_name:\n",
    "            outputs = model(b_input_ids, b_input_mask, b_labels)\n",
    "        else:\n",
    "            outputs = model(b_input_ids, b_token_type_ids, b_input_mask, b_labels)\n",
    "        logits = outputs[1]\n",
    "        loss = outputs[0]\n",
    "\n",
    "\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    ground_truth = b_labels.detach().cpu().numpy()\n",
    "\n",
    "    for l in logits:\n",
    "        logits_test.append(l)\n",
    "    \n",
    "    steps+=1\n",
    "    eval_loss+=loss.detach().item()\n",
    "    for i in ids:\n",
    "        ids_test.append(i)\n",
    "    for tw in b_input_ids:\n",
    "        tweets_test.append(tokenizer.decode(tw))\n",
    "    for p in logits:\n",
    "        if classification:\n",
    "            pred = p.argmax()\n",
    "        else:\n",
    "            pred = round_regression(p)\n",
    "        predictions.append(p>threshold)\n",
    "        for i in range(0, 9):\n",
    "            predictions_sep[i].append(p[i]>threshold)\n",
    "\n",
    "    for gt in ground_truth:\n",
    "        labels.append(gt>threshold)\n",
    "        for i in range(0, 9):\n",
    "            labels_sep[i].append(gt[i]>threshold)\n",
    "\n",
    "MCCs = []\n",
    "for i in range(0, 9):\n",
    "    MCCs.append(metrics.matthews_corrcoef(labels_sep[i], predictions_sep[i]))\n",
    "labels_one = []\n",
    "predictions_one = []\n",
    "for l in labels:\n",
    "    if list(l) == [False, False, False, False, False, False, False, False, False]:\n",
    "        labels_one.append(0)\n",
    "    else:\n",
    "        labels_one.append(1)\n",
    "for p in predictions:\n",
    "    if list(p) == [False, False, False, False, False, False, False, False, False]:\n",
    "        predictions_one.append(0)\n",
    "    else:\n",
    "        predictions_one.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476fc37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame()\n",
    "new_df['predictions'] = logits_test\n",
    "new_df['ids'] = test_ids\n",
    "#new_df.to_csv('./results/task2_cv'+str(k)+'_logits.csv', index=False)\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944d23bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame()\n",
    "new_df['predictions'] = predictions\n",
    "new_df['ids'] = test_ids\n",
    "#new_df.to_csv('./results/teset-roberta-base-task2.csv', index=False)\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1c679d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs = []\n",
    "for i in range(0, len(tweets_test)):\n",
    "    for j in range(0, 32):\n",
    "        tw = tokenizer.decode(tweets_test[i][j].cpu().numpy())\n",
    "        tw = tw.split('<s>')[1].split('</s>')[0]\n",
    "        test_inputs.append(tw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abad6acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(labels)):\n",
    "    for j in range(0, 9):\n",
    "        if labels[i][j] != predictions[i][j]:\n",
    "            print(conspiracies[j], labels[i][j], test_inputs[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fd657b",
   "metadata": {},
   "outputs": [],
   "source": [
    "CVs = []\n",
    "CVs.append(0.690803972664749)\n",
    "CVs.append(0.5558459007027099)\n",
    "CVs.append(0.6826415214096324)\n",
    "CVs.append(0.7313539869700288)\n",
    "CVs.append(0.7069902423375098)\n",
    "\n",
    "L = []\n",
    "try:\n",
    "    for i in range(0, 9):\n",
    "        print((CVs[0][i]+CVs[1][i]+CVs[2][i]+CVs[3][i]+CVs[4][i])/5)\n",
    "        L.append((CVs[0][i]+CVs[1][i]+CVs[2][i]+CVs[3][i]+CVs[4][i])/5)\n",
    "    print(L)\n",
    "except:\n",
    "    print(sum(CVs)/5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed5a9e5",
   "metadata": {},
   "source": [
    "roberta-base-ne-rh-cw 20 epoch\n",
    "    \n",
    "    MCC: 0.6654098900854347\n",
    "    MCCs: [0.5524150476052554, 0.7679068700779181, 0.7434705970293489, 0.53421306694965, 0.31320778183707854, 0.8738837418030807, 0.827025851500666, 0.8654943602641909, 0.6778128959922297]\n",
    "    MCC 1 vs other: 0.6013349313398135\n",
    "    F1 score: 0.673527124816926\n",
    "\n",
    "    CV0\n",
    "     Eval MCC: 0.6693922816899639\n",
    "\t Eval MCCs: [0.42860050121638743, 0.7252650176678446, 0.7722176144987796, 0.6302199352192639, 0.3234153127511404, 0.9098330807202929, 0.852192697907109, 0.8391181413957557, 0.5878134599824882]\n",
    "\t Eval MCC 1 vs other: 0.5622651298968165\n",
    "\t Eval F1 weighted: 0.690803972664749\n",
    "    \n",
    "    CV1\n",
    "     Eval MCC: 0.6000358463779237\n",
    "\t Eval MCCs: [0.5615601503759399, 0.6576259690552511, 0.7209708270079217, 0.40690082151862383, 0.0797899190540391, 0.7750680390694658, 0.780761990291456, 0.8812900814794339, 0.6483410729958033]\n",
    "\t Eval MCC 1 vs other: 0.5575745033112582\n",
    "\t Eval F1 weighted: 0.5558459007027099\n",
    "    \n",
    "    CV2\n",
    "     Eval MCC: 0.6639509510033542\n",
    "\t Eval MCCs: [0.4515089332178771, 0.7955522730224036, 0.7825896115018466, 0.48974569319114025, 0.3298767698501493, 0.8654490514993884, 0.7978919748135389, 0.8506850935533509, 0.7216546169363538]\n",
    "\t Eval MCC 1 vs other: 0.640555078097185\n",
    "\t Eval F1 weighted: 0.6826415214096324\n",
    "     \n",
    "    CV3\n",
    "     Eval MCC: 0.7026550886410515\n",
    "\t Eval MCCs: [0.6700308105163018, 0.7820634449596706, 0.760169654906497, 0.5701386573495464, 0.5108042035961533, 0.9057575757575758, 0.8485167984719844, 0.8202585452844271, 0.7364406779661017]\n",
    "\t Eval MCC 1 vs other: 0.6333095824234448\n",
    "\t Eval F1 weighted: 0.7313539869700288\n",
    "\n",
    "    CV4\n",
    "     Eval MCC: 0.69101528271488\n",
    "\t Eval MCCs: [0.6503748426997711, 0.8790276456844205, 0.6814052772316999, 0.5740602274696759, 0.3221527039339106, 0.9133109619686801, 0.8557657960192416, 0.9361199396079867, 0.6948146520804012]\n",
    "\t Eval MCC 1 vs other: 0.612970362970363\n",
    "\t Eval F1 weighted: 0.7069902423375098\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72b2395",
   "metadata": {},
   "source": [
    "roberta-base-ne-rh-ct 'virus'\n",
    "\n",
    "    MCC 0.5645707750570181\n",
    "    MCCs [0.0, 0.6924143845779678, 0.7367030570567744, 0.48754798145059786, 0.1396599352715253, 0.6487169994487173, 0.7603298255290551, 0.7285900272338771, 0.0]\n",
    "    MCC 1 vs other: 0.5573428018644087\n",
    "    F1 weighted:0.5060391684024019\n",
    "\n",
    "    CV0\n",
    "     Eval MCC: 0.572002402314867\n",
    "\t Eval MCCs: [0.0, 0.7171500972418344, 0.6778292495719211, 0.537598504095889, 0.1441954356128429, 0.8110892593165426, 0.8436782760095076, 0.6018584254641897, 0.0]\n",
    "\t Eval MCC 1 vs other: 0.506613144282354\n",
    "\t Eval F1 weighted: 0.5183449582118178\n",
    "     \n",
    "    CV1\n",
    "     Eval MCC: 0.5675140765889306\n",
    "\t Eval MCCs: [0.0, 0.6700157588234843, 0.7133200840083925, 0.4721250856617546, 0.19309889086137313, 0.6059156682132061, 0.8122992568299051, 0.7928260719998715, 0.0]\n",
    "\t Eval MCC 1 vs other: 0.5828297879563119\n",
    "\t Eval F1 weighted: 0.5018142134794671\n",
    "\n",
    "    CV2\n",
    "     Eval MCC: 0.5228772733803458\n",
    "\t Eval MCCs: [0.0, 0.6335790996296946, 0.7550290307342492, 0.44438977344110725, 0.23157653872238865, 0.401588002072828, 0.54882259801054, 0.8159121192042134, 0.0]\n",
    "\t Eval MCC 1 vs other: 0.5413682948596558\n",
    "\t Eval F1 weighted: 0.47918670656828255\n",
    "\n",
    "    CV3\n",
    "     Eval MCC: 0.5727971743886916\n",
    "\t Eval MCCs: [0.0, 0.730986238974042, 0.7184699986803038, 0.463317674222242, 0.12942881116102176, 0.6675573372931909, 0.7640228470566507, 0.8159121192042134, 0.0]\n",
    "\t Eval MCC 1 vs other: 0.5509235787277967\n",
    "\t Eval F1 weighted: 0.523994837912579\n",
    "\n",
    "    CV4\n",
    "     Eval MCC: 0.5876629486122559\n",
    "\t Eval MCCs: [0.0, 0.7103407282207833, 0.818866922289005, 0.5203088698319962, 0.0, 0.7574347303478193, 0.832826149738672, 0.6164414002968975, 0.0]\n",
    "\t Eval MCC 1 vs other: 0.6049792034959248\n",
    "\t Eval F1 weighted: 0.5068551258398635"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d684a1",
   "metadata": {},
   "source": [
    "roberta-base-ne-rh-ct 'wuhan virus'\n",
    "\n",
    "    MCC 0.5936937677539237\n",
    "    MCCs [0.0, 0.7586291798047583, 0.7269793798057074, 0.5216994682535349, 0.25717796831777495, 0.5339780501326101, 0.7915754861444928, 0.7768418620119917, 0.0]\n",
    "    MCC 1 vs other: 0.5556210118597071\n",
    "    F1 weighted:0.5587084862311549\n",
    "\n",
    "    CV0\n",
    "     Eval MCC: 0.5554272912121295\n",
    "\t Eval MCCs: [0.0, 0.8351536074236927, 0.6322138486966449, 0.4774271024459417, 0.23407803057086504, 0.5697781346214729, 0.7915721885745207, 0.6417189156063434, 0.0]\n",
    "\t Eval MCC 1 vs other: 0.486812780765384\n",
    "\t Eval F1 weighted: 0.5153900339085524\n",
    "     \n",
    "    CV1\n",
    "     Eval MCC: 0.5718827243372663\n",
    "\t Eval MCCs: [0.0, 0.6860171630489652, 0.6558787445931271, 0.5056934321332517, 0.0, 0.6675573372931909, 0.8057464744632933, 0.845139371060569, 0.0]\n",
    "\t Eval MCC 1 vs other: 0.5502019433115497\n",
    "\t Eval F1 weighted: 0.4906719203456417\n",
    "\n",
    "    CV2\n",
    "     Eval MCC: 0.6470762868824049\n",
    "\t Eval MCCs: [0.0, 0.7941665712025352, 0.7905239190483594, 0.5853175186526681, 0.40103364169193567, 0.7574556324219388, 0.7659710913812849, 0.8247393072270269, 0.0]\n",
    "\t Eval MCC 1 vs other: 0.6135952990847273\n",
    "\t Eval F1 weighted: 0.6374894722045488\n",
    "\n",
    "\n",
    "    CV3\n",
    "     Eval MCC: 0.598427658779346\n",
    "\t Eval MCCs: [0.0, 0.7360097161961785, 0.764603775659926, 0.4848061257435033, 0.3291712127171663, 0.6750991463264484, 0.7739841300146298, 0.8384988440308885, 0.0]\n",
    "\t Eval MCC 1 vs other: 0.5630615977102935\n",
    "\t Eval F1 weighted: 0.596768643986279\n",
    "\n",
    "    CV4\n",
    "     Eval MCC: 0.5956548775584712\n",
    "\t Eval MCCs: [0.0, 0.74179884115242, 0.7916766110304798, 0.5552531622923093, 0.3216069566089075, 0.0, 0.8206035462887359, 0.7341128721351309, 0.0]\n",
    "\t Eval MCC 1 vs other: 0.5644334384265817\n",
    "\t Eval F1 weighted: 0.5532223607107528"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa69be4",
   "metadata": {},
   "source": [
    "roberta-base-ne-rh\n",
    "\n",
    "    MCC 0.5831859798218391\n",
    "    MCCs [0.0, 0.7451054119555204, 0.721440142207641, 0.49050237642063255, 0.1976472077983185, 0.5732948384655411, 0.7802770074383321, 0.7912258973690008, 0.04612988295774134]\n",
    "    MCC 1 vs other: 0.5687445014242034\n",
    "    F1 weighted:0.538882154144453\n",
    "\n",
    "    CV0\n",
    "     Eval MCC: 0.6062767753314612\n",
    "\t Eval MCCs: [0.0, 0.7978919748135389, 0.7534478479246006, 0.4958912930781506, 0.22977378007548427, 0.7001170862264607, 0.8352620506089892, 0.7565073210184726, 0.0]\n",
    "\t Eval MCC 1 vs other: 0.5322233699467324\n",
    "\t Eval F1 weighted: 0.5634600925874608\n",
    "\n",
    "    CV1\n",
    "     Eval MCC: 0.5672260567041271\n",
    "\t Eval MCCs: [0.0, 0.6718013873727162, 0.6469791854964575, 0.5124402467976967, 0.11320694593849961, 0.7976905619151519, 0.7732926801197991, 0.7951494147635916, 0.0]\n",
    "\t Eval MCC 1 vs other: 0.6054918412307335\n",
    "\t Eval F1 weighted: 0.5094482393771169\n",
    "\n",
    "    CV2\n",
    "     Eval MCC: 0.6087265482755518\n",
    "\t Eval MCCs: [0.0, 0.809855710442651, 0.8074089012633905, 0.44312948403800534, 0.3724457686984207, 0.2835072205183168, 0.7267181052190558, 0.8008233460395208, 0.24387662137157162]\n",
    "\t Eval MCC 1 vs other: 0.6037893387335158\n",
    "\t Eval F1 weighted: 0.5665388296399381\n",
    "\n",
    "    CV3\n",
    "     Eval MCC: 0.5810343853643407\n",
    "\t Eval MCCs: [0.0, 0.7021614307078777, 0.6936817538580171, 0.5673576808151052, 0.27280954427918797, 0.5154060995580372, 0.7516170021078921, 0.8159894763325497, -0.0132272065828649]\n",
    "\t Eval MCC 1 vs other: 0.5877040351313282\n",
    "\t Eval F1 weighted: 0.5763568993972661\n",
    "\n",
    "    CV4\n",
    "     Eval MCC: 0.5526661334337145\n",
    "\t Eval MCCs: [0.0, 0.7438165564408182, 0.7056830224957397, 0.4336931773742048, 0.0, 0.5697532241097388, 0.8144951991359242, 0.7876599286908698, 0.0]\n",
    "\t Eval MCC 1 vs other: 0.514513922078707\n",
    "\t Eval F1 weighted: 0.46725957398642093"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9d8f4f",
   "metadata": {},
   "source": [
    "roberta-base\n",
    "\n",
    "    MCC 0.5688375262501378\n",
    "    MCCs [0.0, 0.7574712203204303, 0.6755199677685797, 0.492401896189082, 0.16985821970869738, 0.3141116164825034, 0.7731892215113226, 0.7718570131106022, 0.04877532427431432]\n",
    "    MCC 1 vs other: 0.568850900245214\n",
    "    F1 weighted:0.5066875677553945\n",
    "    \n",
    "\n",
    "    CV0\n",
    "     Eval MCC: 0.5711766541368144\n",
    "\t Eval MCCs: [0.0, 0.7659710913812849, 0.7187597006180627, 0.5015032677457548, 0.2718953123929632, 0.5697781346214729, 0.7502508453587193, 0.6926817959734433, 0.0]\n",
    "\t Eval MCC 1 vs other: 0.5387349060978782\n",
    "\t Eval F1 weighted: 0.5389900137652603\n",
    "\n",
    "    CV1\n",
    "     Eval MCC: 0.5851019532287719\n",
    "\t Eval MCCs: [0.0, 0.7289243733533024, 0.6180879948332005, 0.5954018773784128, 0.20495001745210306, 0.7172727272727273, 0.7972091810119979, 0.8096021101018593, 0.0]\n",
    "\t Eval MCC 1 vs other: 0.6259699863614578\n",
    "\t Eval F1 weighted: 0.5422259076156084\n",
    "\n",
    "    CV2\n",
    "     Eval MCC: 0.6087265482755518\n",
    "\t Eval MCCs: [0.0, 0.809855710442651, 0.8074089012633905, 0.44312948403800534, 0.3724457686984207, 0.2835072205183168, 0.7267181052190558, 0.8008233460395208, 0.24387662137157162]\n",
    "\t Eval MCC 1 vs other: 0.6037893387335158\n",
    "\t Eval F1 weighted: 0.5665388296399381\n",
    "\n",
    "    CV3\n",
    "     Eval MCC: 0.5265191071699037\n",
    "\t Eval MCCs: [0.0, 0.7186064694657204, 0.5907177267502242, 0.4041221033925372, 0.0, 0.0, 0.8122026581712178, 0.7948230886553483, 0.0]\n",
    "\t Eval MCC 1 vs other: 0.5853958149828832\n",
    "\t Eval F1 weighted: 0.4465593824117734\n",
    "\n",
    "    CV4\n",
    "     Eval MCC: 0.5526633684396475\n",
    "\t Eval MCCs: [0.0, 0.7639984569591931, 0.6426255153780203, 0.5178527483906997, 0.0, 0.0, 0.779565317795622, 0.7613547247828389, 0.0]\n",
    "\t Eval MCC 1 vs other: 0.49036445505033527\n",
    "\t Eval F1 weighted: 0.43912370534439193"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407eff7b",
   "metadata": {},
   "source": [
    "roberta-model threshold 0.5:\n",
    "\t Eval loss: 0.1838788330554962\n",
    "\t Eval ACC: 0.5273311897106109\n",
    "\t Eval MCC: 0.5960125462227716\n",
    "\t Eval MCCs: [0.0, 0.6329781556312386, 0.7820744987599528, 0.44384639988077706, 0.17954940420025947, 0.7056340922398517, 0.7512385845326037, 0.7732538663940368, 0.0]\n",
    "\t Eval Kappa: 0.5711325966850829\n",
    "\n",
    "roberta-model threshold 0.3:\n",
    "     Eval loss: 0.21058178693056107\n",
    "\t Eval ACC: 0.4662379421221865\n",
    "\t Eval MCC: 0.6573546169315774\n",
    "\t Eval MCCs: [0.0, 0.6876344223650447, 0.7410155972655911, 0.4315558580462087, 0.13755046869478835, 0.0, 0.666130102959929, 0.8384782365353266, 0.0]\n",
    "\t Eval Kappa: 0.6284364666981577\n",
    "\n",
    "roberta-model threshold 0.8:\n",
    "     Eval loss: 0.1838788330554962\n",
    "\t Eval ACC: 0.48231511254019294\n",
    "\t Eval MCC: 0.40335439607990653\n",
    "\t Eval MCCs: [0.0, 0.638688367240694, 0.643168215736974, 0.25474696472398095, 0.0, 0.0, 0.678643320836891, 0.508190350779955, 0.0]\n",
    "\t Eval Kappa: 0.3526192337763878\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007c6702",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
