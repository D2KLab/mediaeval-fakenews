{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc981fc1",
   "metadata": {},
   "source": [
    "# MediaEval 2022 - Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204ed002",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "$('<div id=\"toc\"></div>').css({position: 'fixed', top: '120px', left: 0}).appendTo(document.body);\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe031d73",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e0e9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\" # the GPU on robinson\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35fc6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import deque\n",
    "import random\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertForPreTraining, BertModel, AdamW, AutoTokenizer, BertForSequenceClassification, RobertaForSequenceClassification\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, StackingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "\n",
    "import emoji\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "random_seed = 0\n",
    "torch.manual_seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import nodevectors\n",
    "\n",
    "conspiracies = ['Suppressed Cures',\n",
    "     'Behaviour and Mind Control',\n",
    "     'Antivax',\n",
    "     'Fake virus',\n",
    "     'Intentional Pandemic',\n",
    "     'Harmful Radiation/ Influence',\n",
    "     'Population reduction',\n",
    "     'New World Order',\n",
    "     'Satanism']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4617672",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c2648c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fold\n",
    "k = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab9f59d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ls /data/peskine/mediaeval22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2db79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../../../mediaeval22/'\n",
    "filelist = os.listdir(data_path)\n",
    "\n",
    "\n",
    "df_list = [pd.read_csv(data_path+file) for file in filelist if 'fold' in file]\n",
    "\n",
    "\n",
    "test_df = df_list[k]    \n",
    "#test_df = pd.read_csv('/data/peskine/mediaeval22/task_2_test.csv')\n",
    "\n",
    "train_df = pd.concat(df_list[:k]+df_list[k+1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40a20a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = torch.Tensor(train_df['user_id'].tolist())\n",
    "train_y = torch.Tensor(train_df['user_class_label'].tolist())-1\n",
    "\n",
    "test_x = torch.Tensor(test_df['user_id'].tolist())\n",
    "#test_y = torch.Tensor(test_df['user_class_label'].tolist())-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13eb96dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "train_data = TensorDataset(train_x, train_y)\n",
    "test_data = TensorDataset(test_x)\n",
    "\n",
    "\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c1c3e7",
   "metadata": {},
   "source": [
    "# Graph creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c3104a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path='../../../mediaeval22/'\n",
    "user_graph = pd.read_csv(path+'user_graph.csv')\n",
    "user_info = pd.read_csv(path+'user_info.csv')\n",
    "task_2_dev = pd.read_csv(path+'task_2_dev.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3334ba69",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0d8485",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f63d9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_ids = user_df['user_id'].tolist()\n",
    "users_labels = user_df['user_class_label'].tolist()\n",
    "\n",
    "classes = ['', 'Normal User', 'Misinfo Spreader']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b452fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998a6a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(users_ids), len(users_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e22923",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_dfi = user_graph[user_graph['i'].isin(users_ids)]\n",
    "tmp_dfj = user_graph[user_graph['j'].isin(users_ids)]\n",
    "\n",
    "tmp_df = pd.concat([tmp_dfi, tmp_dfj]).drop_duplicates()\n",
    "tmp_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6dc4eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create networkx graph\n",
    "# this may take a while, don't forget to save the results\n",
    "G = nx.DiGraph()\n",
    "\n",
    "for row in trange(0, len(user_graph)):\n",
    "    i, j, w = user_graph.iloc[row]\n",
    "\n",
    "    G.add_edge(i, j, weight=w)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563cd1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /data/peskine/mediaeval22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd5b13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.read_gexf('../../../mediaeval22/user_graph.gexf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e7ab4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(G.nodes()), len(G.edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b44f4c",
   "metadata": {},
   "source": [
    "# Random Walks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868f94ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nodevectors import Node2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69a4872",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = \"mediaeval22\"\n",
    "r = 10\n",
    "l = 40\n",
    "p = 1\n",
    "q = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f730604",
   "metadata": {},
   "outputs": [],
   "source": [
    "g2v = Node2Vec(\n",
    "    n_components=32,\n",
    "    neighbor_weight=2,\n",
    "    walklen=10\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3c73db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating random walks and train word2vec model.\n",
    "# this may take a while\n",
    "g2v.fit(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3144cec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "g2v.save('../../../mediaeval22/user_graph_w2v_d1024_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcffb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "g2v.save_vectors(\"../../../mediaeval22/user_graph_w2v_d1024_model.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6235a043",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "model = KeyedVectors.load_word2vec_format(\"../../../mediaeval22/user_graph_w2v_d32_model.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d87a29",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39238754",
   "metadata": {},
   "source": [
    "These are tools to visualize the node embeddings (t-sne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cb2dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea03be24",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../../mediaeval22/task_3_dev.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadb8365",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tmp_df = df[['user_id', 'user_class_label']]\n",
    "user_df = tmp_df.drop_duplicates()\n",
    "\n",
    "misinfo_users = user_df[user_df['user_class_label']==2]['user_id'].tolist()\n",
    "normal_users = user_df[user_df['user_class_label']==1]['user_id'].tolist()\n",
    "\n",
    "d = {}\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "user_ids = []\n",
    "\n",
    "for u in misinfo_users:\n",
    "    d[u] = model[str(u)]\n",
    "    X.append(model[str(u)])\n",
    "    y.append(\"Misinfo Spreader\")\n",
    "    user_ids.append(u)\n",
    "\n",
    "for u in normal_users:\n",
    "    d[u] = model[str(u)]\n",
    "    X.append(model[str(u)])\n",
    "    y.append(\"Normal User\")\n",
    "    user_ids.append(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ddcbfc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tsne = TSNE(2)\n",
    "tsne_result = tsne.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59e8d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [18, 18]\n",
    "plt.rcParams.update({'font.size': 22})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69444d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(misinfo_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ddb9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "misinfo_mean = tsne_result[:len(misinfo_users)].mean(axis=0)\n",
    "normal_mean = tsne_result[len(misinfo_users):].mean(axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd46cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_df = pd.DataFrame({'tsne_1': [misinfo_mean[0], normal_mean[0]], 'tsne_2': [misinfo_mean[1], normal_mean[1]], 'label': [\"Misinfo mean\", \"Normal mean\"]})\n",
    "mean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62985fff",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1)\n",
    "\n",
    "tsne_result_df = pd.DataFrame({'tsne_1': tsne_result[:,0], 'tsne_2': tsne_result[:,1], 'label': y})\n",
    "g = sns.scatterplot(x='tsne_1', y='tsne_2', palette=['lightblue', 'orange'], hue='label', data=tsne_result_df, ax=ax,s=120)\n",
    "g = sns.scatterplot(x='tsne_1', y='tsne_2', palette=['deepskyblue', 'darkorange'],  marker='*', hue='label', data=mean_df, ax=ax,s=3000, legend=False)\n",
    "\n",
    "#plt.show(g)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2d19d6",
   "metadata": {},
   "source": [
    "# Entities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103db31e",
   "metadata": {},
   "source": [
    "Are some entities mentioned more by misinfo spreader?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c21181",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38d8d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    text = text.replace('&amp;', '&')\n",
    "    text = text.replace('\\xa0', '')\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = \" \".join(text.split())\n",
    "    return text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5131e2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = df['tweet_text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796bf395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spaCy entityLinker entities\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe(\"entityLinker\", last=True)\n",
    "\n",
    "entities_tweets_spacy = []\n",
    "ex_tweet = []\n",
    "for s in tqdm(tweets):\n",
    "    s = normalize_text(s)\n",
    "    l = []\n",
    "    try:\n",
    "        doc = nlp(s)\n",
    "        all_linked_entities = doc._.linkedEntities\n",
    "        for ent in all_linked_entities:\n",
    "            l.append((ent.label, ent.url))\n",
    "        entities_tweets_spacy.append(l)\n",
    "    except:\n",
    "        ex_tweet.append(tweets.index(s))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8d3927",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[0, 1:10].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd287b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f544fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_misinfo = []\n",
    "entities_normal = []\n",
    "\n",
    "labels = df\n",
    "c=0\n",
    "for i in range(0, len(df)):\n",
    "    labels = df.iloc[i, 1:10].tolist()\n",
    "    if 3 in labels:\n",
    "        c+=1\n",
    "        entities_misinfo.extend([t[0] for t in entities_tweets_spacy[i]])\n",
    "    else:\n",
    "        entities_normal.extend([t[0] for t in entities_tweets_spacy[i]])\n",
    "count_misinfo = Counter(entities_misinfo)\n",
    "count_normal = Counter(entities_normal)\n",
    "\n",
    "all_entities = entities_misinfo+entities_normal\n",
    "count = Counter(all_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114fc81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_misinfo = c\n",
    "n_normal = len(df)-c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9cf867",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top = count.most_common()[:100]\n",
    "top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979f442e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "y = []\n",
    "l = []\n",
    "for t in top:\n",
    "    s = t[0]\n",
    "    x.append(s)\n",
    "    y.append(t[1])\n",
    "    ratio_misinfo = count_misinfo[s]/n_misinfo\n",
    "    ratio_normal = count_normal[s]/n_normal\n",
    "    tmp = 'Both'\n",
    "    if ratio_misinfo>2*ratio_normal:\n",
    "        tmp = 'Misinfo'\n",
    "    if ratio_normal>2*ratio_misinfo:\n",
    "        tmp = 'Normal'\n",
    "    l.append(tmp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b8a218",
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = []\n",
    "for lab in l:\n",
    "    if lab=='Both':\n",
    "        palette.append('lightgrey')\n",
    "    elif lab=='Misinfo':\n",
    "        palette.append('lightblue')\n",
    "    else:\n",
    "        palette.append('orange')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658ef74d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bar_df = pd.DataFrame({'entities':x, 'entities count':y, 'label':l})\n",
    "\n",
    "legend_elements = [Line2D([0], [0], color='lightgrey', lw=20, label='Both'),\n",
    "Line2D([0], [0], color='lightblue', lw=20, label='Misinfo Spreader'),\n",
    "Line2D([0], [0], color='orange', lw=20, label='Normal User')]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.legend(handles=legend_elements, loc='center')\n",
    "\n",
    "sns.set(style=\"darkgrid\")\n",
    "sns.set(font_scale=3)\n",
    "sns.barplot(data=bar_df, x='entities count', y='entities', palette=palette, orient='h')\n",
    "_=plt.xticks([50*i for i in range(1, 10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229e83e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "top[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dadbd49",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9c34d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 8),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "    \n",
    "        return self.layers(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c742d2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP()\n",
    "mlp.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9711cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [len(train_y)/train_y.tolist().count(0), len(train_y)/train_y.tolist().count(1)]\n",
    "weights = torch.Tensor(weights).to('cuda')\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f60a966",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "\n",
    "sig = nn.Sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05922247",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(mlp.parameters(),\n",
    "                  lr=3e-4,\n",
    "                  weight_decay = 0.001)\n",
    "\n",
    "scheduler = ReduceLROnPlateau(optimizer, patience=4, factor=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91746e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "best_MCC = 0\n",
    "best_epoch = 0\n",
    "\n",
    "for e in trange(0, epochs, position=0, leave=True):\n",
    "    train_loss = 0\n",
    "    \n",
    "    mlp.train()\n",
    "    \n",
    "    print('Starting epoch ', e)\n",
    "    \n",
    "    \n",
    "    for x, y in train_dataloader:\n",
    "        \n",
    "        x_features = []\n",
    "\n",
    "        for i in x:\n",
    "            i =str(int(i.item()))\n",
    "            f = model[str(i)]\n",
    "            x_features.append(f)\n",
    "\n",
    "        x_features = torch.Tensor(x_features).to('cuda')\n",
    "        outputs = mlp(x_features)\n",
    "        y_hat = sig(outputs).flatten()\n",
    "        \n",
    "        y = y.to('cuda')\n",
    "        loss = criterion(y_hat, y)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    print('Train Loss = ', train_loss)\n",
    "    \n",
    "    \n",
    "    test_loss = 0\n",
    "    preds = []\n",
    "    y_full = []\n",
    "    mlp.eval()\n",
    "    for x, y in test_dataloader:\n",
    "        \n",
    "        x_features = []\n",
    "\n",
    "        for i in x:\n",
    "            i =str(int(i.item()))\n",
    "            f = model[str(i)]\n",
    "            x_features.append(f)\n",
    "\n",
    "        x_features = torch.Tensor(x_features).to('cuda')\n",
    "        outputs = mlp(x_features)\n",
    "        y_hat = sig(outputs).flatten()\n",
    "        \n",
    "        y = y.to('cuda')\n",
    "        loss = criterion(y_hat, y)\n",
    "        \n",
    "        preds.extend((y_hat>0.5).long().cpu().tolist())\n",
    "        y_full.extend(y.long().cpu().tolist())\n",
    "        test_loss += loss.item()  \n",
    "        \n",
    "    mcc = metrics.matthews_corrcoef(preds, y_full)\n",
    "    \n",
    "    if best_MCC<mcc:\n",
    "        best_MCC = mcc\n",
    "        best_epoch = e\n",
    "        best_state_dict = copy.deepcopy(mlp.state_dict())\n",
    "    \n",
    "    print('\\t\\tTest Loss = ', test_loss)\n",
    "    print('\\t\\tTest MCC = ', round(mcc, 3))\n",
    "    print('\\n')\n",
    "    print('---'*25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f653e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_epoch, best_MCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a612bb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(best_state_dict, '../../../mediaeval22/models/task2_MLP+_CV'+str(k)+'_e'+str(best_epoch)+'_'+str(round(best_MCC, 3))+'.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1426f5a",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd8ef91",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ../../../mediaeval22/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8995aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.load_state_dict(torch.load('../../../mediaeval22/models/task2_MLP+_CV4_e163_0.455.pth'))\n",
    "mlp.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e137fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.eval()\n",
    "preds = []\n",
    "tids = test_df['user_id'].tolist()\n",
    "for x in test_dataloader:\n",
    "\n",
    "    x_features = []\n",
    "    x = x[0]\n",
    "    for i in x:\n",
    "        i =str(int(i.item()))\n",
    "        f = model[str(i)]\n",
    "        x_features.append(f)\n",
    "\n",
    "    x_features = torch.Tensor(x_features).to('cuda')\n",
    "    outputs = mlp(x_features)\n",
    "    y_hat = sig(outputs).flatten()\n",
    "\n",
    "    preds.extend((y_hat>0.5).long().cpu().tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de736c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = pd.DataFrame()\n",
    "sub_df['-1'] = tids\n",
    "sub_df['0'] = [i+1 for i in preds]\n",
    "sub_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4ed7b9",
   "metadata": {},
   "source": [
    "# Sklearn ML algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c3dc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment those you want to try\n",
    "classifiers = [\n",
    "    #KNeighborsClassifier(),\n",
    "    #SVC(),\n",
    "    #GaussianProcessClassifier(),\n",
    "    #DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    #MLPClassifier(max_iter=500),\n",
    "    #AdaBoostClassifier(),\n",
    "    #GaussianNB(),\n",
    "    #QuadraticDiscriminantAnalysis(),\n",
    "    #GradientBoostingClassifier()\n",
    "]\n",
    "MCCs = []\n",
    "for i in range(0, len(classifiers)):\n",
    "    MCCs.append([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7173e30c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_path = '../../../mediaeval22/'\n",
    "filelist = os.listdir(data_path)\n",
    "df_list = [pd.read_csv(data_path+file) for file in filelist if 'fold' in file]\n",
    "\n",
    "test_preds = []\n",
    "all_preds = []\n",
    "\n",
    "for k in trange(0, 5):\n",
    "    test_df = df_list[k]    \n",
    "    train_df = pd.concat(df_list[:k]+df_list[k+1:])\n",
    "    \n",
    "    test_df2 = pd.read_csv('../../../mediaeval22/task_2_test.csv')\n",
    "    \n",
    "    train_x = torch.Tensor(train_df['user_id'].tolist())\n",
    "    train_y = torch.Tensor(train_df['user_class_label'].tolist())-1\n",
    "\n",
    "    test_x = torch.Tensor(test_df['user_id'].tolist())\n",
    "    test_y = torch.Tensor(test_df['user_class_label'].tolist())-1\n",
    "    \n",
    "    test_x2 = torch.Tensor(test_df2['user_id'].tolist())\n",
    "    \n",
    "    batch_size = 512\n",
    "    train_data = TensorDataset(train_x, train_y)\n",
    "    test_data = TensorDataset(test_x, test_y)\n",
    "    \n",
    "    test_data2 = TensorDataset(test_x2)\n",
    "\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "    test_sampler = SequentialSampler(test_data)\n",
    "    test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
    "    test_sampler2 = SequentialSampler(test_data2)\n",
    "    test_dataloader2 = DataLoader(test_data2, sampler=test_sampler2, batch_size=batch_size)\n",
    "    \n",
    "    for c in range(0, len(classifiers)):\n",
    "        clf = classifiers[c]\n",
    "\n",
    "        x_features = []\n",
    "        y_true = []\n",
    "        for x, y in train_dataloader:\n",
    "            y_true.extend(y.long().numpy().tolist())\n",
    "            for i in x:\n",
    "                i =str(int(i.item()))\n",
    "                f = model[str(i)]\n",
    "                x_features.append(f.tolist())\n",
    "\n",
    "        clf.fit(x_features, y_true)\n",
    "\n",
    "        x_features = []\n",
    "        y_true = []\n",
    "        for x, y in test_dataloader:\n",
    "            y_true.extend(y.long().numpy().tolist())\n",
    "            for i in x:\n",
    "                i =str(int(i.item()))\n",
    "                f = model[str(i)]\n",
    "                x_features.append(f.tolist())\n",
    "        preds = clf.predict(x_features).tolist()\n",
    "        all_preds.append(preds)\n",
    "        \n",
    "        mcc = metrics.matthews_corrcoef(preds, y_true)\n",
    "        MCCs[c].append(mcc)\n",
    "    \n",
    "        x_features = []\n",
    "        for x in test_dataloader2:\n",
    "            x=x[0]\n",
    "            for i in x:\n",
    "                i =str(int(i.item()))\n",
    "                f = model[str(i)]\n",
    "                x_features.append(f.tolist())\n",
    "        test_preds.append(clf.predict(x_features).tolist())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bed0024",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../../mediaeval22/submissions/mlp-4.csv', names=['-1', '0'])\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
