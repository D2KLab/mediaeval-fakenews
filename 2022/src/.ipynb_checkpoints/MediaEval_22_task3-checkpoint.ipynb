{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6e20570",
   "metadata": {},
   "source": [
    "# MediaEval 2022 - Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ce4b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "$('<div id=\"toc\"></div>').css({position: 'fixed', top: '120px', left: 0}).appendTo(document.body);\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecddd0c",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce035d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" # the GPU on robinson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8b1f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import deque\n",
    "import random\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertForPreTraining, BertModel, AutoTokenizer, BertForSequenceClassification, RobertaForSequenceClassification\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.optim import AdamW\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "\n",
    "from tqdm.notebook import tqdm, trange\n",
    "\n",
    "import emoji\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "random_seed = 0\n",
    "torch.manual_seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "conspiracies = ['Suppressed Cures',\n",
    "     'Behaviour and Mind Control',\n",
    "     'Antivax',\n",
    "     'Fake virus',\n",
    "     'Intentional Pandemic',\n",
    "     'Harmful Radiation/ Influence',\n",
    "     'Population reduction',\n",
    "     'New World Order',\n",
    "     'Satanism']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a523e551",
   "metadata": {},
   "source": [
    "# Flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb2da66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = 'twitter'\n",
    "model_name = 'twitter'\n",
    "\n",
    "replace_lowercase_flag = False\n",
    "remove_stopwords_flag = False\n",
    "remove_hashtags_flag = True\n",
    "replace_emojis_flag = True\n",
    "clean_tweets_flag = False\n",
    "\n",
    "all_data = False\n",
    "\n",
    "classification = True\n",
    "\n",
    "# fold\n",
    "k=4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75039c44",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4179e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweets(tweets):\n",
    "    char_to_remove = ['\\n', '\\xa0']\n",
    "    corona_synonyms = ['coronavirus',\n",
    "                      'covid-19',\n",
    "                      'covid19',\n",
    "                      'covid 19',\n",
    "                      'covid',\n",
    "                      'corona',\n",
    "                      'sarscov2'\n",
    "                      'sars',\n",
    "                      'Coronaviruses',\n",
    "                      'Coronavirus',\n",
    "                      'Corona',\n",
    "                      'Covid19',\n",
    "                      'COVID19',\n",
    "                      'Covid-19',\n",
    "                      'COVID-19',\n",
    "                      'COVID 19',\n",
    "                      'Covid',\n",
    "                      'COVID',\n",
    "                      'SARSCOV2',\n",
    "                      'SARS']\n",
    "    \n",
    "    tweets_clean = []\n",
    "    for tw in tweets:\n",
    "        for c in char_to_remove:\n",
    "            tw = tw.replace(c, '')\n",
    "        tw = tw.replace('&amp;', '&')\n",
    "        \n",
    "        for syn in corona_synonyms:\n",
    "            if syn in tw:\n",
    "                tw = tw.replace(syn, 'virus')\n",
    "        tweets_clean.append(tw)\n",
    "    return tweets_clean\n",
    "\n",
    "def extract_hashtags(tweet):\n",
    "    # Returns hashtags in a list for a given tweet\n",
    "    \n",
    "    #tweet = tweet.replace('\\xa0','')\n",
    "    #tweet = tweet.replace('\\n','')\n",
    "    \n",
    "    tweet_words = tweet.split(' ')\n",
    "    tweet_words = [w for w in tweet_words if w!='']\n",
    "    hashtags = []\n",
    "    for word in tweet_words:\n",
    "        if word[0]=='#':\n",
    "            hashtags.append(word)\n",
    "    return hashtags\n",
    "\n",
    "def extract_emojis (tw):\n",
    "    # Returns emojis in a list for a given tweet\n",
    "    # Using Deque for a sliding window (emojis can be combined together to form other emojis)\n",
    "    \n",
    "    emojis = []\n",
    "    \n",
    "    l = []\n",
    "    max_l = 7\n",
    "    \n",
    "    for i in range(0, max_l):\n",
    "        l.append(tw[-1-i])\n",
    "    l = deque(l, maxlen=max_l)\n",
    "    skip=0\n",
    "    \n",
    "    for i in range (0, len(tw)):\n",
    "        if skip == 0:\n",
    "            for j in range (max_l-1, -1, -1):\n",
    "                str_to_test = ''\n",
    "                for k in range (0, j+1):\n",
    "                    str_to_test+=l[j-k]\n",
    "                if str_to_test in emoji.UNICODE_EMOJI:\n",
    "                    \n",
    "                    emojis.append(str_to_test)\n",
    "                    skip=j\n",
    "                    break\n",
    "            try:\n",
    "                l.append(tw[-1-i-max_l])\n",
    "            except IndexError:\n",
    "                l.append('')\n",
    "        else:\n",
    "            skip=skip-1\n",
    "            try:\n",
    "                l.append(tw[-1-i-max_l])\n",
    "            except IndexError:\n",
    "                l.append('')\n",
    "    emojis.reverse()\n",
    "    return emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae3fc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_lowercase(tweets):\n",
    "    tweets_lowercase = []\n",
    "    for tw in tweets:\n",
    "        tweets_lowercase.append(tw.lower())\n",
    "    return tweets_lowercase\n",
    "\n",
    "def remove_stopwords(tweets):\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    tweets_no_stopwords = []\n",
    "    for tw in tweets:\n",
    "        tw = tw.split(' ')\n",
    "        tweets_no_stopwords.append(' '.join([word for word in tw if not word in stop_words]))\n",
    "\n",
    "    return tweets_no_stopwords\n",
    "\n",
    "def remove_hashtags(tweets):\n",
    "    tweets = [tw.replace('#', '') for tw in tweets]\n",
    "    return tweets\n",
    "\n",
    "def replace_emojis(tweets):\n",
    "    tweets_no_emojis = []\n",
    "    for tw in tweets:\n",
    "        emojis = extract_emojis(tw)\n",
    "        for e in emojis:\n",
    "            e_text = emoji.UNICODE_EMOJI[e].replace('_',' ').replace(':', '')\n",
    "            tw = tw.replace(e, e_text)\n",
    "        tweets_no_emojis.append(tw)\n",
    "\n",
    "    return tweets_no_emojis\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fb8631",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5eac3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../../../mediaeval22/'\n",
    "filelist = os.listdir(data_path)\n",
    "\n",
    "\n",
    "df_list = [pd.read_csv(data_path+file) for file in filelist if 'fold' in file]\n",
    "\n",
    "\n",
    "#test_df = df_list[k]    \n",
    "train_df = pd.concat(df_list[:k]+df_list[k+1:])\n",
    "\n",
    "test_df = pd.read_csv('../../../mediaeval22/task_3_test.csv')\n",
    "\n",
    "\n",
    "tw_train = train_df['tweet_text'].tolist()\n",
    "tw_test = test_df['tweet_text'].tolist()\n",
    "ids_test = test_df['tweet_id'].tolist()\n",
    "\n",
    "\n",
    "if all_data:\n",
    "    df = pd.read_csv(data_path+'task_3_dev.csv')\n",
    "    tw_train = df['tweet']\n",
    "    labels_train = df.iloc[:,1:10].values.tolist()\n",
    "\n",
    "if clean_tweets_flag:\n",
    "    tw_train = clean_tweets(tw_train)\n",
    "    tw_test = clean_tweets(tw_test)\n",
    "\n",
    "if replace_lowercase_flag:\n",
    "    tw_train = to_lowercase(tw_train)\n",
    "    tw_test = to_lowercase(tw_test)\n",
    "\n",
    "if remove_stopwords_flag:\n",
    "    tw_train = remove_stopwords(tw_train)\n",
    "    tw_test = remove_stopwords(tw_test)\n",
    "\n",
    "if remove_hashtags_flag:\n",
    "    tw_train = remove_hashtags(tw_train)\n",
    "    tw_test = remove_hashtags(tw_test)\n",
    "\n",
    "if replace_emojis_flag:\n",
    "    tw_train = replace_emojis(tw_train)\n",
    "    tw_test = replace_emojis(tw_test)\n",
    "\n",
    "\n",
    "if not all_data:\n",
    "    labels_train = train_df.iloc[:,1:10].values.tolist()\n",
    "#labels_test = test_df.iloc[:,1:10].values.tolist()\n",
    "\n",
    "labels_train = [[l-1 for l in L] for L in labels_train]\n",
    "#labels_test = [[l-1 for l in L] for L in labels_test]\n",
    "\n",
    "user_ids_train = train_df['user_id'].tolist()\n",
    "user_ids_test = test_df['user_id'].tolist()\n",
    "\n",
    "weights_tmp = [0,0,0,0,0,0,0,0,0]\n",
    "for i in range(0, 9):\n",
    "    for j in range(0, len(labels_train)):\n",
    "        if labels_train[j][i]>0:\n",
    "            weights_tmp[i]+=1\n",
    "        \n",
    "weights_inter_conspiracies = [len(labels_train)/w for w in weights_tmp]\n",
    "#weights_inter_conspiracies is no longer used\n",
    "\n",
    "weights_inter_conspiracies = torch.FloatTensor(weights_inter_conspiracies).to(device)\n",
    "\n",
    "weights_intra_conspiracy = [[len(l)/l.count(j) for j in range(0, 3)] for l in [[k[i] for k in labels_train] for i in range(0, 9)]]\n",
    "weights_intra_conspiracy = torch.FloatTensor(weights_intra_conspiracy).to(device)\n",
    "#weights_intra_conspiracy represent the different weights loss for the different conspiracy theories\n",
    "\n",
    "weights_inter_conspiracies, weights_intra_conspiracy \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d5ba93",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.array(labels_train).flatten().tolist()\n",
    "weights_intra = [W.count(0), W.count(1), W.count(2)]\n",
    "weights_intra = sum(weights_intra)/torch.FloatTensor(weights_intra).cuda()\n",
    "weights_intra = weights_intra_conspiracy.mean(dim=0)\n",
    "weights_intra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16052af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('digitalepidemiologylab/covid-twitter-bert')\n",
    "\n",
    "tokenized_input = tokenizer(tw_train)\n",
    "\n",
    "m = 0\n",
    "for tokens in tokenized_input['input_ids']:\n",
    "    if len(tokens)>m:\n",
    "        m=len(tokens)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d7bb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 128 # < m some tweets will be truncated\n",
    "\n",
    "tokenized_input = tokenizer(tw_train, max_length=MAX_LEN, padding='max_length', truncation=True)\n",
    "tokenized_test = tokenizer(tw_test, max_length=MAX_LEN, padding='max_length', truncation=True)\n",
    "\n",
    "\n",
    "train_input_ids, train_token_type_ids, train_attention_mask = tokenized_input['input_ids'], tokenized_input['token_type_ids'], tokenized_input['attention_mask']\n",
    "test_input_ids, test_token_type_ids, test_attention_mask = tokenized_test['input_ids'], tokenized_test['token_type_ids'], tokenized_test['attention_mask']\n",
    "\n",
    "train_token_type_ids = torch.tensor(train_token_type_ids)\n",
    "test_token_type_ids = torch.tensor(test_token_type_ids)\n",
    "    \n",
    "    \n",
    "train_labels = labels_train\n",
    "#test_labels = labels_test\n",
    "\n",
    "\n",
    "# Convert to torch tensor\n",
    "train_input_ids = torch.tensor(train_input_ids)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "train_attention_mask = torch.tensor(train_attention_mask)\n",
    "user_ids_train = torch.Tensor(user_ids_train)\n",
    "\n",
    "test_input_ids = torch.tensor(test_input_ids)\n",
    "#test_labels = torch.tensor(test_labels)\n",
    "test_attention_mask = torch.tensor(test_attention_mask)\n",
    "test_ids = torch.tensor(ids_test)\n",
    "user_ids_test = torch.Tensor(user_ids_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28dcee5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 12 #\n",
    "\n",
    "train_data = TensorDataset(train_input_ids, train_attention_mask, train_labels, train_token_type_ids, user_ids_train)\n",
    "test_data = TensorDataset(test_input_ids, test_attention_mask, test_token_type_ids, test_ids, user_ids_test)\n",
    "\n",
    "    \n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff83af33",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3a41fc",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bc239f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CovidTwitterBertClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_classes):\n",
    "        super().__init__()\n",
    "        self.n_classes = n_classes\n",
    "        self.bert = BertForPreTraining.from_pretrained('digitalepidemiologylab/covid-twitter-bert-v2')    \n",
    "        self.bert.cls.seq_relationship = nn.Linear(1024, n_classes)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, input_ids, token_type_ids, input_mask):\n",
    "        outputs = self.bert(input_ids = input_ids, token_type_ids = token_type_ids, attention_mask = input_mask)\n",
    "\n",
    "        logits = outputs[1]\n",
    "        \n",
    "        return logits  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5dbd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /data/peskine/mediaeval22/models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ab70bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearnedFeaturesFusion(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.text_model = CovidTwitterBertClassifier(9*3)\n",
    "        self.text_model.load_state_dict(torch.load('../../../mediaeval22/models/task1_twitter_CV4_e24_0.725.pth'))\n",
    "        \n",
    "        self.graph_model = KeyedVectors.load_word2vec_format(\"../../../mediaeval22/user_graph_w2v_d32_model.bin\")\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(32+27, 32),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 27)\n",
    "        )\n",
    "        \n",
    "    def forward(self, input_ids, token_type_ids, input_mask, user_ids):\n",
    "        features_text = self.text_model(input_ids, token_type_ids, input_mask)\n",
    "        features_graph = torch.Tensor(self.graph_model[[str(int(uid)) for uid in user_ids.tolist()]]).to(device)\n",
    "        features = torch.cat([features_text, features_graph], dim=1)\n",
    "        \n",
    "        logits = self.classifier(features)\n",
    "        \n",
    "        return logits\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64477ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LearnedFeaturesFusion()\n",
    "    \n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362edb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in model.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "    \n",
    "# only require grad for the classification head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d00bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr=5e-3,\n",
    "                  weight_decay = 0.01)\n",
    "\n",
    "scheduler = ReduceLROnPlateau(optimizer, patience=4, factor=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feafc7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterions = []\n",
    "\n",
    "for i in range(0, 9):\n",
    "    criterions.append(nn.CrossEntropyLoss(weight = weights_intra_conspiracy[i]))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebf909c",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b646e59f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = 25\n",
    "\n",
    "threshold = 0.5\n",
    "\n",
    "best_MCCA = 0\n",
    "best_F1 = 0\n",
    "best_MCCs = []\n",
    "best_MCCNC = 0\n",
    "best_loss = 999\n",
    "best_acc = 0\n",
    "best_state_dict = model.state_dict()\n",
    "best_epoch = 0\n",
    "best_MCCs = []\n",
    "best_MCCs_task2 = 0\n",
    "best_MCCA_task2 = 0\n",
    "best_MCC_task1 = 0\n",
    "\n",
    "for e in trange(0, epochs, position=0, leave=True):\n",
    "\n",
    "    # Training\n",
    "    print(\"Starting epoch \", e)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        b_input_ids, b_input_mask, b_labels, b_token_type_ids, b_user_ids = batch\n",
    "        \n",
    "        b_labels = b_labels.float()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        logits = model(b_input_ids, b_token_type_ids, b_input_mask, b_user_ids)\n",
    "        \n",
    "        losses = []\n",
    "        for i in range(0, 9):\n",
    "            logits_i = logits[:,3*i:3*i+3]\n",
    "            labels_i = b_labels[:, i].long()\n",
    "            loss_i = criterions[i](logits_i, labels_i)\n",
    "            losses.append(loss_i*weights_inter_conspiracies[i])\n",
    "        #loss = [losses[i]*weights_inter_conspiracies[i] for i in range(0, len(losses))]\n",
    "        loss = sum(losses)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "\n",
    "    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "    \n",
    "    \n",
    "    # Testing\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    tweets_test = []\n",
    "    \n",
    "    predictions_sep = [[], [], [], [], [], [], [], [], []]\n",
    "    predictions_task1 = []\n",
    "    \n",
    "    labels_sep = [[], [], [], [], [], [], [], [], []]\n",
    "    labels_task1 = []\n",
    "    \n",
    "    eval_loss = 0\n",
    "    steps=0\n",
    "    for step, batch in enumerate(test_dataloader):\n",
    "\n",
    "        # Add batch to GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        b_input_ids, b_input_mask, b_labels, b_token_type_ids, ids, b_user_ids = batch\n",
    "            \n",
    "        b_labels = b_labels.float()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "\n",
    "            logits = model(b_input_ids, b_token_type_ids, b_input_mask, b_user_ids)\n",
    "            losses = []\n",
    "            for i in range(0, 9):\n",
    "                logits_i = logits[:,3*i:3*i+3]\n",
    "                labels_i = b_labels[:, i].long()\n",
    "                loss_i = criterions[i](logits_i, labels_i)\n",
    "                losses.append(loss_i*weights_inter_conspiracies[i])\n",
    "            #loss = [losses[i]*weights_inter_conspiracies[i] for i in range(0, len(losses))]\n",
    "            loss = sum(losses)\n",
    "    \n",
    "\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        ground_truth = b_labels.detach().cpu().numpy()\n",
    "        \n",
    "        steps+=1\n",
    "        eval_loss+=loss.detach().item()\n",
    "        \n",
    "        tweets_test.append(b_input_ids)\n",
    "        for i in range(0, len(logits)):\n",
    "            p = logits[i]\n",
    "            l = ground_truth[i]\n",
    "\n",
    "            predictions_task1.append(max([p[3*i: 3*i+3].argmax() for i in range(0,9)]))\n",
    "            labels_task1.append(l.max())\n",
    "            \n",
    "        for i in range(0, 9):\n",
    "            for p in logits:\n",
    "                p_i = p[3*i:3*i+3]\n",
    "                pred = np.argmax(p_i)\n",
    "                predictions_sep[i].append(pred)\n",
    "            for l in ground_truth:\n",
    "                labels_sep[i].append(l[i])\n",
    "            \n",
    "    MCCs = []\n",
    "    for i in range(0, 9):\n",
    "        MCCs.append(round(metrics.matthews_corrcoef(labels_sep[i], predictions_sep[i]), 3))\n",
    "\n",
    "    \n",
    "    scheduler.step(eval_loss/steps)\n",
    "    LOSS = eval_loss/steps\n",
    "    MCCA = np.mean(MCCs)\n",
    "    \n",
    "    if MCCA> best_MCCA:\n",
    "        best_MCCA = MCCA\n",
    "        best_loss = LOSS\n",
    "        best_MCCs = MCCs\n",
    "        best_state_dict = copy.deepcopy(model.state_dict())\n",
    "        best_epoch = e\n",
    "    \n",
    "    print(\"\\t Eval loss: {}\".format(LOSS))\n",
    "    print(\"\\t Eval MCC for task 1: {}\".format(MCC_task1))\n",
    "    print(\"\\t Eval MCCA: {}\".format(MCCA))\n",
    "    print(\"\\t Eval MCCs: {}\".format(MCCs))\n",
    "    print(\"---\"*25)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1849de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_MCCA, best_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b8284d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(best_state_dict, '../../../mediaeval22/models/task3_'+model_name+'_CV'+str(k)+'_e'+str(best_epoch)+'_'+str(round(best_MCCA, 3))+'.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51408c25",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b5a814",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ../../../mediaeval22/models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda7234c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('../../../mediaeval22/models/task3_twitter_CV4_e16_0.705.pth'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc1a110",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "tweets_test = []\n",
    "\n",
    "predictions_sep = [[], [], [], [], [], [], [], [], []]\n",
    "predictions_task1 = []\n",
    "\n",
    "labels_sep = [[], [], [], [], [], [], [], [], []]\n",
    "labels_task1 = []\n",
    "\n",
    "eval_loss = 0\n",
    "steps=0\n",
    "for step, batch in enumerate(test_dataloader):\n",
    "\n",
    "    # Add batch to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "    b_input_ids, b_input_mask, b_token_type_ids, ids, b_user_ids = batch\n",
    "\n",
    "    #b_labels = b_labels.float()\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        logits = model(b_input_ids, b_token_type_ids, b_input_mask, b_user_ids)\n",
    "        #losses = []\n",
    "        #for i in range(0, 9):\n",
    "        #    logits_i = logits[:,3*i:3*i+3]\n",
    "        #    labels_i = b_labels[:, i].long()\n",
    "        #    loss_i = criterions[i](logits_i, labels_i)\n",
    "        #    losses.append(loss_i*weights_inter_conspiracies[i])\n",
    "        #loss = [losses[i]*weights_inter_conspiracies[i] for i in range(0, len(losses))]\n",
    "        #loss = sum(losses)\n",
    "\n",
    "\n",
    "\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    #ground_truth = b_labels.detach().cpu().numpy()\n",
    "\n",
    "    steps+=1\n",
    "    #eval_loss+=loss.detach().item()\n",
    "\n",
    "    tweets_test.append(b_input_ids)\n",
    "    for i in range(0, len(logits)):\n",
    "        p = logits[i]\n",
    "        #l = ground_truth[i]\n",
    "\n",
    "        predictions_task1.append(max([p[3*i: 3*i+3].argmax() for i in range(0,9)]))\n",
    "        #labels_task1.append(l.max())\n",
    "\n",
    "    for i in range(0, 9):\n",
    "        for p in logits:\n",
    "            p_i = p[3*i:3*i+3]\n",
    "            pred = np.argmax(p_i)\n",
    "            predictions_sep[i].append(pred)\n",
    "        #for l in ground_truth:\n",
    "        #    labels_sep[i].append(l[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c832ddea",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = pd.DataFrame()\n",
    "sub_df['-1'] = test_df['tweet_id'].tolist()\n",
    "for i in range(0, 9):\n",
    "    sub_df[i]=[j+1 for j in predictions_sep[i]]\n",
    "    \n",
    "sub_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
