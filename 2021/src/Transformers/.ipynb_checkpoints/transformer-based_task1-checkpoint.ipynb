{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1f23731",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import deque\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertForPreTraining, BertModel, AdamW, AutoTokenizer,XLNetForSequenceClassification, BertForSequenceClassification, RobertaForSequenceClassification\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import emoji\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "random_seed = 0\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "conspiracies = ['Suppressed Cures',\n",
    "     'Behaviour and Mind Control',\n",
    "     'Antivax',\n",
    "     'Fake virus',\n",
    "     'Intentional Pandemic',\n",
    "     'Harmful Radiation/ Influence',\n",
    "     'Population reduction',\n",
    "     'New World Order',\n",
    "     'Satanism']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "275c41d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'twitter'\n",
    "\n",
    "replace_lowercase_flag = False\n",
    "remove_stopwords_flag = False\n",
    "remove_hashtags_flag = True\n",
    "replace_emojis_flag = True\n",
    "clean_tweets_flag = False\n",
    "\n",
    "class_weights_flag = True\n",
    "all_data = False\n",
    "\n",
    "classification = True\n",
    "\n",
    "# fold\n",
    "k=4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91907c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweets(tweets):\n",
    "    char_to_remove = ['\\n', '\\xa0']\n",
    "    # replace these words with a word that the model understands\n",
    "    corona_synonyms = ['coronavirus',\n",
    "                      'covid-19',\n",
    "                      'covid19',\n",
    "                      'covid 19',\n",
    "                      'covid',\n",
    "                      'corona',\n",
    "                      'sarscov2'\n",
    "                      'sars',\n",
    "                      'Coronavirus',\n",
    "                      'Corona',\n",
    "                      'Covid19',\n",
    "                      'COVID19',\n",
    "                      'Covid-19',\n",
    "                      'COVID-19',\n",
    "                      'COVID 19',\n",
    "                      'Covid',\n",
    "                      'COVID',\n",
    "                      'SARSCOV2',\n",
    "                      'SARS',\n",
    "                      'Coronaviruses']\n",
    "    corona_replaced = 'virus'\n",
    "    tweets_clean = []\n",
    "    for tw in tweets:\n",
    "        for c in char_to_remove:\n",
    "            tw = tw.replace(c, '')\n",
    "        tw = tw.replace('&amp;', '&')\n",
    "        \n",
    "        for syn in corona_synonyms:\n",
    "            if syn in tw:\n",
    "                tw = tw.replace(syn, corona_replaced)\n",
    "        tweets_clean.append(tw)\n",
    "    return tweets_clean\n",
    "\n",
    "def extract_hashtags(tweet):\n",
    "    # Returns hashtags in a list for a given tweet\n",
    "    \n",
    "    #tweet = tweet.replace('\\xa0','')\n",
    "    #tweet = tweet.replace('\\n','')\n",
    "    \n",
    "    tweet_words = tweet.split(' ')\n",
    "    tweet_words = [w for w in tweet_words if w!='']\n",
    "    hashtags = []\n",
    "    for word in tweet_words:\n",
    "        if word[0]=='#':\n",
    "            hashtags.append(word)\n",
    "    return hashtags\n",
    "\n",
    "def extract_emojis (tw):\n",
    "    # Returns emojis in a list for a given tweet\n",
    "    # Using Deque for a sliding window (emojis can be combined together to form other emojis)\n",
    "    \n",
    "    emojis = []\n",
    "    \n",
    "    l = []\n",
    "    max_l = 7\n",
    "    \n",
    "    for i in range(0, max_l):\n",
    "        l.append(tw[-1-i])\n",
    "    l = deque(l, maxlen=max_l)\n",
    "    skip=0\n",
    "    \n",
    "    for i in range (0, len(tw)):\n",
    "        if skip == 0:\n",
    "            for j in range (max_l-1, -1, -1):\n",
    "                str_to_test = ''\n",
    "                for k in range (0, j+1):\n",
    "                    str_to_test+=l[j-k]\n",
    "                if str_to_test in emoji.UNICODE_EMOJI['en']:\n",
    "                    \n",
    "                    emojis.append(str_to_test)\n",
    "                    skip=j\n",
    "                    break\n",
    "            try:\n",
    "                l.append(tw[-1-i-max_l])\n",
    "            except IndexError:\n",
    "                l.append('')\n",
    "        else:\n",
    "            skip=skip-1\n",
    "            try:\n",
    "                l.append(tw[-1-i-max_l])\n",
    "            except IndexError:\n",
    "                l.append('')\n",
    "    emojis.reverse()\n",
    "    return emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dddbd3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_lowercase(tweets):\n",
    "    # Converts tweets to lowercase\n",
    "    tweets_lowercase = []\n",
    "    for tw in tweets:\n",
    "        tweets_lowercase.append(tw.lower())\n",
    "    return tweets_lowercase\n",
    "\n",
    "def remove_stopwords(tweets):\n",
    "    # Removes stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    tweets_no_stopwords = []\n",
    "    for tw in tweets:\n",
    "        tw = tw.split(' ')\n",
    "        tweets_no_stopwords.append(' '.join([word for word in tw if not word in stop_words]))\n",
    "\n",
    "    return tweets_no_stopwords\n",
    "\n",
    "def remove_hashtags(tweets):\n",
    "    # Remove the # char\n",
    "    \n",
    "    tweets = [tw.replace('#', '') for tw in tweets]\n",
    "    return tweets\n",
    "\n",
    "def replace_emojis(tweets):\n",
    "    # Convert emoji to text\n",
    "    \n",
    "    tweets_no_emojis = []\n",
    "    for tw in tweets:\n",
    "        emojis = extract_emojis(tw)\n",
    "        for e in emojis:\n",
    "            e_text = emoji.UNICODE_EMOJI['en'][e].replace('_',' ').replace(':', '')\n",
    "            tw = tw.replace(e, e_text)\n",
    "        tweets_no_emojis.append(tw)\n",
    "\n",
    "    return tweets_no_emojis\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b96c60ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './data/task1/'\n",
    "filelist = os.listdir(data_path)\n",
    "\n",
    "\n",
    "df_list = [pd.read_csv(data_path+file) for file in filelist]\n",
    "\n",
    "\n",
    "test_df = df_list[k]    \n",
    "train_df = pd.concat(df_list[:k]+df_list[k+1:])\n",
    "\n",
    "\n",
    "tw_train = train_df['tweet'].tolist()\n",
    "tw_test = test_df['tweet'].tolist()\n",
    "\n",
    "\n",
    "if all_data:\n",
    "    df = pd.read_csv('./data/dev-full-task-1-clean.csv')\n",
    "    tw_train = df['tweet'].tolist()\n",
    "    labels_train = df['1'].tolist()\n",
    "\n",
    "if clean_tweets_flag:\n",
    "    tw_train = clean_tweets(tw_train)\n",
    "    tw_test = clean_tweets(tw_test)\n",
    "\n",
    "if replace_lowercase_flag:\n",
    "    tw_train = to_lowercase(tw_train)\n",
    "    tw_test = to_lowercase(tw_test)\n",
    "\n",
    "if remove_stopwords_flag:\n",
    "    tw_train = remove_stopwords(tw_train)\n",
    "    tw_test = remove_stopwords(tw_test)\n",
    "\n",
    "if remove_hashtags_flag:\n",
    "    tw_train = remove_hashtags(tw_train)\n",
    "    tw_test = remove_hashtags(tw_test)\n",
    "\n",
    "if replace_emojis_flag:\n",
    "    tw_train = replace_emojis(tw_train)\n",
    "    tw_test = replace_emojis(tw_test)\n",
    "\n",
    "\n",
    "if not all_data:\n",
    "    labels_train = train_df['label'].tolist()\n",
    "\n",
    "labels_train = [labels_train[i]-1 for i in range(0, len(labels_train))]\n",
    "labels_test = test_df['label'].tolist()\n",
    "labels_test = [labels_test[i]-1 for i in range(0, len(labels_test))]\n",
    "ids_test = test_df['ids'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4eee9191",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.0261, 5.7343, 3.0116], device='cuda:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./data/'+'dev-full-task-1-clean.csv')\n",
    "labels = df['1'].tolist()\n",
    "labels =(np.array(labels)-1).tolist()\n",
    "tweets = df['tweet'].tolist()\n",
    "\n",
    "weights = torch.FloatTensor([1, 1, 1]).cuda()\n",
    "\n",
    "if class_weights_flag:\n",
    "    weights = torch.FloatTensor([len(labels)/labels.count(i) for i in range(0,3)]).cuda()\n",
    "\n",
    "# The weights are the inverse frequency of the class multiplied by the number of tweets in the training data (1554)    \n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51af1837",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if 'twitter' in model_name:\n",
    "    tokenizer = AutoTokenizer.from_pretrained('digitalepidemiologylab/covid-twitter-bert')\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenized_input = tokenizer(tw_train)\n",
    "\n",
    "m = 0\n",
    "for tokens in tokenized_input['input_ids']:\n",
    "    if len(tokens)>m:\n",
    "        m=len(tokens)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "303231ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 128\n",
    "\n",
    "tokenized_input = tokenizer(tw_train, max_length=MAX_LEN, padding='max_length', truncation=True)\n",
    "tokenized_test = tokenizer(tw_test, max_length=MAX_LEN, padding='max_length', truncation=True)\n",
    "\n",
    "\n",
    "if 'roberta' in model_name:\n",
    "    train_input_ids, train_attention_mask = tokenized_input['input_ids'], tokenized_input['attention_mask']\n",
    "    test_input_ids, test_attention_mask = tokenized_test['input_ids'], tokenized_test['attention_mask']\n",
    "\n",
    "\n",
    "else:\n",
    "    train_input_ids, train_token_type_ids, train_attention_mask = tokenized_input['input_ids'], tokenized_input['token_type_ids'], tokenized_input['attention_mask']\n",
    "    test_input_ids, test_token_type_ids, test_attention_mask = tokenized_test['input_ids'], tokenized_test['token_type_ids'], tokenized_test['attention_mask']\n",
    "\n",
    "    train_token_type_ids = torch.tensor(train_token_type_ids)\n",
    "    test_token_type_ids = torch.tensor(test_token_type_ids)\n",
    "\n",
    "\n",
    "train_labels = labels_train\n",
    "test_labels = labels_test\n",
    "\n",
    "\n",
    "# Convert to torch tensor\n",
    "train_input_ids = (torch.tensor(train_input_ids))\n",
    "train_labels = (torch.tensor(train_labels))\n",
    "train_attention_mask = (torch.tensor(train_attention_mask))\n",
    "\n",
    "test_input_ids = (torch.tensor(test_input_ids))\n",
    "test_labels = (torch.tensor(test_labels))\n",
    "test_attention_mask = (torch.tensor(test_attention_mask))\n",
    "test_ids = torch.tensor(ids_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7731e4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 13 \n",
    "\n",
    "if 'roberta' in model_name:\n",
    "    train_data = (TensorDataset(train_input_ids, train_attention_mask, train_labels))\n",
    "    test_data = (TensorDataset(test_input_ids, test_attention_mask, test_labels, test_ids))\n",
    "\n",
    "else:\n",
    "    train_data = (TensorDataset(train_input_ids, train_attention_mask, train_labels, train_token_type_ids))\n",
    "    test_data = (TensorDataset(test_input_ids, test_attention_mask, test_labels, test_token_type_ids, test_ids))\n",
    "\n",
    "# Build DataLoaders\n",
    "train_sampler = (RandomSampler(train_data))\n",
    "train_dataloader = (DataLoader(train_data, sampler=train_sampler, batch_size=batch_size))\n",
    "\n",
    "test_sampler = (SequentialSampler(test_data))\n",
    "test_dataloader = (DataLoader(test_data, sampler=test_sampler, batch_size=batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc4aef79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tesla K80'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be5e1726",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_classes):\n",
    "        super().__init__()\n",
    "        self.n_classes = n_classes\n",
    "        self.bert = BertForSequenceClassification.from_pretrained(model_name, num_labels=n_classes)\n",
    "        if n_classes >1:\n",
    "            # For classification\n",
    "            self.criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "        else:\n",
    "            # For regression\n",
    "            self.criterion = nn.MSELoss()\n",
    "        \n",
    "    def forward(self, input_ids, input_mask, labels):\n",
    "        outputs = self.bert(input_ids, input_mask)\n",
    "        \n",
    "        logits = outputs[0]\n",
    "        if self.n_classes == 1:\n",
    "            labels=labels.float()\n",
    "        loss = self.criterion(logits, labels)\n",
    "        \n",
    "        return loss, logits\n",
    "\n",
    "class CovidTwitterBertClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_classes):\n",
    "        super().__init__()\n",
    "        self.n_classes = n_classes\n",
    "        self.bert = BertForPreTraining.from_pretrained('digitalepidemiologylab/covid-twitter-bert-v2')    \n",
    "        #self.bert = BertForSequenceClassification.from_pretrained(model_name, num_labels=n_classes)\n",
    "        self.bert.cls.seq_relationship = nn.Linear(1024, n_classes)\n",
    "        \n",
    "        if n_classes >1:\n",
    "            self.criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "        else:\n",
    "            self.criterion = nn.MSELoss()\n",
    "        \n",
    "    def forward(self, input_ids, token_type_ids, input_mask, labels):\n",
    "        outputs = self.bert(input_ids = input_ids, token_type_ids = token_type_ids, attention_mask = input_mask)\n",
    "        #outputs = self.classifier(outputs.pooler_output)\n",
    "        \n",
    "        logits = outputs[1]\n",
    "        \n",
    "        loss = self.criterion(logits, labels)\n",
    "        return loss, logits\n",
    "    \n",
    "class RobertaClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_classes):\n",
    "        super().__init__()\n",
    "        self.n_classes = n_classes\n",
    "        self.bert = RobertaForSequenceClassification.from_pretrained(model_name, num_labels=n_classes)\n",
    "        if n_classes >1:\n",
    "            # For classification\n",
    "            self.criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "        else:\n",
    "            # For regression\n",
    "            self.criterion = nn.MSELoss()\n",
    "        \n",
    "    def forward(self, input_ids, input_mask, labels):\n",
    "        outputs = self.bert(input_ids, input_mask)\n",
    "        \n",
    "        logits = outputs[0]\n",
    "        if self.n_classes == 1:\n",
    "            labels=labels.float()\n",
    "        loss = self.criterion(logits, labels)\n",
    "        \n",
    "        return loss, logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae612207",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaClassifier(\n",
       "  (bert): RobertaForSequenceClassification(\n",
       "    (roberta): RobertaModel(\n",
       "      (embeddings): RobertaEmbeddings(\n",
       "        (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "        (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "        (token_type_embeddings): Embedding(1, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): RobertaEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (3): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (4): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (5): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (6): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (7): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (8): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (9): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (10): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (11): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (classifier): RobertaClassificationHead(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (out_proj): Linear(in_features=768, out_features=3, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (criterion): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if 'roberta' in model_name:\n",
    "    model = RobertaClassifier(3)\n",
    "elif 'twitter' in model_name:\n",
    "    model = CovidTwitterBertClassifier(3)\n",
    "else:\n",
    "    model = BertClassifier(3)\n",
    "    \n",
    "    \n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9fb421a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer_grouped_parameters\n",
    "# lr = 5e-5 good for base\n",
    "# lr = 5e-6 good for large\n",
    "\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr=7e-6,\n",
    "                  weight_decay = 0.001)\n",
    "\n",
    "scheduler = ReduceLROnPlateau(optimizer, patience=4, factor=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6a413c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_regression(val):\n",
    "    if val<0.5:\n",
    "        return 0\n",
    "    elif val<1.5:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc92b42b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   0%|                                                 | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.072143609109132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  10%|████                                     | 1/10 [01:06<09:56, 66.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Eval loss: 0.819002652168274\n",
      "\t Eval ACC: 0.5691318327974276\n",
      "\t Eval MCC: 0.3827846853381036\n",
      "\t Eval MCC 1 vs other: 0.25776954343712466\n",
      "[82, 95, 134] [153, 55, 103]\n",
      "Train loss: 0.8108663662620212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  20%|████████▏                                | 2/10 [02:12<08:51, 66.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Eval loss: 0.5889796257019043\n",
      "\t Eval ACC: 0.7781350482315113\n",
      "\t Eval MCC: 0.6686022437953016\n",
      "\t Eval MCC 1 vs other: 0.699785223971466\n",
      "[120, 84, 107] [153, 55, 103]\n",
      "Train loss: 0.524394567893899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  30%|████████████▎                            | 3/10 [03:19<07:45, 66.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Eval loss: 0.2790530055761337\n",
      "\t Eval ACC: 0.8713826366559485\n",
      "\t Eval MCC: 0.8051505210753744\n",
      "\t Eval MCC 1 vs other: 0.7862475952141406\n",
      "[126, 61, 124] [153, 55, 103]\n",
      "Train loss: 0.3164292865473291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  40%|████████████████▍                        | 4/10 [04:26<06:39, 66.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Eval loss: 0.13135399296879768\n",
      "\t Eval ACC: 0.954983922829582\n",
      "\t Eval MCC: 0.9285197760699242\n",
      "\t Eval MCC 1 vs other: 0.9430231421581566\n",
      "[146, 52, 113] [153, 55, 103]\n",
      "Train loss: 0.16268873376690823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  50%|████████████████████▌                    | 5/10 [05:32<05:32, 66.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Eval loss: 0.04699884001165629\n",
      "\t Eval ACC: 0.9742765273311897\n",
      "\t Eval MCC: 0.9595502019953146\n",
      "\t Eval MCC 1 vs other: 0.9497576168011006\n",
      "[145, 56, 110] [153, 55, 103]\n",
      "Train loss: 0.12096793107364488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  60%|████████████████████████▌                | 6/10 [06:39<04:26, 66.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Eval loss: 0.027229376137256622\n",
      "\t Eval ACC: 0.9871382636655949\n",
      "\t Eval MCC: 0.9795004486333443\n",
      "\t Eval MCC 1 vs other: 0.9808791872737402\n",
      "[150, 59, 102] [153, 55, 103]\n",
      "Train loss: 0.08926799889329982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  70%|████████████████████████████▋            | 7/10 [07:45<03:19, 66.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Eval loss: 0.008718459773808718\n",
      "\t Eval ACC: 1.0\n",
      "\t Eval MCC: 1.0\n",
      "\t Eval MCC 1 vs other: 1.0\n",
      "[153, 55, 103] [153, 55, 103]\n",
      "Train loss: 0.04810868067990826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  80%|████████████████████████████████▊        | 8/10 [08:52<02:12, 66.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Eval loss: 0.0023711236659437417\n",
      "\t Eval ACC: 1.0\n",
      "\t Eval MCC: 1.0\n",
      "\t Eval MCC 1 vs other: 1.0\n",
      "[153, 55, 103] [153, 55, 103]\n",
      "Train loss: 0.01338563962718067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  90%|████████████████████████████████████▉    | 9/10 [09:58<01:06, 66.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Eval loss: 0.001979693048633635\n",
      "\t Eval ACC: 1.0\n",
      "\t Eval MCC: 1.0\n",
      "\t Eval MCC 1 vs other: 1.0\n",
      "[153, 55, 103] [153, 55, 103]\n",
      "Train loss: 0.027174129454500002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|████████████████████████████████████████| 10/10 [11:04<00:00, 66.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Eval loss: 0.0017233240650966763\n",
      "\t Eval ACC: 1.0\n",
      "\t Eval MCC: 1.0\n",
      "\t Eval MCC 1 vs other: 1.0\n",
      "[153, 55, 103] [153, 55, 103]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "best_MCC = 0\n",
    "best_MCCNC = 0\n",
    "best_loss = 999\n",
    "best_acc = 0\n",
    "best_state_dict = model.state_dict()\n",
    "best_epoch = 0\n",
    "\n",
    "for e in trange(epochs, desc=\"Epoch\"):\n",
    "\n",
    "    # Training\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        if 'roberta' in model_name:\n",
    "            b_input_ids, b_input_mask, b_labels = batch\n",
    "        else:    \n",
    "            b_input_ids, b_input_mask, b_labels, b_token_type_ids = batch\n",
    "\n",
    "        if not classification:\n",
    "            b_labels = b_labels.view(-1, 1)        \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        \n",
    "        if 'roberta' in model_name:\n",
    "            outputs = model(b_input_ids, b_input_mask, b_labels)\n",
    "        else:\n",
    "            outputs = model(b_input_ids, b_token_type_ids, b_input_mask, b_labels)\n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "        \n",
    "        #print(step, loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "\n",
    "    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "\n",
    "\n",
    "    # Testing\n",
    "\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    labels = []\n",
    "\n",
    "    eval_loss = 0\n",
    "    steps=0\n",
    "    for step, batch in enumerate(test_dataloader):\n",
    "\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        if 'roberta' in model_name:\n",
    "            b_input_ids, b_input_mask, b_labels, ids = batch\n",
    "        else:    \n",
    "            b_input_ids, b_input_mask, b_labels, b_token_type_ids, ids = batch\n",
    "\n",
    "        if not classification:\n",
    "            b_labels = b_labels.view(-1, 1)        \n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            if 'roberta' in model_name:\n",
    "                outputs = model(b_input_ids, b_input_mask, b_labels)\n",
    "            else:\n",
    "                outputs = model(b_input_ids, b_token_type_ids, b_input_mask, b_labels)\n",
    "            logits = outputs[1]\n",
    "            loss = outputs[0]\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        ground_truth = b_labels.detach().cpu().numpy()\n",
    "\n",
    "        steps+=1\n",
    "        eval_loss+=loss.detach().item()\n",
    "\n",
    "        \n",
    "        for p in logits:\n",
    "            if classification:\n",
    "                pred = p.argmax()\n",
    "            else:\n",
    "                pred = round_regression(p)\n",
    "            predictions.append(pred)\n",
    "\n",
    "        for gt in ground_truth:\n",
    "            labels.append(gt)\n",
    "    \n",
    "    scheduler.step(eval_loss/steps)\n",
    "    \n",
    "    LOSS = eval_loss/steps\n",
    "    ACC = metrics.accuracy_score(labels, predictions)\n",
    "    MCC = metrics.matthews_corrcoef(labels, predictions)\n",
    "    MCCNC = metrics.matthews_corrcoef(np.array(labels)>0, np.array(predictions)>0)\n",
    "    \n",
    "    if MCC>best_MCC:\n",
    "        best_loss = LOSS\n",
    "        best_acc = ACC\n",
    "        best_MCC =MCC\n",
    "        best_MCCNC = MCCNC\n",
    "        best_epoch = e\n",
    "        best_state_dict = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    print(\"\\t Eval loss: {}\".format(LOSS))\n",
    "    print(\"\\t Eval ACC: {}\".format(ACC))\n",
    "    print(\"\\t Eval MCC: {}\".format(MCC))\n",
    "    print(\"\\t Eval MCC 1 vs other: {}\".format(MCCNC))\n",
    "    print([predictions.count(i) for i in range(0,3)], [labels.count(i) for i in range(0, 3)])\n",
    "\n",
    "#torch.save(best_state_dict, './Models/task1/roberta_CV'+str(k)+'_e'+str(best_epoch)+'_'+str(round(best_MCC, 3))+'_classification.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8db86a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best epoch  6\n",
      "\t Eval loss: 0.008718459773808718\n",
      "\t Eval ACC: 1.0\n",
      "\t Eval MCC: 1.0\n",
      "\t Eval MCC 1 vs other: 1.0\n"
     ]
    }
   ],
   "source": [
    "print('Best epoch ', best_epoch)\n",
    "print(\"\\t Eval loss: {}\".format(best_loss))\n",
    "print(\"\\t Eval ACC: {}\".format(best_acc))\n",
    "print(\"\\t Eval MCC: {}\".format(best_MCC))\n",
    "print(\"\\t Eval MCC 1 vs other: {}\".format(best_MCCNC))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5c7f5914",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(best_state_dict, './Models/task1/'+model_name+'_cv'+str(k)+'.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
