{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "358b7706",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import deque\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertForPreTraining\n",
    "from transformers import BertModel, AdamW, AutoTokenizer, BertForSequenceClassification, RobertaForSequenceClassification\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import emoji\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "random_seed = 0\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "conspiracies = ['Suppressed Cures',\n",
    "     'Behaviour and Mind Control',\n",
    "     'Antivax',\n",
    "     'Fake virus',\n",
    "     'Intentional Pandemic',\n",
    "     'Harmful Radiation/ Influence',\n",
    "     'Population reduction',\n",
    "     'New World Order',\n",
    "     'Satanism']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04bbfb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_name = 'bert-base-cased'\n",
    "# model_name = 'roberta-base'\n",
    "model_name = 'twitter'\n",
    "# model_name = 'roberta-large'\n",
    "\n",
    "replace_lowercase_flag = False\n",
    "remove_stopwords_flag = False\n",
    "remove_hashtags_flag = True\n",
    "replace_emojis_flag = True\n",
    "clean_tweets_flag = False\n",
    "\n",
    "all_data = False\n",
    "class_weights_flag = True # always true for now\n",
    "\n",
    "classification = True\n",
    "\n",
    "# fold\n",
    "k=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2abe94ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweets(tweets):\n",
    "    char_to_remove = ['\\n', '\\xa0']\n",
    "    corona_synonyms = ['coronavirus',\n",
    "                      'covid-19',\n",
    "                      'covid19',\n",
    "                      'covid 19',\n",
    "                      'covid',\n",
    "                      'corona',\n",
    "                      'sarscov2'\n",
    "                      'sars',\n",
    "                      'Coronaviruses',\n",
    "                      'Coronavirus',\n",
    "                      'Corona',\n",
    "                      'Covid19',\n",
    "                      'COVID19',\n",
    "                      'Covid-19',\n",
    "                      'COVID-19',\n",
    "                      'COVID 19',\n",
    "                      'Covid',\n",
    "                      'COVID',\n",
    "                      'SARSCOV2',\n",
    "                      'SARS']\n",
    "    \n",
    "    tweets_clean = []\n",
    "    for tw in tweets:\n",
    "        for c in char_to_remove:\n",
    "            tw = tw.replace(c, '')\n",
    "        tw = tw.replace('&amp;', '&')\n",
    "        \n",
    "        for syn in corona_synonyms:\n",
    "            if syn in tw:\n",
    "                tw = tw.replace(syn, 'wuhan virus')\n",
    "        tweets_clean.append(tw)\n",
    "    return tweets_clean\n",
    "\n",
    "def extract_hashtags(tweet):\n",
    "    # Returns hashtags in a list for a given tweet\n",
    "    \n",
    "    #tweet = tweet.replace('\\xa0','')\n",
    "    #tweet = tweet.replace('\\n','')\n",
    "    \n",
    "    tweet_words = tweet.split(' ')\n",
    "    tweet_words = [w for w in tweet_words if w!='']\n",
    "    hashtags = []\n",
    "    for word in tweet_words:\n",
    "        if word[0]=='#':\n",
    "            hashtags.append(word)\n",
    "    return hashtags\n",
    "\n",
    "def extract_emojis (tw):\n",
    "    # Returns emojis in a list for a given tweet\n",
    "    # Using Deque for a sliding window (emojis can be combined together to form other emojis)\n",
    "    \n",
    "    emojis = []\n",
    "    \n",
    "    l = []\n",
    "    max_l = 7\n",
    "    \n",
    "    for i in range(0, max_l):\n",
    "        l.append(tw[-1-i])\n",
    "    l = deque(l, maxlen=max_l)\n",
    "    skip=0\n",
    "    \n",
    "    for i in range (0, len(tw)):\n",
    "        if skip == 0:\n",
    "            for j in range (max_l-1, -1, -1):\n",
    "                str_to_test = ''\n",
    "                for k in range (0, j+1):\n",
    "                    str_to_test+=l[j-k]\n",
    "                if str_to_test in emoji.UNICODE_EMOJI['en']:\n",
    "                    \n",
    "                    emojis.append(str_to_test)\n",
    "                    skip=j\n",
    "                    break\n",
    "            try:\n",
    "                l.append(tw[-1-i-max_l])\n",
    "            except IndexError:\n",
    "                l.append('')\n",
    "        else:\n",
    "            skip=skip-1\n",
    "            try:\n",
    "                l.append(tw[-1-i-max_l])\n",
    "            except IndexError:\n",
    "                l.append('')\n",
    "    emojis.reverse()\n",
    "    return emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a03dccbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_lowercase(tweets):\n",
    "    tweets_lowercase = []\n",
    "    for tw in tweets:\n",
    "        tweets_lowercase.append(tw.lower())\n",
    "    return tweets_lowercase\n",
    "\n",
    "def remove_stopwords(tweets):\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    tweets_no_stopwords = []\n",
    "    for tw in tweets:\n",
    "        tw = tw.split(' ')\n",
    "        tweets_no_stopwords.append(' '.join([word for word in tw if not word in stop_words]))\n",
    "\n",
    "    return tweets_no_stopwords\n",
    "\n",
    "def remove_hashtags(tweets):\n",
    "    tweets = [tw.replace('#', '') for tw in tweets]\n",
    "    return tweets\n",
    "\n",
    "def replace_emojis(tweets):\n",
    "    tweets_no_emojis = []\n",
    "    for tw in tweets:\n",
    "        emojis = extract_emojis(tw)\n",
    "        for e in emojis:\n",
    "            e_text = emoji.UNICODE_EMOJI['en'][e].replace('_',' ').replace(':', '')\n",
    "            tw = tw.replace(e, e_text)\n",
    "        tweets_no_emojis.append(tw)\n",
    "\n",
    "    return tweets_no_emojis\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0c19f33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([44.3929, 10.9035,  8.1242,  6.8297,  6.7189, 27.0217, 11.3000, 12.4300,\n",
       "        19.4219], device='cuda:0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = './data/task2/'\n",
    "filelist = os.listdir(data_path)\n",
    "\n",
    "\n",
    "df_list = [pd.read_csv(data_path+file) for file in filelist]\n",
    "\n",
    "\n",
    "test_df = df_list[k]    \n",
    "train_df = pd.concat(df_list[:k]+df_list[k+1:])\n",
    "\n",
    "\n",
    "tw_train = train_df['tweet'].tolist()\n",
    "tw_test = test_df['tweet'].tolist()\n",
    "\n",
    "if all_data:\n",
    "    df = pd.read_csv('./data/dev-full-task-2-clean.csv')\n",
    "    tw_train = df['tweet']\n",
    "    labels_train = df.iloc[:,1:10].values.tolist()\n",
    "\n",
    "if clean_tweets_flag:\n",
    "    tw_train = clean_tweets(tw_train)\n",
    "    tw_test = clean_tweets(tw_test)\n",
    "\n",
    "if replace_lowercase_flag:\n",
    "    tw_train = to_lowercase(tw_train)\n",
    "    tw_test = to_lowercase(tw_test)\n",
    "\n",
    "if remove_stopwords_flag:\n",
    "    tw_train = remove_stopwords(tw_train)\n",
    "    tw_test = remove_stopwords(tw_test)\n",
    "\n",
    "if remove_hashtags_flag:\n",
    "    tw_train = remove_hashtags(tw_train)\n",
    "    tw_test = remove_hashtags(tw_test)\n",
    "\n",
    "if replace_emojis_flag:\n",
    "    tw_train = replace_emojis(tw_train)\n",
    "    tw_test = replace_emojis(tw_test)\n",
    "\n",
    "weights_tmp = [0,0,0,0,0,0,0,0,0]\n",
    "weights = [1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "if not all_data:\n",
    "    labels_train = train_df.iloc[:,1:10].values.tolist()\n",
    "labels_test = test_df.iloc[:,1:10].values.tolist()\n",
    "ids_test = test_df['ids'].tolist()\n",
    "\n",
    "for i in range(0, 9):\n",
    "    for j in range(0, len(labels_train)):\n",
    "        weights_tmp[i]+=labels_train[j][i]\n",
    "        \n",
    "if class_weights_flag:\n",
    "    weights = [len(labels_train)/w for w in weights_tmp]\n",
    "else:\n",
    "    weights = [1,1,1,1,1,1,1,1,1]\n",
    "#weights = [1/w for w in weights_tmp]\n",
    "\n",
    "weights = torch.FloatTensor(weights).cuda()\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32a72ced",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if 'twitter' in model_name:\n",
    "    tokenizer = AutoTokenizer.from_pretrained('digitalepidemiologylab/covid-twitter-bert')\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenized_input = tokenizer(tw_train)\n",
    "\n",
    "m = 0\n",
    "for tokens in tokenized_input['input_ids']:\n",
    "    if len(tokens)>m:\n",
    "        m=len(tokens)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbb366dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_b_size = {'roberta-base':32,\n",
    "                 'bert-base-cased':32,\n",
    "                 'twitter':12,\n",
    "                 'roberta-large':10}\n",
    "batch_size = models_b_size[model_name]\n",
    "\n",
    "if 'roberta' in model_name:\n",
    "    train_data = TensorDataset(train_input_ids, train_attention_mask, train_labels)\n",
    "    test_data = TensorDataset(test_input_ids, test_attention_mask, test_labels, test_ids)\n",
    "    \n",
    "else:\n",
    "    train_data = TensorDataset(train_input_ids, train_attention_mask, train_labels, train_token_type_ids)\n",
    "    test_data = TensorDataset(test_input_ids, test_attention_mask, test_labels, test_token_type_ids, test_ids)\n",
    "\n",
    "    \n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d922f9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0e9de3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_classes):\n",
    "        super().__init__()\n",
    "        self.n_classes = n_classes\n",
    "        #self.bert = BertForPreTraining.from_pretrained('digitalepidemiologylab/covid-twitter-bert-v2')    \n",
    "        self.bert = BertForSequenceClassification.from_pretrained(model_name, num_labels=n_classes)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        #self.classifier = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "        #self.bert.cls.seq_relationship = nn.Linear(1024, n_classes)\n",
    "        \n",
    "        if n_classes >1:\n",
    "            self.criterion = nn.BCELoss()\n",
    "        else:\n",
    "            self.criterion = nn.MSELoss()\n",
    "        \n",
    "    def forward(self, input_ids, token_type_ids, input_mask, labels):\n",
    "        outputs = self.bert(input_ids = input_ids, token_type_ids = token_type_ids, attention_mask = input_mask)\n",
    "        #outputs = self.classifier(outputs.pooler_output)\n",
    "        \n",
    "        logits = self.sigmoid(outputs[0])\n",
    "        \n",
    "        loss = self.criterion(logits, labels)\n",
    "        loss = (loss * weights).mean()\n",
    "        \n",
    "        \n",
    "        return loss, logits\n",
    "\n",
    "class CovidTwitterBertClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_classes):\n",
    "        super().__init__()\n",
    "        self.n_classes = n_classes\n",
    "        self.bert = BertForPreTraining.from_pretrained('digitalepidemiologylab/covid-twitter-bert-v2')    \n",
    "        #self.bert = BertForSequenceClassification.from_pretrained(model_name, num_labels=n_classes)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        #self.classifier = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "        self.bert.cls.seq_relationship = nn.Linear(1024, n_classes)\n",
    "        \n",
    "        if n_classes >1:\n",
    "            self.criterion = nn.BCELoss(reduction='none')\n",
    "        else:\n",
    "            self.criterion = nn.MSELoss()\n",
    "        \n",
    "    def forward(self, input_ids, token_type_ids, input_mask, labels):\n",
    "        outputs = self.bert(input_ids = input_ids, token_type_ids = token_type_ids, attention_mask = input_mask)\n",
    "        #outputs = self.classifier(outputs.pooler_output)\n",
    "        logits = outputs[1]\n",
    "        logits = self.sigmoid(logits)\n",
    "        \n",
    "        loss = self.criterion(logits, labels)\n",
    "        loss = (loss * weights).mean()\n",
    "        \n",
    "        \n",
    "        return loss, logits\n",
    "    \n",
    "    \n",
    "class RobertaClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_classes):\n",
    "        super().__init__()\n",
    "        self.n_classes = n_classes\n",
    "        self.bert = RobertaForSequenceClassification.from_pretrained(model_name, num_labels=n_classes)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        if n_classes >1:\n",
    "            self.criterion = nn.BCELoss(reduction='none')\n",
    "            #self.criterion = nn.BCEWithLogitsLoss()\n",
    "            \n",
    "        else:\n",
    "            self.criterion = nn.MSELoss()\n",
    "        \n",
    "    def forward(self, input_ids, input_mask, labels):\n",
    "        outputs = self.bert(input_ids, input_mask)\n",
    "        #outputs = self.classifier(outputs.pooler_output)\n",
    "        logits = outputs[0]\n",
    "        logits = self.sigmoid(logits)\n",
    "        \n",
    "        if self.n_classes == 1:\n",
    "            labels=labels.float()\n",
    "        loss = self.criterion(logits, labels)\n",
    "        loss = (loss * weights).mean()\n",
    "        \n",
    "        return loss, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28149c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaClassifier(\n",
       "  (bert): RobertaForSequenceClassification(\n",
       "    (roberta): RobertaModel(\n",
       "      (embeddings): RobertaEmbeddings(\n",
       "        (word_embeddings): Embedding(50265, 1024, padding_idx=1)\n",
       "        (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
       "        (token_type_embeddings): Embedding(1, 1024)\n",
       "        (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): RobertaEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (3): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (4): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (5): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (6): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (7): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (8): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (9): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (10): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (11): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (12): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (13): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (14): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (15): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (16): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (17): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (18): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (19): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (20): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (21): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (22): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (23): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (classifier): RobertaClassificationHead(\n",
       "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (out_proj): Linear(in_features=1024, out_features=9, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (sigmoid): Sigmoid()\n",
       "  (criterion): BCELoss()\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if 'roberta' in model_name:\n",
    "    model = RobertaClassifier(9)\n",
    "elif 'twitter' in model_name:\n",
    "    model = CovidTwitterBertClassifier(9)\n",
    "else:\n",
    "    model = BertClassifier(9)\n",
    "    \n",
    "    \n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d39099d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer_grouped_parameters\n",
    "# lr 5e-5 for base models\n",
    "# lr 7e-6 for larger models\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr=7e-6,\n",
    "                  weight_decay = 0.001)\n",
    "\n",
    "scheduler = ReduceLROnPlateau(optimizer, patience=4, factor=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d29ce0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_regression(val):\n",
    "    if val<0.5:\n",
    "        return 0\n",
    "    elif val<1.5:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b67f71",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   0%|                                                 | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 4.15667368221283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   3%|                                    | 1/30 [03:34<1:43:28, 214.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Eval loss: 3.38576590269804\n",
      "\t Eval ACC: 0.4855305466237942\n",
      "\t Eval MCCA: 0.0\n",
      "\t Eval MCCF: 0.0\n",
      "\t Eval MCCs: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "\t Eval MCC 1 vs other: 0.0\n",
      "\t Eval F1 weighted: 0.0\n",
      "Train loss: 3.2818857460021973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   7%|                                   | 2/30 [07:08<1:39:54, 214.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Eval loss: 2.694883955642581\n",
      "\t Eval ACC: 0.49517684887459806\n",
      "\t Eval MCCA: 0.16344438884748153\n",
      "\t Eval MCCF: 0.25266826996359093\n",
      "\t Eval MCCs: [0.0, 0.25577021009073103, 0.40414003022006, 0.0, 0.0, 0.8110892593165426, 0.0, 0.0, 0.0]\n",
      "\t Eval MCC 1 vs other: 0.23360346365431425\n",
      "\t Eval F1 weighted: 0.10810495286460613\n",
      "Train loss: 2.500204922199249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  10%|                                  | 3/30 [10:41<1:36:13, 213.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Eval loss: 2.13862815964967\n",
      "\t Eval ACC: 0.5530546623794212\n",
      "\t Eval MCCA: 0.4078677103884172\n",
      "\t Eval MCCF: 0.5048323020601615\n",
      "\t Eval MCCs: [0.0, 0.7333542672347239, 0.5197391466671093, 0.0, 0.0, 0.9098330807202929, 0.7856140711454555, 0.7222688277281736, 0.0]\n",
      "\t Eval MCC 1 vs other: 0.4412783365849191\n",
      "\t Eval F1 weighted: 0.3724528034800572\n",
      "Train loss: 1.882095663547516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  13%|                                 | 4/30 [14:15<1:32:37, 213.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Eval loss: 1.8714201068505645\n",
      "\t Eval ACC: 0.5852090032154341\n",
      "\t Eval MCCA: 0.5162407828255708\n",
      "\t Eval MCCF: 0.5446122067370258\n",
      "\t Eval MCCs: [0.5301802308720894, 0.7659710913812849, 0.5581446400094104, 0.0, 0.0, 0.8151982686453727, 0.8970672676222476, 0.72222082374386, 0.3573847231558724]\n",
      "\t Eval MCC 1 vs other: 0.4977314829734596\n",
      "\t Eval F1 weighted: 0.4139253603086656\n"
     ]
    }
   ],
   "source": [
    "epochs = 30\n",
    "\n",
    "threshold = 0.5\n",
    "\n",
    "best_MCCF = 0\n",
    "best_MCCA = 0\n",
    "best_F1 = 0\n",
    "best_MCCs = []\n",
    "best_MCCNC = 0\n",
    "best_loss = 999\n",
    "best_acc = 0\n",
    "best_state_dict = model.state_dict()\n",
    "best_epoch = 0\n",
    "\n",
    "for e in trange(epochs, desc=\"Epoch\"):\n",
    "\n",
    "    # Training\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        if 'roberta' in model_name:\n",
    "            b_input_ids, b_input_mask, b_labels = batch\n",
    "        else:    \n",
    "            b_input_ids, b_input_mask, b_labels, b_token_type_ids = batch\n",
    "            \n",
    "        if not classification:\n",
    "            b_labels = b_labels.view(-1, 1)        \n",
    "        \n",
    "        b_labels = b_labels.float()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if 'roberta' in model_name:\n",
    "            outputs = model(b_input_ids, b_input_mask, b_labels)\n",
    "        else:\n",
    "            outputs = model(b_input_ids, b_token_type_ids, b_input_mask, b_labels)\n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "        #print(step, loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "\n",
    "    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "    \n",
    "    \n",
    "    # Testing\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    tweets_test = []\n",
    "    \n",
    "    predictions = []\n",
    "    predictions_sep = [[], [], [], [], [], [], [], [], []]\n",
    "    \n",
    "    labels = []\n",
    "    labels_sep = [[], [], [], [], [], [], [], [], []]\n",
    "    \n",
    "    eval_loss = 0\n",
    "    steps=0\n",
    "    # Train the data for one epoch\n",
    "    for step, batch in enumerate(test_dataloader):\n",
    "\n",
    "        # Add batch to GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        if 'roberta' in model_name:\n",
    "            b_input_ids, b_input_mask, b_labels, ids = batch\n",
    "        else:    \n",
    "            b_input_ids, b_input_mask, b_labels, b_token_type_ids, ids = batch\n",
    "            \n",
    "        if not classification:\n",
    "            b_labels = b_labels.view(-1, 1)        \n",
    "        \n",
    "        b_labels = b_labels.float()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "\n",
    "            if 'roberta' in model_name:\n",
    "                outputs = model(b_input_ids, b_input_mask, b_labels)\n",
    "            else:\n",
    "                outputs = model(b_input_ids, b_token_type_ids, b_input_mask, b_labels)\n",
    "            logits = outputs[1]\n",
    "            loss = outputs[0]\n",
    "\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        ground_truth = b_labels.detach().cpu().numpy()\n",
    "        \n",
    "        steps+=1\n",
    "        eval_loss+=loss.detach().item()\n",
    "        \n",
    "        tweets_test.append(b_input_ids)\n",
    "        for p in logits:\n",
    "            if classification:\n",
    "                pred = p.argmax()\n",
    "            else:\n",
    "                pred = round_regression(p)\n",
    "            predictions.append(p>threshold)\n",
    "            for i in range(0, 9):\n",
    "                predictions_sep[i].append(p[i]>threshold)\n",
    "            \n",
    "        for gt in ground_truth:\n",
    "            labels.append(gt>threshold)\n",
    "            for i in range(0, 9):\n",
    "                labels_sep[i].append(gt[i]>threshold)\n",
    "        \n",
    "    MCCs = []\n",
    "    for i in range(0, 9):\n",
    "        MCCs.append(metrics.matthews_corrcoef(labels_sep[i], predictions_sep[i]))\n",
    "    labels_one = []\n",
    "    predictions_one = []\n",
    "    for l in labels:\n",
    "        if list(l) == [False, False, False, False, False, False, False, False, False]:\n",
    "            labels_one.append(0)\n",
    "        else:\n",
    "            labels_one.append(1)\n",
    "    for p in predictions:\n",
    "        if list(p) == [False, False, False, False, False, False, False, False, False]:\n",
    "            predictions_one.append(0)\n",
    "        else:\n",
    "            predictions_one.append(1)\n",
    "    \n",
    "    \n",
    "    scheduler.step(eval_loss/steps)\n",
    "    MCCF = metrics.matthews_corrcoef(np.array(labels).flatten(), np.array(predictions).flatten())\n",
    "    ACC = metrics.accuracy_score(labels, predictions)\n",
    "    LOSS = eval_loss/steps\n",
    "    MCCNC = metrics.matthews_corrcoef(labels_one, predictions_one)\n",
    "    F1 = metrics.f1_score(labels, predictions, average='weighted')\n",
    "    MCCA = np.array(MCCs).mean()\n",
    "    if MCCA> best_MCCA:\n",
    "        best_MCCF = MCCF\n",
    "        best_MCCA = MCCA\n",
    "        best_loss = LOSS\n",
    "        best_acc = ACC\n",
    "        best_F1 = F1\n",
    "        best_MCCs = MCCs\n",
    "        best_MCCNC = MCCNC\n",
    "        best_state_dict = copy.deepcopy(model.state_dict())\n",
    "        best_epoch = e\n",
    "    \n",
    "    print(\"\\t Eval loss: {}\".format(LOSS))\n",
    "    print(\"\\t Eval ACC: {}\".format(ACC))\n",
    "    print(\"\\t Eval MCCA: {}\".format(MCCA))\n",
    "    print(\"\\t Eval MCCF: {}\".format(MCCF))\n",
    "    print(\"\\t Eval MCCs: {}\".format(MCCs))\n",
    "    print(\"\\t Eval MCC 1 vs other: {}\".format(MCCNC))\n",
    "    #print(\"\\t Eval Kappa: {}\".format(metrics.cohen_kappa_score(np.array(labels).flatten(), np.array(predictions).flatten())))\n",
    "    print(\"\\t Eval F1 weighted: {}\".format(F1))\n",
    "    #print(\"\\t Eval F1 micro: {}\".format(metrics.f1_score(labels, predictions, average='micro')))\n",
    "    #print(\"\\t Eval F1 samples: {}\".format(metrics.f1_score(labels, predictions, average='samples')))\n",
    "    #print(\"\\t Eval F1 None: {}\".format(metrics.f1_score(labels, predictions, average=None)))\n",
    "    \n",
    "    #print([predictions.count(i) for i in range(0,3)], [labels.count(i) for i in range(0, 3)])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e5e0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best epoch ', best_epoch)\n",
    "print(\"\\t Eval loss: {}\".format(best_loss))\n",
    "print(\"\\t Eval ACC: {}\".format(best_acc))\n",
    "print(\"\\t Eval MCCA: {}\".format(best_MCCA))\n",
    "print(\"\\t Eval MCCF: {}\".format(best_MCCF))\n",
    "print(\"\\t Eval MCCs: {}\".format(best_MCCs))\n",
    "print(\"\\t Eval MCC 1 vs other: {}\".format(best_MCCNC))\n",
    "print(\"\\t Eval F1 weighted: {}\".format(best_F1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ea48b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(best_state_dict, '../Models/task2/'+model_name+'_cv'+str(k)+'.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
