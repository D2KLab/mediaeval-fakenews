{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "+ Sentiment Analysis / WordNet polarity?\n",
    "# MNLI / SNLI ?\n",
    "# Normalize SBERT\n",
    "+ Viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "from sklearn import metrics, preprocessing, linear_model\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "from sklearn.linear_model import SGDClassifier, SGDRegressor, LogisticRegression, LinearRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "en_stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/dev-full-task-1-clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = lambda i: {3: 'Promotes/Supports Conspiracy', 2: 'Discusses Conspiracy', 1:'Non-Conspiracy'}[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['label'] = data['1'].apply(id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['tweet', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tweet'][data['tweet'].str.contains('presidential election')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['1'].value_counts() / len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def remove_sw(tokens):\n",
    "    return [tok.text for tok in tokens if tok.is_stop is False]\n",
    "\n",
    "\n",
    "def remove_extra_spaces(tokens):\n",
    "    return [tok.strip() for tok in tokens]\n",
    "\n",
    "\n",
    "def remove_short_words(tokens):\n",
    "    return [tok for tok in tokens if len(tok) > 2]\n",
    "\n",
    "\n",
    "def remove_puntaction(tokens):\n",
    "    return [re.sub('[\\W]+', '', tok.lower()) for tok in tokens]\n",
    "\n",
    "\n",
    "def tokenizer_call(text, spacy_nlp=spacy_nlp):\n",
    "    tokenizer = spacy_nlp.tokenizer\n",
    "    tokens = tokenizer(text)\n",
    "    tokens = remove_sw(tokens)\n",
    "    tokens = remove_puntaction(tokens)\n",
    "    tokens = remove_extra_spaces(tokens)\n",
    "    tokens = remove_short_words(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(tokenizer=tokenizer_call, lowercase=True, ngram_range=(1, 3), min_df=2)\n",
    "tfidf_X = vectorizer.fit_transform(data.tweet.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_idx, X_test_idx, y_train, y_test = train_test_split(range(len(data)), y, stratify=y, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = tfidf_X[X_train_idx], tfidf_X[X_test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_X, y, stratify=y, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = linear_model.LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_weights = dict(zip(vectorizer.get_feature_names(), reg.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(word_weights.items(), key=lambda x: -x[1])[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer(min_df=2, stop_words='english', lowercase=True, ngram_range=(1, 3))\n",
    "X_vec = cv.fit_transform(data.tweet.values)\n",
    "\n",
    "mut_information_scores = dict(zip(cv.get_feature_names(), mutual_info_classif(X_vec, data['1'], discrete_features=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(mut_information_scores.items(), key=lambda x: x[1])[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(mut_information_scores.items(), key=lambda x: -x[1])[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls ../data/task2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folds = [pd.read_csv(f'../data/task1/dev-full-split-{i}.csv') for i in range(5)]\n",
    "folds = [pd.read_csv(f'../data/task1/dev-full-split-{i}.csv').ids.values for i in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data.ids.isin(folds[0])].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(s):\n",
    "    return ' '.join(tokenizer_call(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(tokenizer=tokenizer_call, lowercase=True, ngram_range=(1, 3), min_df=2)\n",
    "tfidf_X = vectorizer.fit_transform(data.tweet.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeMCC(y_test, y_pred):\n",
    "    value = 0\n",
    "    for y1, y2 in zip(y_test, y_pred):\n",
    "        try:\n",
    "            value += metrics.matthews_corrcoef(y1, y2)\n",
    "        except ValueError:\n",
    "            print(y1)\n",
    "            print(y2)\n",
    "            exit(1)\n",
    "    mcc = value / len(y_test)\n",
    "    return mcc\n",
    "\n",
    "def computeMCCclass(y_test, y_pred):\n",
    "    mccs = []\n",
    "    for i in range(len(y_test[0,:])):\n",
    "        mccs.append(metrics.matthews_corrcoef(y_test[:,i], y_pred[:,i]))\n",
    "    return np.mean(mccs)\n",
    "\n",
    "def one_hot_encoding(labels):\n",
    "    dictionary = {1: [0, 0, 1],\n",
    "                  2: [0, 1, 0],\n",
    "                  3: [1, 0, 0]}\n",
    "    enc_labels = []\n",
    "    for el in labels:\n",
    "        enc_labels.append(dictionary[el])\n",
    "    return np.array(enc_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['1'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {c: 1/np.log(v) for c, v in data['1'].value_counts().items()}\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_sgd_log = []\n",
    "for i, fold in enumerate(folds):\n",
    "    train_indices = data[~data.ids.isin(fold)].index\n",
    "    test_indices = data[data.ids.isin(fold)].index\n",
    "    \n",
    "    X_train, X_test = tfidf_X[train_indices], tfidf_X[test_indices]\n",
    "    Y_train, Y_test = data.iloc[train_indices]['1'], data.iloc[test_indices]['1']\n",
    "\n",
    "    # sgd = SGDClassifier('log')\n",
    "    sgd = SGDClassifier('log', class_weight='balanced')\n",
    "    sgd.fit(X_train, Y_train)\n",
    "    \n",
    "    Y_pred = sgd.predict(X_test)\n",
    "    Y_prob = sgd.predict_proba(X_test)\n",
    "    acc = metrics.accuracy_score(Y_test, Y_pred)\n",
    "    f1s = metrics.f1_score(Y_test, Y_pred, average='weighted')\n",
    "    auc = metrics.roc_auc_score(Y_test, Y_prob, average='weighted', multi_class='ovr')\n",
    "    mcc = computeMCC(one_hot_encoding(Y_test), one_hot_encoding(Y_pred))\n",
    "    mccc = computeMCCclass(one_hot_encoding(Y_test), one_hot_encoding(Y_pred))\n",
    "    print(mccc)\n",
    "    \n",
    "    scores_sgd_log.append({'ACC':acc, 'F1':f1s, 'AUC':auc, 'MCC': mcc, 'MCCC':mccc})\n",
    "    \n",
    "    print(f'For fold {i}:', ' - '.join(f'{m}: {s:.4}' for m,s in scores_sgd_log[-1].items()))\n",
    "\n",
    "scores_sgd_log = pd.DataFrame(scores_sgd_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_sgd_log.describe().loc['mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_sgd_log.describe().loc['mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stsb-xlm-r-multilingual\n",
    "#distiluse-base-multilingual-cased-v1\n",
    "#paraphrase-xlm-r-multilingual-v1\n",
    "#sentence-transformers/all-mpnet-base-v2\n",
    "\n",
    "sbert1 = SentenceTransformer('facebook/bart-large-mnli') \n",
    "sbert2 = SentenceTransformer('distilbert-base-nli-stsb-mean-tokens')\n",
    "# sbert3 = SentenceTransformer('multi-qa-mpnet-base-dot-v1') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sbert1_X = sbert1.encode(data.tweet.values)\n",
    "sbert1_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sbert2_X = sbert2.encode(data.tweet.values)\n",
    "sbert2_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_sbert1 = []\n",
    "for i, fold in enumerate(folds):\n",
    "    train_indices = data[~data.ids.isin(fold)].index\n",
    "    test_indices = data[data.ids.isin(fold)].index\n",
    "    \n",
    "    X_train, X_test = sbert1_X[train_indices], sbert1_X[test_indices]\n",
    "    Y_train, Y_test = data.iloc[train_indices]['1'], data.iloc[test_indices]['1']\n",
    "\n",
    "    sgd = SGDClassifier('log')\n",
    "    sgd.fit(X_train, Y_train)\n",
    "    \n",
    "    Y_pred = sgd.predict(X_test)\n",
    "    Y_prob = sgd.predict_proba(X_test)\n",
    "    acc = metrics.accuracy_score(Y_test, Y_pred)\n",
    "    f1s = metrics.f1_score(Y_test, Y_pred, average='weighted')\n",
    "    auc = metrics.roc_auc_score(Y_test, Y_prob, average='weighted', multi_class='ovr')\n",
    "    mcc = computeMCC(one_hot_encoding(Y_test), one_hot_encoding(Y_pred))\n",
    "    mccc = computeMCCclass(one_hot_encoding(Y_test), one_hot_encoding(Y_pred))\n",
    "    print(mccc)\n",
    "    \n",
    "    scores_sbert1.append({'ACC':acc, 'F1':f1s, 'AUC':auc, 'MCC': mcc, 'MCCC':mccc})\n",
    "    \n",
    "    print(f'For fold {i}:', ' - '.join(f'{m}: {s:.4}' for m,s in scores_sbert1[-1].items()))\n",
    "\n",
    "scores_sbert1_log = pd.DataFrame(scores_sbert1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(sbert2_X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_sbert2 = []\n",
    "for i, fold in enumerate(folds):\n",
    "    train_indices = data[~data.ids.isin(fold)].index\n",
    "    test_indices = data[data.ids.isin(fold)].index\n",
    "    \n",
    "    X_train, X_test = sbert2_X[train_indices], sbert2_X[test_indices]\n",
    "    Y_train, Y_test = data.iloc[train_indices]['1'], data.iloc[test_indices]['1']\n",
    "\n",
    "    sgd = SGDClassifier('log')\n",
    "    sgd.fit(X_train, Y_train)\n",
    "    \n",
    "    Y_pred = sgd.predict(X_test)\n",
    "    Y_prob = sgd.predict_proba(X_test)\n",
    "    acc = metrics.accuracy_score(Y_test, Y_pred)\n",
    "    f1s = metrics.f1_score(Y_test, Y_pred, average='weighted')\n",
    "    auc = metrics.roc_auc_score(Y_test, Y_prob, average='weighted', multi_class='ovr')\n",
    "    mcc = computeMCC(one_hot_encoding(Y_test), one_hot_encoding(Y_pred))\n",
    "    mccc = computeMCCclass(one_hot_encoding(Y_test), one_hot_encoding(Y_pred))\n",
    "    print(mccc)\n",
    "    \n",
    "    scores_sbert2.append({'ACC':acc, 'F1':f1s, 'AUC':auc, 'MCC': mcc, 'MCCC':mccc})\n",
    "    \n",
    "    print(f'For fold {i}:', ' - '.join(f'{m}: {s:.4}' for m,s in scores_sbert2[-1].items()))\n",
    "\n",
    "scores_sbert2_log = pd.DataFrame(scores_sbert2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_sbert1 = []\n",
    "for i, fold in enumerate(folds):\n",
    "    train_indices = data[~data.ids.isin(fold)].index\n",
    "    test_indices = data[data.ids.isin(fold)].index\n",
    "    \n",
    "    X_train, X_test = sbert1_X[train_indices], sbert1_X[test_indices]\n",
    "    Y_train, Y_test = data.iloc[train_indices]['1'], data.iloc[test_indices]['1']\n",
    "\n",
    "    knn = KNeighborsClassifier(n_neighbors=5)\n",
    "    knn.fit(X_train, Y_train)\n",
    "    \n",
    "    Y_pred = knn.predict(X_test)\n",
    "    Y_prob = knn.predict_proba(X_test)\n",
    "    acc = metrics.accuracy_score(Y_test, Y_pred)\n",
    "    f1s = metrics.f1_score(Y_test, Y_pred, average='weighted')\n",
    "    auc = metrics.roc_auc_score(Y_test, Y_prob, average='weighted', multi_class='ovr')\n",
    "    mcc = computeMCC(one_hot_encoding(Y_test), one_hot_encoding(Y_pred))\n",
    "    mccc = computeMCCclass(one_hot_encoding(Y_test), one_hot_encoding(Y_pred))\n",
    "    print(mccc)\n",
    "    \n",
    "    scores_sbert1.append({'ACC':acc, 'F1':f1s, 'AUC':auc, 'MCC': mcc, 'MCCC':mccc})\n",
    "    \n",
    "    print(f'For fold {i}:', ' - '.join(f'{m}: {s:.4}' for m,s in scores_sbert1[-1].items()))\n",
    "\n",
    "scores_sbert1_knn = pd.DataFrame(scores_sbert1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_sbert1_X = preprocessing.StandardScaler().fit_transform(sbert1_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(scaled_sbert1_X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_sbert1 = []\n",
    "for i, fold in enumerate(folds):\n",
    "    train_indices = data[~data.ids.isin(fold)].index\n",
    "    test_indices = data[data.ids.isin(fold)].index\n",
    "    \n",
    "    X_train, X_test = scaled_sbert1_X[train_indices], scaled_sbert1_X[test_indices]\n",
    "    Y_train, Y_test = data.iloc[train_indices]['1'], data.iloc[test_indices]['1']\n",
    "\n",
    "    knn = KNeighborsClassifier(n_neighbors=30)\n",
    "    knn.fit(X_train, Y_train)\n",
    "    \n",
    "    Y_pred = knn.predict(X_test)\n",
    "    Y_prob = knn.predict_proba(X_test)\n",
    "    acc = metrics.accuracy_score(Y_test, Y_pred)\n",
    "    f1s = metrics.f1_score(Y_test, Y_pred, average='weighted')\n",
    "    auc = metrics.roc_auc_score(Y_test, Y_prob, average='weighted', multi_class='ovr')\n",
    "    mcc = computeMCC(one_hot_encoding(Y_test), one_hot_encoding(Y_pred))\n",
    "    mccc = computeMCCclass(one_hot_encoding(Y_test), one_hot_encoding(Y_pred))\n",
    "    print(mccc)\n",
    "    \n",
    "    scores_sbert1.append({'ACC':acc, 'F1':f1s, 'AUC':auc, 'MCC': mcc, 'MCCC':mccc})\n",
    "    \n",
    "    print(f'For fold {i}:', ' - '.join(f'{m}: {s:.4}' for m,s in scores_sbert1[-1].items()))\n",
    "\n",
    "scores_sbert1_scaled_knn = pd.DataFrame(scores_sbert1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(0.4116 + 0.4791 + 0.4164 + 0.3468 + 0.3971)/5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [s.replace('&amp; ', '').split() for s in data.tweet.str.lower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "T  = {}\n",
    "for n_topics in tqdm([20, 50, 100]):\n",
    "    for random_state in tqdm([0, 1]):\n",
    "        print(f'N = {n_topics}, seed = {random_state}')\n",
    "        # Create Dictionary\n",
    "        docs_filtered = [[w for w in d if w not in en_stopwords] for d in docs]\n",
    "        id2word = corpora.Dictionary(docs_filtered)\n",
    "\n",
    "        # Term Document Frequency\n",
    "        corpus = [id2word.doc2bow(text) for text in docs_filtered]\n",
    "\n",
    "        num_topics = n_topics\n",
    "        lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                                   id2word=id2word,\n",
    "                                                   num_topics=num_topics, \n",
    "                                                   random_state=random_state,\n",
    "                                                   passes=10, # 30 is too good\n",
    "                                                   alpha='auto',\n",
    "                                                   eta='auto',\n",
    "                                                   per_word_topics=True,\n",
    "                                                   minimum_probability=0.05)\n",
    "\n",
    "        for t in lda_model.show_topics(40, num_words=8):\n",
    "            print('Topic', t[0], end=': ')\n",
    "            for w in t[1].split(' + '):\n",
    "                print(w.split('*')[1], end=', ')\n",
    "            print('')\n",
    "\n",
    "\n",
    "        preds = [lda_model[p] for p in corpus]\n",
    "        topic_docs = np.zeros((len(corpus), n_topics))\n",
    "\n",
    "        for i, d in enumerate(preds):\n",
    "            for (j, p) in d[0]:\n",
    "                topic_docs[i][j] = p\n",
    "\n",
    "        T['N='+str(n_topics)+', seed=' + str(random_state)] = topic_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_tm20_0 = []\n",
    "for i, fold in enumerate(folds):\n",
    "    train_indices = data[~data.ids.isin(fold)].index\n",
    "    test_indices = data[data.ids.isin(fold)].index\n",
    "    \n",
    "    X_train, X_test = T['N=20, seed=1'][train_indices], T['N=20, seed=1'][test_indices]\n",
    "    Y_train, Y_test = data.iloc[train_indices]['1'], data.iloc[test_indices]['1']\n",
    "\n",
    "    sgd = SGDClassifier('log')\n",
    "    sgd.fit(X_train, Y_train)\n",
    "    \n",
    "    Y_pred = sgd.predict(X_test)\n",
    "    Y_prob = sgd.predict_proba(X_test)\n",
    "    acc = metrics.accuracy_score(Y_test, Y_pred)\n",
    "    f1s = metrics.f1_score(Y_test, Y_pred, average='weighted')\n",
    "    auc = metrics.roc_auc_score(Y_test, Y_prob, average='weighted', multi_class='ovr')\n",
    "    mcc = computeMCC(one_hot_encoding(Y_test), one_hot_encoding(Y_pred))\n",
    "    mccc = computeMCCclass(one_hot_encoding(Y_test), one_hot_encoding(Y_pred))\n",
    "    print(mccc)\n",
    "    \n",
    "    scores_tm20_0.append({'ACC':acc, 'F1':f1s, 'AUC':auc, 'MCC': mcc, 'MCCC':mccc})\n",
    "    \n",
    "    print(f'For fold {i}:', ' - '.join(f'{m}: {s:.4}' for m,s in scores_tm20_0[-1].items()))\n",
    "\n",
    "scores_tm20_0 = pd.DataFrame(scores_tm20_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_tm20_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = util.pytorch_cos_sim(sbert2_X, sbert2_X).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_df = pd.DataFrame(sim)\n",
    "sim_df['label'] = data['1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_df[data['1'] == 1][0].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_df[data['1'] == 2][0].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_df[data['1'] == 3][0].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_nli_1nn = []\n",
    "\n",
    "for i, fold in enumerate(folds):\n",
    "    train_indices = data[~data.ids.isin(fold)].index\n",
    "    test_indices = data[data.ids.isin(fold)].index\n",
    "    \n",
    "    X_train, X_test = sbert1_X[train_indices], sbert1_X[test_indices]\n",
    "    Y_train, Y_test = data.iloc[train_indices]['1'], data.iloc[test_indices]['1']\n",
    "\n",
    "    Y_pred = []\n",
    "    Y_prob  = []\n",
    "    for i in test_indices:\n",
    "        sim_scores = sim_df.iloc[i]\n",
    "        sorted_scores_idx = np.argsort(-sim_scores)\n",
    "        for j in sorted_scores_idx:\n",
    "            if j in train_indices:\n",
    "                cls = data['1'].iloc[j]\n",
    "                Y_pred.append(cls)\n",
    "                Y_prob.append([[1, 0, 0], [0, 1, 0], [0, 0, 1]][cls - 1])\n",
    "                break\n",
    "    \n",
    "    acc = metrics.accuracy_score(Y_test, Y_pred)\n",
    "    f1s = metrics.f1_score(Y_test, Y_pred, average='weighted')\n",
    "    auc = metrics.roc_auc_score(Y_test, Y_prob, average='weighted', multi_class='ovr')\n",
    "    mcc = computeMCC(one_hot_encoding(Y_test), one_hot_encoding(Y_pred))\n",
    "    mccc = computeMCCclass(one_hot_encoding(Y_test), one_hot_encoding(Y_pred))\n",
    "    print(mccc)\n",
    "    \n",
    "    scores_nli_1nn.append({'ACC':acc, 'F1':f1s, 'AUC':auc, 'MCC': mcc, 'MCCC':mccc})\n",
    "    \n",
    "    print(f'For fold {i}:', ' - '.join(f'{m}: {s:.4}' for m,s in scores_nli_1nn[-1].items()))\n",
    "\n",
    "scores_nli_knn = pd.DataFrame(scores_nli_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_nli_knn = []\n",
    "\n",
    "for k, fold in enumerate(folds):\n",
    "    train_indices = data[~data.ids.isin(fold)].index\n",
    "    test_indices = data[data.ids.isin(fold)].index\n",
    "    \n",
    "    X_train, X_test = sbert1_X[train_indices], sbert1_X[test_indices]\n",
    "    Y_train, Y_test = data.iloc[train_indices]['1'], data.iloc[test_indices]['1']\n",
    "\n",
    "    Y_pred = []\n",
    "    Y_prob  = []\n",
    "    for i in test_indices:\n",
    "        sim_scores = sim_df.iloc[i]\n",
    "        sorted_scores_idx = [j for j in np.argsort(-sim_scores) if j in train_indices]\n",
    "        top_classes = [data['1'].iloc[j] for j in sorted_scores_idx[:25]]\n",
    "        cls = Counter(top_classes).most_common()[0][0]\n",
    "        Y_pred.append(cls)\n",
    "        Y_prob.append([[1, 0, 0], [0, 1, 0], [0, 0, 1]][cls - 1])\n",
    "        \n",
    "    acc = metrics.accuracy_score(Y_test, Y_pred)\n",
    "    f1s = metrics.f1_score(Y_test, Y_pred, average='weighted')\n",
    "    auc = metrics.roc_auc_score(Y_test, Y_prob, average='weighted', multi_class='ovr')\n",
    "    mcc = computeMCC(one_hot_encoding(Y_test), one_hot_encoding(Y_pred))\n",
    "    mccc = computeMCCclass(one_hot_encoding(Y_test), one_hot_encoding(Y_pred))\n",
    "    \n",
    "    scores_nli_knn.append({'ACC':acc, 'F1':f1s, 'AUC':auc, 'MCC': mcc, 'MCCC':mccc})\n",
    "    \n",
    "    print(f'For fold {k}:', ' - '.join(f'{m}: {s:.4}' for m,s in scores_nli_knn[-1].items()))\n",
    "\n",
    "scores_nli_avg = pd.DataFrame(scores_nli_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_nli_avg.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(vector):\n",
    "    e = np.exp(vector)\n",
    "    return e / e.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_nli_avg = []\n",
    "\n",
    "for k, fold in enumerate(folds):\n",
    "    train_indices = data[~data.ids.isin(fold)].index\n",
    "    test_indices = data[data.ids.isin(fold)].index\n",
    "    \n",
    "    X_train, X_test = sbert1_X[train_indices], sbert1_X[test_indices]\n",
    "    Y_train, Y_test = data.iloc[train_indices]['1'], data.iloc[test_indices]['1']\n",
    "\n",
    "    Y_pred = []\n",
    "    Y_prob  = []\n",
    "    \n",
    "    class_embeddings = np.array([X_train[Y_train == c].mean(axis=0) for c in [1, 2, 3]])\n",
    "    \n",
    "    for i, x in enumerate(X_test):\n",
    "        sim_scores = util.pytorch_cos_sim(x, class_embeddings).numpy()\n",
    "        cls = np.argsort(-sim_scores)[0][0]\n",
    "        Y_pred.append(cls+1)\n",
    "        Y_prob.append(softmax(sim_scores[0]))\n",
    "\n",
    "    acc = metrics.accuracy_score(Y_test, Y_pred)\n",
    "    f1s = metrics.f1_score(Y_test, Y_pred, average='weighted')\n",
    "    auc = metrics.roc_auc_score(Y_test, Y_prob, average='weighted', multi_class='ovr')\n",
    "    mcc = computeMCC(one_hot_encoding(Y_test), one_hot_encoding(Y_pred))\n",
    "    mccc = computeMCCclass(one_hot_encoding(Y_test), one_hot_encoding(Y_pred))\n",
    "    \n",
    "    scores_nli_avg.append({'ACC':acc, 'F1':f1s, 'AUC':auc, 'MCC': mcc, 'MCCC':mccc})\n",
    "    \n",
    "    print(f'For fold {k}:', ' - '.join(f'{m}: {s:.4}' for m,s in scores_nli_avg[-1].items()))\n",
    "\n",
    "scores_nli_avg = pd.DataFrame(scores_nli_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entailment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hg_model_hub_name = \"ynie/roberta-large-snli_mnli_fever_anli_R1_R2_R3-nli\"\n",
    "# hg_model_hub_name = \"ynie/albert-xxlarge-v2-snli_mnli_fever_anli_R1_R2_R3-nli\"\n",
    "# hg_model_hub_name = \"ynie/bart-large-snli_mnli_fever_anli_R1_R2_R3-nli\"\n",
    "# hg_model_hub_name = \"ynie/electra-large-discriminator-snli_mnli_fever_anli_R1_R2_R3-nli\"\n",
    "# hg_model_hub_name = \"ynie/xlnet-large-cased-snli_mnli_fever_anli_R1_R2_R3-nli\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(hg_model_hub_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(hg_model_hub_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 512\n",
    "\n",
    "\n",
    "for k, fold in enumerate(folds):\n",
    "    train_indices = data[~data.ids.isin(fold)].index\n",
    "    test_indices = data[data.ids.isin(fold)].index\n",
    "    \n",
    "    X_train, X_test = data.tweets[train_indices], data.tweets[test_indices]\n",
    "    Y_train, Y_test = data.iloc[train_indices]['1'], data.iloc[test_indices]['1']\n",
    "\n",
    "    tokenized_input_seq_pair = tokenizer.encode_plus(premise, hypothesis,\n",
    "                                                     max_length=max_length,\n",
    "                                                     return_token_type_ids=True, truncation=True)\n",
    "\n",
    "\n",
    "    input_ids = torch.Tensor(tokenized_input_seq_pair['input_ids']).long().unsqueeze(0)\n",
    "    # remember bart doesn't have 'token_type_ids', remove the line below if you are using bart.\n",
    "    token_type_ids = torch.Tensor(tokenized_input_seq_pair['token_type_ids']).long().unsqueeze(0)\n",
    "    attention_mask = torch.Tensor(tokenized_input_seq_pair['attention_mask']).long().unsqueeze(0)\n",
    "\n",
    "    outputs = model(input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    token_type_ids=token_type_ids,\n",
    "                    labels=None)\n",
    "    # Note:\n",
    "    # \"id2label\": {\n",
    "    #     \"0\": \"entailment\",\n",
    "    #     \"1\": \"neutral\",\n",
    "    #     \"2\": \"contradiction\"\n",
    "    # },\n",
    "\n",
    "    predicted_probability = torch.softmax(outputs[0], dim=1)[0].tolist()  # batch_size only one\n",
    "\n",
    "    print(\"Premise:\", premise)\n",
    "    print(\"Hypothesis:\", hypothesis)\n",
    "    print(\"Entailment:\", predicted_probability[0])\n",
    "    print(\"Neutral:\", predicted_probability[1])\n",
    "    print(\"Contradiction:\", predicted_probability[2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BEST TWEET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbert_X = sbert2.encode(data.tweet.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tweet_category in data.label.unique:\n",
    "    mccs[tweet_category] = []\n",
    "    relevant_tweets = data[data.label == tweet_category].tweet.values\n",
    "    \n",
    "    for i, tweet in enumerate(relevant_tweets):\n",
    "        avg_mcc = 0\n",
    "        hypo = sbert2.encode([tweet])[0]\n",
    "        sim_matrix_tweet = util.pytorch_cos_sim(hypo, sbert_X).numpy()[0]\n",
    "\n",
    "        for k, fold in enumerate(folds):\n",
    "            train_indices = data[~data.ids.isin(fold)].index\n",
    "            test_indices = data[data.ids.isin(fold)].index\n",
    "\n",
    "            Y_train, Y_test = data.iloc[train_indices][tweet_category], data.iloc[test_indices][tweet_category]\n",
    "\n",
    "            Y_prob = sim_matrix_tweet[test_indices]\n",
    "            Y_pred = np.array(Y_prob >= 0.5).astype(int)\n",
    "\n",
    "            if sum(Y_pred) > 0:\n",
    "                mcc = metrics.matthews_corrcoef(Y_test, Y_pred)\n",
    "                avg_mcc += mcc\n",
    "\n",
    "        avg_mcc /= len(folds)\n",
    "        mccs[tweet_category].append((avg_mcc, i))\n",
    "\n",
    "    print('For', id2label[int(tweet_category) - 1], ': ')\n",
    "    best_mccs = sorted(mccs[tweet_category], key=lambda x: -x[0])\n",
    "    print(best_mccs[:5])\n",
    "    for supertweet_size in range(1, 2):\n",
    "        avg_mcc = 0\n",
    "        # print('MCC for supertweet_size =', supertweet_size, ': ', end='')\n",
    "        supertweet = ' and '.join(relevant_tweets[j[1]][:-1].lower() for j in best_mccs[:supertweet_size])\n",
    "        hypo = sbert.encode([supertweet])[0]\n",
    "        sim_matrix_tweet = util.pytorch_cos_sim(hypo, sbert_X).numpy()[0]\n",
    "        \n",
    "        for k, fold in enumerate(folds):\n",
    "            train_indices = data[~data.ids.isin(fold)].index\n",
    "            test_indices = data[data.ids.isin(fold)].index\n",
    "\n",
    "            Y_train, Y_test = data.iloc[train_indices][tweet_category], data.iloc[test_indices][tweet_category]\n",
    "\n",
    "            Y_prob = sim_matrix_tweet[test_indices]\n",
    "            Y_pred = np.array(Y_prob >= 0.5).astype(int)\n",
    "\n",
    "            if sum(Y_test) > 0:\n",
    "                mcc = metrics.matthews_corrcoef(Y_test, Y_pred)\n",
    "                avg_mcc += mcc\n",
    "            \n",
    "        avg_mcc /= len(folds)\n",
    "        print('supertweet_size', supertweet_size, avg_mcc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
