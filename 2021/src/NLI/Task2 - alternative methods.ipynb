{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "from sklearn import metrics, preprocessing, linear_model\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "from sklearn.linear_model import SGDClassifier, SGDRegressor, LogisticRegression, LinearRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "en_stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeMCC(y_test, y_pred):\n",
    "    value = 0\n",
    "    for y1, y2 in zip(y_test, y_pred):\n",
    "        try:\n",
    "            value += metrics.matthews_corrcoef(y1, y2)\n",
    "        except ValueError:\n",
    "            print(y1)\n",
    "            print(y2)\n",
    "            exit(1)\n",
    "    mcc = value / len(y_test)\n",
    "    return mcc\n",
    "\n",
    "def computeMCCclass(y_test, y_pred):\n",
    "    mccs = []\n",
    "    for i in range(len(y_test[0,:])):\n",
    "        mccs.append(metrics.matthews_corrcoef(y_test[:,i], y_pred[:,i]))\n",
    "    return np.mean(mccs)\n",
    "\n",
    "def one_hot_encoding(labels):\n",
    "    dictionary = {0: [1, 0,],\n",
    "                  1: [0, 1,]}\n",
    "    enc_labels = []\n",
    "    for el in labels:\n",
    "        enc_labels.append(dictionary[el])\n",
    "    return np.array(enc_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbert = SentenceTransformer('ynie/roberta-large-snli_mnli_fever_anli_R1_R2_R3-nli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls ../data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/dev-full-task-2-clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_words = ['sars', 'covid', 'corona']\n",
    "found = set()\n",
    "\n",
    "for seed in seed_words:\n",
    "    for tweet in data.tweet:\n",
    "        for word in tweet.translate(str.maketrans('', '', string.punctuation)).split(' '):\n",
    "            if seed in word.lower():\n",
    "                found.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "list(found)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {0: 'Suppressed cures',\n",
    "            1: 'Behaviour and Mind Control',\n",
    "            2: 'Antivax',\n",
    "            3: 'Fake virus',\n",
    "            4: 'Intentional Pandemic',\n",
    "            5: 'Harmful Radiation',\n",
    "            6: 'Population reduction',\n",
    "            7: 'New World Order',\n",
    "            8: 'Satanism'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folds = [pd.read_csv(f'../data/task2/dev-full-split-{i}.csv') for i in range(5)]\n",
    "folds = [pd.read_csv(f'../data/task2/dev-full-split-{i}.csv').ids.values for i in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbert_X = sbert.encode(data.tweet.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['1', '2', '3', '4', '5', '6', '7', '8', '9']].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Antivax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data['9'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypotheses = ['Refusals to vaccinate, delaying vaccines, or using certain vaccines but not others. Total opposition to vaccination.',\n",
    "              'I will not vaccinate because vaccines are a lie.',\n",
    "              'Vaccines are a hoax']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for hypothesis in hypotheses:\n",
    "    hypo = sbert.encode([hypothesis])[0]\n",
    "    print('Hypothesis: '+ hypothesis)\n",
    "\n",
    "    sim_matrix = util.pytorch_cos_sim(hypo, sbert_X).numpy()[0]\n",
    "    scores_nli = []\n",
    "\n",
    "    for k, fold in enumerate(folds):\n",
    "        train_indices = data[~data.ids.isin(fold)].index\n",
    "        test_indices = data[data.ids.isin(fold)].index\n",
    "\n",
    "        Y_train, Y_test = data.iloc[train_indices]['3'], data.iloc[test_indices]['3']\n",
    "\n",
    "        Y_prob = sim_matrix[test_indices]\n",
    "        Y_pred = np.array(Y_prob >= 0.5).astype(int)\n",
    "\n",
    "        acc = metrics.accuracy_score(Y_test, Y_pred)\n",
    "        f1s = metrics.f1_score(Y_test, Y_pred, average='weighted')\n",
    "        auc = metrics.roc_auc_score(Y_test, Y_prob, average='weighted', multi_class='ovr')\n",
    "        mcc = metrics.matthews_corrcoef(Y_test, Y_pred)\n",
    "        # mccc = computeMCCclass(one_hot_encoding(Y_test), one_hot_encoding(Y_pred))\n",
    "\n",
    "        scores_nli.append({'ACC':acc, 'F1':f1s, 'AUC':auc, 'MCC': mcc})\n",
    "\n",
    "        print(f'For fold {k}:', ' - '.join(f'{m}: {s:.4}' for m,s in scores_nli[-1].items()))\n",
    "    print()\n",
    "    scores_nli = pd.DataFrame(scores_nli)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Satanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis = 'Satanism is a group of ideological and philosophical beliefs based on Satan. Satanism existed primarily as an accusation by various Christian groups toward perceived ideological opponents, rather than a self-identity. '\n",
    "hypothesis = 'The Mark of the Beast, Covid vaccine?  (Revelation 13) both small and great, both rich and poor, both free and slave, to be marked on the right hand or the forehead, so that no one can buy or sell unless he has the mark, that is, the name of the beast or the number of its name.'\n",
    "hypo = sbert.encode([hypothesis])[0]\n",
    "\n",
    "satanism_tweets = data[data['9'] == 1].tweet.values\n",
    "\n",
    "sim_matrix = util.pytorch_cos_sim(hypo, sbert_X).numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_nli = []\n",
    "\n",
    "best_mcc = (0, 0)\n",
    "\n",
    "for i, tweet in enumerate(satanism_tweets):\n",
    "    if i in [54, 6, 58, 72]:\n",
    "        continue\n",
    "    avg_mcc = 0\n",
    "    hypo = sbert.encode([tweet])[0]\n",
    "    sim_matrix_tweet = util.pytorch_cos_sim(hypo, sbert_X).numpy()[0]\n",
    "    \n",
    "    for k, fold in enumerate(folds):\n",
    "        train_indices = data[~data.ids.isin(fold)].index\n",
    "        test_indices = data[data.ids.isin(fold)].index\n",
    "\n",
    "        Y_train, Y_test = data.iloc[train_indices]['9'], data.iloc[test_indices]['9']\n",
    "\n",
    "        Y_prob = sim_matrix_tweet[test_indices]\n",
    "        Y_pred = np.array(Y_prob >= 0.5).astype(int)\n",
    "\n",
    "        \n",
    "        mcc = metrics.matthews_corrcoef(Y_test, Y_pred)\n",
    "        avg_mcc += mcc\n",
    "        \n",
    "    avg_mcc /= 5\n",
    "    if avg_mcc > best_mcc[0]:\n",
    "        best_mcc = (avg_mcc, i)\n",
    "\n",
    "print(best_mcc, satanism_tweets[best_mcc[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "super_tweet = '. '.join(s[:-1] for s in satanism_tweets[[54]]) + '.'\n",
    "super_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_mcc = 0\n",
    "hypo = sbert.encode([super_tweet])[0]\n",
    "sim_matrix_tweet = util.pytorch_cos_sim(hypo, sbert_X).numpy()[0]\n",
    "\n",
    "for k, fold in enumerate(folds):\n",
    "    train_indices = data[~data.ids.isin(fold)].index\n",
    "    test_indices = data[data.ids.isin(fold)].index\n",
    "\n",
    "    Y_train, Y_test = data.iloc[train_indices]['9'], data.iloc[test_indices]['9']\n",
    "\n",
    "    Y_prob = sim_matrix_tweet[test_indices]\n",
    "    Y_pred = np.array(Y_prob >= 0.5).astype(int)\n",
    "\n",
    "\n",
    "    mcc = metrics.matthews_corrcoef(Y_test, Y_pred)\n",
    "    avg_mcc += mcc\n",
    "\n",
    "avg_mcc /= 5\n",
    "print(avg_mcc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New World Order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis = 'The New World Order is a conspiracy theory which hypothesizes a secretly emerging totalitarian world government.'\n",
    "hypo = sbert.encode([hypothesis])[0]\n",
    "\n",
    "sim_matrix = util.pytorch_cos_sim(hypo, sbert_X).numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_nli = []\n",
    "\n",
    "for k, fold in enumerate(folds):\n",
    "    train_indices = data[~data.ids.isin(fold)].index\n",
    "    test_indices = data[data.ids.isin(fold)].index\n",
    "    \n",
    "    Y_train, Y_test = data.iloc[train_indices]['1'], data.iloc[test_indices]['1']\n",
    "\n",
    "    Y_prob = sim_matrix[test_indices]\n",
    "    Y_pred = np.array(Y_prob >= 0.5).astype(int)\n",
    "\n",
    "    acc = metrics.accuracy_score(Y_test, Y_pred)\n",
    "    f1s = metrics.f1_score(Y_test, Y_pred, average='weighted')\n",
    "    auc = metrics.roc_auc_score(Y_test, Y_prob, average='weighted', multi_class='ovr')\n",
    "    mcc = computeMCC(one_hot_encoding(Y_test), one_hot_encoding(Y_pred))\n",
    "    mccc = computeMCCclass(one_hot_encoding(Y_test), one_hot_encoding(Y_pred))\n",
    "    \n",
    "    scores_nli.append({'ACC':acc, 'F1':f1s, 'AUC':auc, 'MCC': mcc, 'MCCC':mccc})\n",
    "    \n",
    "    print(f'For fold {k}:', ' - '.join(f'{m}: {s:.4}' for m,s in scores_nli[-1].items()))\n",
    "\n",
    "scores_nli = pd.DataFrame(scores_nli)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Tweets to Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mccs = {}\n",
    "for tweet_category in tqdm([str(t) for t in range(1, 10)]):\n",
    "    mccs[tweet_category] = []\n",
    "    relevant_tweets = data[data[tweet_category] == 1].tweet.values\n",
    "    for i, tweet in enumerate(relevant_tweets):\n",
    "        avg_mcc = 0\n",
    "        hypo = sbert.encode([tweet])[0]\n",
    "        sim_matrix_tweet = util.pytorch_cos_sim(hypo, sbert_X).numpy()[0]\n",
    "\n",
    "        for k, fold in enumerate(folds):\n",
    "            train_indices = data[~data.ids.isin(fold)].index\n",
    "            test_indices = data[data.ids.isin(fold)].index\n",
    "\n",
    "            Y_train, Y_test = data.iloc[train_indices][tweet_category], data.iloc[test_indices][tweet_category]\n",
    "\n",
    "            Y_prob = sim_matrix_tweet[test_indices]\n",
    "            Y_pred = np.array(Y_prob >= 0.5).astype(int)\n",
    "\n",
    "            if sum(Y_pred) > 0:\n",
    "                mcc = metrics.matthews_corrcoef(Y_test, Y_pred)\n",
    "                avg_mcc += mcc\n",
    "\n",
    "        avg_mcc /= len(folds)\n",
    "        mccs[tweet_category].append((avg_mcc, i))\n",
    "\n",
    "    print('For', id2label[int(tweet_category) - 1])\n",
    "    best_mccs = sorted(mccs[tweet_category], key=lambda x: -x[0])\n",
    "    for supertweet_size in range(1, 12, 2):\n",
    "        \n",
    "        print('MCC for supertweet_size =', supertweet_size, ': ', end='')\n",
    "        supertweet = '[SEP]'.join(relevant_tweets[j[1]][:-1].lower() for j in best_mccs[:supertweet_size]) + '.'\n",
    "        hypo = sbert.encode([supertweet])[0]\n",
    "        sim_matrix_tweet = util.pytorch_cos_sim(hypo, sbert_X).numpy()[0]\n",
    "        \n",
    "        for k, fold in enumerate(folds):\n",
    "            train_indices = data[~data.ids.isin(fold)].index\n",
    "            test_indices = data[data.ids.isin(fold)].index\n",
    "\n",
    "            Y_train, Y_test = data.iloc[train_indices][tweet_category], data.iloc[test_indices][tweet_category]\n",
    "\n",
    "            Y_prob = sim_matrix_tweet[test_indices]\n",
    "            Y_pred = np.array(Y_prob >= 0.5).astype(int)\n",
    "\n",
    "            if sum(Y_pred) > 0:\n",
    "                mcc = metrics.matthews_corrcoef(Y_test, Y_pred)\n",
    "                avg_mcc += mcc\n",
    "\n",
    "        avg_mcc /= len(folds)\n",
    "        print(avg_mcc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [0.37400027228104693, 0.26894977597247083, 0.3339076702319483, 0.4949745459618075, 0.24668037087907382,\n",
    "     0.25236930172231153, 0.5345499352319787, 0.40177709057511046, 0.2003782867163418]\n",
    "print(sum(l) / len(l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass entailment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "candidate_labels = list(id2label.values())\n",
    "candidate_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_labels = ['Suppressed cures', \n",
    "                    'Behaviour and Mind Control', \n",
    "                    'Antivax', \n",
    "                    'Fake virus', \n",
    "                    'Intentional Pandemic', \n",
    "                    'Harmful Radiation', \n",
    "                    'Population reduction', \n",
    "                    'New World Order', \n",
    "                    'Satanism']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "thresh = 0.5\n",
    "for k, fold in tqdm(enumerate(folds)):\n",
    "    train_indices = data[~data.ids.isin(fold)].index\n",
    "    test_indices = data[data.ids.isin(fold)].index\n",
    "\n",
    "    X_train, X_test = data.tweet[train_indices], data.tweet[test_indices]\n",
    "    Y_train, Y_test = data.iloc[train_indices][[str(i) for i in range(1, 10)]], data.iloc[test_indices][[str(i) for i in range(1, 10)]]\n",
    "    \n",
    "    per_class_true = {c:[] for c in range(len(candidate_labels))}\n",
    "    per_class_pred = {c:[] for c in range(len(candidate_labels))}\n",
    "\n",
    "    for i, tweet in tqdm(enumerate(X_test), total=len(X_test)):\n",
    "        # print(i, test_indices[i], tweet)\n",
    "        # print('True label:', Y_test.values[i])\n",
    "        output = classifier(tweet, candidate_labels, multi_label=True)\n",
    "        \n",
    "        for j, s in enumerate(output['scores']):\n",
    "            per_class_true[j].append(Y_test.values[i][j])\n",
    "            per_class_pred[j].append(int(s > thresh))\n",
    "        \n",
    "    # print(per_class_true)\n",
    "    # print(per_class_pred)\n",
    "    results[k] = (per_class_true, per_class_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "rest = classifier(list(X_test.values), candidate_labels, multi_label=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
